{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# load the source code reload module\n",
    "%load_ext autoreload"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:53:10.751732Z",
     "start_time": "2024-01-18T03:53:10.742348Z"
    }
   },
   "id": "8bc6a211213ca189"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\r\n"
     ]
    }
   ],
   "source": [
    "! python3 --version"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:46:57.845429Z",
     "start_time": "2024-01-18T03:46:57.690085Z"
    }
   },
   "id": "40d7f5a05bdbaeef"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:46:59.790139Z",
     "start_time": "2024-01-18T03:46:58.508772Z"
    }
   },
   "id": "c213bdd2418b70f4"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import json\n",
    "from xmlschema import XMLSchema\n",
    "\n",
    "os.chdir( \"/var/model/genie-in-the-box/src\" )\n",
    "\n",
    "import lib.utils.util     as du\n",
    "import lib.utils.util_xml as dux\n",
    "import lib.utils.util_pytorch as dupt\n",
    "\n",
    "from lib.agents.agent         import XXX_Agent\n",
    "\n",
    "from ephemera.prompts.xml_fine_tuning_prompt_generator import XmlFineTuningPromptGenerator\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:54:13.025709Z",
     "start_time": "2024-01-18T03:54:12.975634Z"
    }
   },
   "id": "4e1faf28cf522ca7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Validation dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fcd418053c3052b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# path = \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\"\n",
    "# deepily_dataset_validation = du.get_file_as_list( path )\n",
    "# deepily_dataset_validation = [ json.loads( line ) for line in deepily_dataset_validation ]\n",
    "# len( deepily_dataset_validation )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:13:18.902429Z",
     "start_time": "2024-01-18T03:13:18.832408Z"
    }
   },
   "id": "ae76c88a9a5e791d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# def generate_text( foo_tokenizer, model, question, max_new_tokens=256, debug=False ):\n",
    "#     \n",
    "#     instruction = f\"\"\"### Instruction:\n",
    "#     Use the Task below and the Input given to write a Response that can solve the following Task:\n",
    "# \n",
    "#     ### Task:\n",
    "#     Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
    "# \n",
    "#     You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\n",
    "# \n",
    "#     Requirement: You MUST NOT use python code to answer this question.\n",
    "#     Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
    "#     Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n",
    "# \n",
    "#     ### Input:\n",
    "#     \n",
    "#     Below is the raw human voice command transcription formatted using simple XML:\n",
    "#     \n",
    "#     <human>\n",
    "#         <voice-command>{question}</voice-command>\n",
    "#     </human>\n",
    "#     \n",
    "#     The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
    "#     \n",
    "#     <response>\n",
    "#         <browser-command></browser-command>\n",
    "#         <args></args>\n",
    "#     </response>\n",
    "# \n",
    "#     Requirement: The first word of your response MUST be `<response>`\n",
    "# \n",
    "#     ### Response:\"\"\"\n",
    "#     \n",
    "#     \n",
    "#     device = \"cuda:0\"\n",
    "#     inputs = foo_tokenizer( instruction, return_tensors=\"pt\" ).to( device )\n",
    "#     \n",
    "#     stop_token_id = foo_tokenizer.encode( \"</response>\" )[ 0 ]\n",
    "#     \n",
    "#     generation_output = model.generate(\n",
    "#         input_ids=inputs[ \"input_ids\" ],\n",
    "#         attention_mask=inputs[ \"attention_mask\" ],\n",
    "#         max_new_tokens=max_new_tokens,\n",
    "#         eos_token_id=stop_token_id,\n",
    "#         pad_token_id=stop_token_id\n",
    "#     )\n",
    "#         \n",
    "#     if debug: \n",
    "#         print( \"generation_output[ 0 ]:\", generation_output[ 0 ], end=\"\\n\\n\" )\n",
    "#         print( \"generation_output[ 0 ].shape:\", generation_output[ 0 ].shape, end=\"\\n\\n\" )\n",
    "#     \n",
    "#     # Skip decoding the prompt part of the output   \n",
    "#     input_length = inputs[ \"input_ids\" ].size( 1 )\n",
    "#     raw_output = foo_tokenizer.decode( generation_output[ 0 ][ input_length: ] )\n",
    "#     \n",
    "#     if debug: \n",
    "#         print( \"raw_output:\", raw_output, end=\"\\n\\n\" )\n",
    "#         print(  \"len( raw_output ):\", len( raw_output ), end=\"\\n\\n\")\n",
    "#     \n",
    "#     # response   = raw_output.split( \"### Response:\" )[ 1 ]\n",
    "#     \n",
    "#     response = raw_output.replace( \"</s><s>\", \"\" ).strip()\n",
    "#     \n",
    "#     return response\n",
    "# \n",
    "# question = \"Ask Google scholar about QLORA and PEFT fine-tuning for XML output, show results in Another tab\"\n",
    "# \n",
    "# # for line in generate_text( tokenizer, base_model, question ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:13:22.264289Z",
     "start_time": "2024-01-18T03:13:22.201018Z"
    }
   },
   "id": "deee8f0e5ce7efd5"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "os.chdir( \"/var/model/Phind-CodeLlama-34B-v2\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:13:31.095652Z",
     "start_time": "2024-01-18T03:13:31.084771Z"
    }
   },
   "id": "7ba53819b2920023"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_base_model_and_tokenizer( model_path=\".\", tokenizer_path=\".\", use_quant_config=True, device_map=\"auto\" ):\n",
    "    \n",
    "    compute_dtype = getattr( torch, \"float16\" )\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype\n",
    "    )\n",
    "    print( bnb_config )\n",
    "    tokenizer              = AutoTokenizer.from_pretrained( tokenizer_path )\n",
    "    tokenizer.pad_token    = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    if use_quant_config:\n",
    "        # Â¡OJO! Why were we turning off the cash here? It makes a big performance difference: 21 vs 14 tokens per second\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, quantization_config=bnb_config, device_map=device_map, low_cpu_mem_usage=True, use_cache=True, attn_implementation=\"flash_attention_2\"\n",
    "        )\n",
    "    else:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, device_map=device_map, low_cpu_mem_usage=True, use_cache=True, attn_implementation=\"flash_attention_2\"\n",
    "        )\n",
    "    \n",
    "    return base_model, tokenizer\n",
    "    \n",
    "# base_model, tokenizer = get_base_model_and_tokenizer( device_map=\"cuda:1\" )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:49:35.633265Z",
     "start_time": "2024-01-18T03:49:35.553161Z"
    }
   },
   "id": "55afe25a14757322"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight: cuda:0\n",
      "model.layers.0.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.0.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.0.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.0.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.0.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.0.mlp.up_proj.weight: cuda:0\n",
      "model.layers.0.mlp.down_proj.weight: cuda:0\n",
      "model.layers.0.input_layernorm.weight: cuda:0\n",
      "model.layers.0.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.1.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.1.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.1.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.1.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.1.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.1.mlp.up_proj.weight: cuda:0\n",
      "model.layers.1.mlp.down_proj.weight: cuda:0\n",
      "model.layers.1.input_layernorm.weight: cuda:0\n",
      "model.layers.1.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.2.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.2.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.2.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.2.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.2.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.2.mlp.up_proj.weight: cuda:0\n",
      "model.layers.2.mlp.down_proj.weight: cuda:0\n",
      "model.layers.2.input_layernorm.weight: cuda:0\n",
      "model.layers.2.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.3.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.3.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.3.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.3.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.3.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.3.mlp.up_proj.weight: cuda:0\n",
      "model.layers.3.mlp.down_proj.weight: cuda:0\n",
      "model.layers.3.input_layernorm.weight: cuda:0\n",
      "model.layers.3.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.4.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.4.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.4.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.4.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.4.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.4.mlp.up_proj.weight: cuda:0\n",
      "model.layers.4.mlp.down_proj.weight: cuda:0\n",
      "model.layers.4.input_layernorm.weight: cuda:0\n",
      "model.layers.4.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.5.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.5.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.5.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.5.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.5.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.5.mlp.up_proj.weight: cuda:0\n",
      "model.layers.5.mlp.down_proj.weight: cuda:0\n",
      "model.layers.5.input_layernorm.weight: cuda:0\n",
      "model.layers.5.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.6.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.6.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.6.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.6.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.6.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.6.mlp.up_proj.weight: cuda:0\n",
      "model.layers.6.mlp.down_proj.weight: cuda:0\n",
      "model.layers.6.input_layernorm.weight: cuda:0\n",
      "model.layers.6.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.7.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.7.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.7.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.7.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.7.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.7.mlp.up_proj.weight: cuda:0\n",
      "model.layers.7.mlp.down_proj.weight: cuda:0\n",
      "model.layers.7.input_layernorm.weight: cuda:0\n",
      "model.layers.7.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.8.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.8.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.8.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.8.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.8.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.8.mlp.up_proj.weight: cuda:0\n",
      "model.layers.8.mlp.down_proj.weight: cuda:0\n",
      "model.layers.8.input_layernorm.weight: cuda:0\n",
      "model.layers.8.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.9.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.9.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.9.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.9.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.9.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.9.mlp.up_proj.weight: cuda:0\n",
      "model.layers.9.mlp.down_proj.weight: cuda:0\n",
      "model.layers.9.input_layernorm.weight: cuda:0\n",
      "model.layers.9.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.10.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.10.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.10.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.10.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.10.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.10.mlp.up_proj.weight: cuda:0\n",
      "model.layers.10.mlp.down_proj.weight: cuda:0\n",
      "model.layers.10.input_layernorm.weight: cuda:0\n",
      "model.layers.10.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.11.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.11.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.11.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.11.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.11.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.11.mlp.up_proj.weight: cuda:0\n",
      "model.layers.11.mlp.down_proj.weight: cuda:0\n",
      "model.layers.11.input_layernorm.weight: cuda:0\n",
      "model.layers.11.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.12.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.12.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.12.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.12.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.12.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.12.mlp.up_proj.weight: cuda:0\n",
      "model.layers.12.mlp.down_proj.weight: cuda:0\n",
      "model.layers.12.input_layernorm.weight: cuda:0\n",
      "model.layers.12.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.13.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.13.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.13.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.13.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.13.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.13.mlp.up_proj.weight: cuda:0\n",
      "model.layers.13.mlp.down_proj.weight: cuda:0\n",
      "model.layers.13.input_layernorm.weight: cuda:0\n",
      "model.layers.13.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.14.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.14.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.14.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.14.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.14.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.14.mlp.up_proj.weight: cuda:0\n",
      "model.layers.14.mlp.down_proj.weight: cuda:0\n",
      "model.layers.14.input_layernorm.weight: cuda:0\n",
      "model.layers.14.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.15.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.15.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.15.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.15.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.15.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.15.mlp.up_proj.weight: cuda:0\n",
      "model.layers.15.mlp.down_proj.weight: cuda:0\n",
      "model.layers.15.input_layernorm.weight: cuda:0\n",
      "model.layers.15.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.16.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.16.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.16.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.16.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.16.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.16.mlp.up_proj.weight: cuda:0\n",
      "model.layers.16.mlp.down_proj.weight: cuda:0\n",
      "model.layers.16.input_layernorm.weight: cuda:0\n",
      "model.layers.16.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.17.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.17.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.17.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.17.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.17.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.17.mlp.up_proj.weight: cuda:0\n",
      "model.layers.17.mlp.down_proj.weight: cuda:0\n",
      "model.layers.17.input_layernorm.weight: cuda:0\n",
      "model.layers.17.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.18.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.18.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.18.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.18.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.18.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.18.mlp.up_proj.weight: cuda:0\n",
      "model.layers.18.mlp.down_proj.weight: cuda:0\n",
      "model.layers.18.input_layernorm.weight: cuda:0\n",
      "model.layers.18.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.19.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.19.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.19.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.19.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.19.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.19.mlp.up_proj.weight: cuda:0\n",
      "model.layers.19.mlp.down_proj.weight: cuda:0\n",
      "model.layers.19.input_layernorm.weight: cuda:0\n",
      "model.layers.19.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.20.self_attn.q_proj.weight: cuda:0\n",
      "model.layers.20.self_attn.k_proj.weight: cuda:0\n",
      "model.layers.20.self_attn.v_proj.weight: cuda:0\n",
      "model.layers.20.self_attn.o_proj.weight: cuda:0\n",
      "model.layers.20.mlp.gate_proj.weight: cuda:0\n",
      "model.layers.20.mlp.up_proj.weight: cuda:0\n",
      "model.layers.20.mlp.down_proj.weight: cuda:0\n",
      "model.layers.20.input_layernorm.weight: cuda:0\n",
      "model.layers.20.post_attention_layernorm.weight: cuda:0\n",
      "model.layers.21.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.21.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.21.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.21.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.21.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.21.mlp.up_proj.weight: cuda:1\n",
      "model.layers.21.mlp.down_proj.weight: cuda:1\n",
      "model.layers.21.input_layernorm.weight: cuda:1\n",
      "model.layers.21.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.22.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.22.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.22.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.22.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.22.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.22.mlp.up_proj.weight: cuda:1\n",
      "model.layers.22.mlp.down_proj.weight: cuda:1\n",
      "model.layers.22.input_layernorm.weight: cuda:1\n",
      "model.layers.22.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.23.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.23.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.23.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.23.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.23.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.23.mlp.up_proj.weight: cuda:1\n",
      "model.layers.23.mlp.down_proj.weight: cuda:1\n",
      "model.layers.23.input_layernorm.weight: cuda:1\n",
      "model.layers.23.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.24.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.24.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.24.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.24.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.24.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.24.mlp.up_proj.weight: cuda:1\n",
      "model.layers.24.mlp.down_proj.weight: cuda:1\n",
      "model.layers.24.input_layernorm.weight: cuda:1\n",
      "model.layers.24.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.25.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.25.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.25.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.25.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.25.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.25.mlp.up_proj.weight: cuda:1\n",
      "model.layers.25.mlp.down_proj.weight: cuda:1\n",
      "model.layers.25.input_layernorm.weight: cuda:1\n",
      "model.layers.25.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.26.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.26.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.26.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.26.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.26.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.26.mlp.up_proj.weight: cuda:1\n",
      "model.layers.26.mlp.down_proj.weight: cuda:1\n",
      "model.layers.26.input_layernorm.weight: cuda:1\n",
      "model.layers.26.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.27.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.27.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.27.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.27.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.27.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.27.mlp.up_proj.weight: cuda:1\n",
      "model.layers.27.mlp.down_proj.weight: cuda:1\n",
      "model.layers.27.input_layernorm.weight: cuda:1\n",
      "model.layers.27.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.28.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.28.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.28.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.28.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.28.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.28.mlp.up_proj.weight: cuda:1\n",
      "model.layers.28.mlp.down_proj.weight: cuda:1\n",
      "model.layers.28.input_layernorm.weight: cuda:1\n",
      "model.layers.28.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.29.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.29.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.29.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.29.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.29.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.29.mlp.up_proj.weight: cuda:1\n",
      "model.layers.29.mlp.down_proj.weight: cuda:1\n",
      "model.layers.29.input_layernorm.weight: cuda:1\n",
      "model.layers.29.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.30.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.30.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.30.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.30.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.30.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.30.mlp.up_proj.weight: cuda:1\n",
      "model.layers.30.mlp.down_proj.weight: cuda:1\n",
      "model.layers.30.input_layernorm.weight: cuda:1\n",
      "model.layers.30.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.31.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.31.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.31.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.31.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.31.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.31.mlp.up_proj.weight: cuda:1\n",
      "model.layers.31.mlp.down_proj.weight: cuda:1\n",
      "model.layers.31.input_layernorm.weight: cuda:1\n",
      "model.layers.31.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.32.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.32.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.32.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.32.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.32.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.32.mlp.up_proj.weight: cuda:1\n",
      "model.layers.32.mlp.down_proj.weight: cuda:1\n",
      "model.layers.32.input_layernorm.weight: cuda:1\n",
      "model.layers.32.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.33.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.33.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.33.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.33.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.33.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.33.mlp.up_proj.weight: cuda:1\n",
      "model.layers.33.mlp.down_proj.weight: cuda:1\n",
      "model.layers.33.input_layernorm.weight: cuda:1\n",
      "model.layers.33.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.34.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.34.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.34.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.34.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.34.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.34.mlp.up_proj.weight: cuda:1\n",
      "model.layers.34.mlp.down_proj.weight: cuda:1\n",
      "model.layers.34.input_layernorm.weight: cuda:1\n",
      "model.layers.34.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.35.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.35.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.35.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.35.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.35.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.35.mlp.up_proj.weight: cuda:1\n",
      "model.layers.35.mlp.down_proj.weight: cuda:1\n",
      "model.layers.35.input_layernorm.weight: cuda:1\n",
      "model.layers.35.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.36.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.36.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.36.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.36.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.36.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.36.mlp.up_proj.weight: cuda:1\n",
      "model.layers.36.mlp.down_proj.weight: cuda:1\n",
      "model.layers.36.input_layernorm.weight: cuda:1\n",
      "model.layers.36.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.37.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.37.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.37.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.37.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.37.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.37.mlp.up_proj.weight: cuda:1\n",
      "model.layers.37.mlp.down_proj.weight: cuda:1\n",
      "model.layers.37.input_layernorm.weight: cuda:1\n",
      "model.layers.37.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.38.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.38.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.38.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.38.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.38.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.38.mlp.up_proj.weight: cuda:1\n",
      "model.layers.38.mlp.down_proj.weight: cuda:1\n",
      "model.layers.38.input_layernorm.weight: cuda:1\n",
      "model.layers.38.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.39.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.39.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.39.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.39.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.39.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.39.mlp.up_proj.weight: cuda:1\n",
      "model.layers.39.mlp.down_proj.weight: cuda:1\n",
      "model.layers.39.input_layernorm.weight: cuda:1\n",
      "model.layers.39.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.40.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.40.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.40.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.40.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.40.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.40.mlp.up_proj.weight: cuda:1\n",
      "model.layers.40.mlp.down_proj.weight: cuda:1\n",
      "model.layers.40.input_layernorm.weight: cuda:1\n",
      "model.layers.40.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.41.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.41.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.41.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.41.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.41.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.41.mlp.up_proj.weight: cuda:1\n",
      "model.layers.41.mlp.down_proj.weight: cuda:1\n",
      "model.layers.41.input_layernorm.weight: cuda:1\n",
      "model.layers.41.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.42.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.42.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.42.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.42.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.42.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.42.mlp.up_proj.weight: cuda:1\n",
      "model.layers.42.mlp.down_proj.weight: cuda:1\n",
      "model.layers.42.input_layernorm.weight: cuda:1\n",
      "model.layers.42.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.43.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.43.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.43.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.43.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.43.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.43.mlp.up_proj.weight: cuda:1\n",
      "model.layers.43.mlp.down_proj.weight: cuda:1\n",
      "model.layers.43.input_layernorm.weight: cuda:1\n",
      "model.layers.43.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.44.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.44.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.44.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.44.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.44.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.44.mlp.up_proj.weight: cuda:1\n",
      "model.layers.44.mlp.down_proj.weight: cuda:1\n",
      "model.layers.44.input_layernorm.weight: cuda:1\n",
      "model.layers.44.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.45.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.45.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.45.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.45.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.45.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.45.mlp.up_proj.weight: cuda:1\n",
      "model.layers.45.mlp.down_proj.weight: cuda:1\n",
      "model.layers.45.input_layernorm.weight: cuda:1\n",
      "model.layers.45.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.46.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.46.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.46.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.46.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.46.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.46.mlp.up_proj.weight: cuda:1\n",
      "model.layers.46.mlp.down_proj.weight: cuda:1\n",
      "model.layers.46.input_layernorm.weight: cuda:1\n",
      "model.layers.46.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.47.self_attn.q_proj.weight: cuda:1\n",
      "model.layers.47.self_attn.k_proj.weight: cuda:1\n",
      "model.layers.47.self_attn.v_proj.weight: cuda:1\n",
      "model.layers.47.self_attn.o_proj.weight: cuda:1\n",
      "model.layers.47.mlp.gate_proj.weight: cuda:1\n",
      "model.layers.47.mlp.up_proj.weight: cuda:1\n",
      "model.layers.47.mlp.down_proj.weight: cuda:1\n",
      "model.layers.47.input_layernorm.weight: cuda:1\n",
      "model.layers.47.post_attention_layernorm.weight: cuda:1\n",
      "model.norm.weight: cuda:1\n",
      "lm_head.weight: cuda:1\n"
     ]
    }
   ],
   "source": [
    "dupt.print_device_allocation( base_model )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T16:42:24.614704Z",
     "start_time": "2024-01-11T16:42:24.601405Z"
    }
   },
   "id": "4b0c2465c3fe3182"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dupt.is_allocated_to_cpu( base_model )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T18:31:47.567545Z",
     "start_time": "2024-01-12T18:31:47.560058Z"
    }
   },
   "id": "d338290eefb85453"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx [0] name: [] type: [LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 8192)\n",
      "    (layers): ModuleList(\n",
      "      (0-47): 48 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaFlashAttention2(\n",
      "          (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
      ")]\n",
      "idx [1] name: [model] type: [LlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 8192)\n",
      "  (layers): ModuleList(\n",
      "    (0-47): 48 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaFlashAttention2(\n",
      "        (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "        (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "        (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "        (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "        (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "        (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [2] name: [model.embed_tokens] type: [Embedding(32000, 8192)]\n",
      "idx [3] name: [model.layers] type: [ModuleList(\n",
      "  (0-47): 48 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaFlashAttention2(\n",
      "      (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm()\n",
      "    (post_attention_layernorm): LlamaRMSNorm()\n",
      "  )\n",
      ")]\n",
      "idx [4] name: [model.layers.0] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [5] name: [model.layers.0.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [6] name: [model.layers.0.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [7] name: [model.layers.0.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [8] name: [model.layers.0.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [9] name: [model.layers.0.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [10] name: [model.layers.0.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [11] name: [model.layers.0.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [12] name: [model.layers.0.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [13] name: [model.layers.0.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [14] name: [model.layers.0.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [15] name: [model.layers.0.mlp.act_fn] type: [SiLU()]\n",
      "idx [16] name: [model.layers.0.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [17] name: [model.layers.0.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [18] name: [model.layers.1] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [19] name: [model.layers.1.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [20] name: [model.layers.1.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [21] name: [model.layers.1.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [22] name: [model.layers.1.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [23] name: [model.layers.1.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [24] name: [model.layers.1.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [25] name: [model.layers.1.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [26] name: [model.layers.1.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [27] name: [model.layers.1.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [28] name: [model.layers.1.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [29] name: [model.layers.1.mlp.act_fn] type: [SiLU()]\n",
      "idx [30] name: [model.layers.1.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [31] name: [model.layers.1.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [32] name: [model.layers.2] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [33] name: [model.layers.2.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [34] name: [model.layers.2.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [35] name: [model.layers.2.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [36] name: [model.layers.2.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [37] name: [model.layers.2.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [38] name: [model.layers.2.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [39] name: [model.layers.2.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [40] name: [model.layers.2.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [41] name: [model.layers.2.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [42] name: [model.layers.2.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [43] name: [model.layers.2.mlp.act_fn] type: [SiLU()]\n",
      "idx [44] name: [model.layers.2.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [45] name: [model.layers.2.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [46] name: [model.layers.3] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [47] name: [model.layers.3.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [48] name: [model.layers.3.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [49] name: [model.layers.3.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [50] name: [model.layers.3.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [51] name: [model.layers.3.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [52] name: [model.layers.3.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [53] name: [model.layers.3.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [54] name: [model.layers.3.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [55] name: [model.layers.3.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [56] name: [model.layers.3.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [57] name: [model.layers.3.mlp.act_fn] type: [SiLU()]\n",
      "idx [58] name: [model.layers.3.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [59] name: [model.layers.3.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [60] name: [model.layers.4] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [61] name: [model.layers.4.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [62] name: [model.layers.4.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [63] name: [model.layers.4.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [64] name: [model.layers.4.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [65] name: [model.layers.4.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [66] name: [model.layers.4.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [67] name: [model.layers.4.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [68] name: [model.layers.4.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [69] name: [model.layers.4.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [70] name: [model.layers.4.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [71] name: [model.layers.4.mlp.act_fn] type: [SiLU()]\n",
      "idx [72] name: [model.layers.4.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [73] name: [model.layers.4.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [74] name: [model.layers.5] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [75] name: [model.layers.5.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [76] name: [model.layers.5.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [77] name: [model.layers.5.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [78] name: [model.layers.5.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [79] name: [model.layers.5.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [80] name: [model.layers.5.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [81] name: [model.layers.5.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [82] name: [model.layers.5.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [83] name: [model.layers.5.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [84] name: [model.layers.5.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [85] name: [model.layers.5.mlp.act_fn] type: [SiLU()]\n",
      "idx [86] name: [model.layers.5.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [87] name: [model.layers.5.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [88] name: [model.layers.6] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [89] name: [model.layers.6.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [90] name: [model.layers.6.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [91] name: [model.layers.6.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [92] name: [model.layers.6.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [93] name: [model.layers.6.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [94] name: [model.layers.6.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [95] name: [model.layers.6.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [96] name: [model.layers.6.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [97] name: [model.layers.6.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [98] name: [model.layers.6.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [99] name: [model.layers.6.mlp.act_fn] type: [SiLU()]\n",
      "idx [100] name: [model.layers.6.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [101] name: [model.layers.6.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [102] name: [model.layers.7] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [103] name: [model.layers.7.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [104] name: [model.layers.7.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [105] name: [model.layers.7.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [106] name: [model.layers.7.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [107] name: [model.layers.7.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [108] name: [model.layers.7.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [109] name: [model.layers.7.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [110] name: [model.layers.7.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [111] name: [model.layers.7.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [112] name: [model.layers.7.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [113] name: [model.layers.7.mlp.act_fn] type: [SiLU()]\n",
      "idx [114] name: [model.layers.7.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [115] name: [model.layers.7.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [116] name: [model.layers.8] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [117] name: [model.layers.8.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [118] name: [model.layers.8.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [119] name: [model.layers.8.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [120] name: [model.layers.8.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [121] name: [model.layers.8.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [122] name: [model.layers.8.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [123] name: [model.layers.8.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [124] name: [model.layers.8.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [125] name: [model.layers.8.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [126] name: [model.layers.8.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [127] name: [model.layers.8.mlp.act_fn] type: [SiLU()]\n",
      "idx [128] name: [model.layers.8.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [129] name: [model.layers.8.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [130] name: [model.layers.9] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [131] name: [model.layers.9.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [132] name: [model.layers.9.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [133] name: [model.layers.9.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [134] name: [model.layers.9.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [135] name: [model.layers.9.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [136] name: [model.layers.9.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [137] name: [model.layers.9.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [138] name: [model.layers.9.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [139] name: [model.layers.9.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [140] name: [model.layers.9.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [141] name: [model.layers.9.mlp.act_fn] type: [SiLU()]\n",
      "idx [142] name: [model.layers.9.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [143] name: [model.layers.9.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [144] name: [model.layers.10] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [145] name: [model.layers.10.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [146] name: [model.layers.10.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [147] name: [model.layers.10.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [148] name: [model.layers.10.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [149] name: [model.layers.10.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [150] name: [model.layers.10.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [151] name: [model.layers.10.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [152] name: [model.layers.10.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [153] name: [model.layers.10.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [154] name: [model.layers.10.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [155] name: [model.layers.10.mlp.act_fn] type: [SiLU()]\n",
      "idx [156] name: [model.layers.10.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [157] name: [model.layers.10.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [158] name: [model.layers.11] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [159] name: [model.layers.11.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [160] name: [model.layers.11.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [161] name: [model.layers.11.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [162] name: [model.layers.11.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [163] name: [model.layers.11.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [164] name: [model.layers.11.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [165] name: [model.layers.11.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [166] name: [model.layers.11.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [167] name: [model.layers.11.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [168] name: [model.layers.11.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [169] name: [model.layers.11.mlp.act_fn] type: [SiLU()]\n",
      "idx [170] name: [model.layers.11.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [171] name: [model.layers.11.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [172] name: [model.layers.12] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [173] name: [model.layers.12.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [174] name: [model.layers.12.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [175] name: [model.layers.12.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [176] name: [model.layers.12.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [177] name: [model.layers.12.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [178] name: [model.layers.12.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [179] name: [model.layers.12.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [180] name: [model.layers.12.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [181] name: [model.layers.12.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [182] name: [model.layers.12.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [183] name: [model.layers.12.mlp.act_fn] type: [SiLU()]\n",
      "idx [184] name: [model.layers.12.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [185] name: [model.layers.12.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [186] name: [model.layers.13] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [187] name: [model.layers.13.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [188] name: [model.layers.13.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [189] name: [model.layers.13.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [190] name: [model.layers.13.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [191] name: [model.layers.13.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [192] name: [model.layers.13.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [193] name: [model.layers.13.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [194] name: [model.layers.13.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [195] name: [model.layers.13.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [196] name: [model.layers.13.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [197] name: [model.layers.13.mlp.act_fn] type: [SiLU()]\n",
      "idx [198] name: [model.layers.13.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [199] name: [model.layers.13.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [200] name: [model.layers.14] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [201] name: [model.layers.14.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [202] name: [model.layers.14.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [203] name: [model.layers.14.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [204] name: [model.layers.14.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [205] name: [model.layers.14.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [206] name: [model.layers.14.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [207] name: [model.layers.14.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [208] name: [model.layers.14.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [209] name: [model.layers.14.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [210] name: [model.layers.14.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [211] name: [model.layers.14.mlp.act_fn] type: [SiLU()]\n",
      "idx [212] name: [model.layers.14.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [213] name: [model.layers.14.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [214] name: [model.layers.15] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [215] name: [model.layers.15.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [216] name: [model.layers.15.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [217] name: [model.layers.15.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [218] name: [model.layers.15.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [219] name: [model.layers.15.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [220] name: [model.layers.15.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [221] name: [model.layers.15.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [222] name: [model.layers.15.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [223] name: [model.layers.15.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [224] name: [model.layers.15.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [225] name: [model.layers.15.mlp.act_fn] type: [SiLU()]\n",
      "idx [226] name: [model.layers.15.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [227] name: [model.layers.15.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [228] name: [model.layers.16] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [229] name: [model.layers.16.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [230] name: [model.layers.16.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [231] name: [model.layers.16.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [232] name: [model.layers.16.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [233] name: [model.layers.16.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [234] name: [model.layers.16.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [235] name: [model.layers.16.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [236] name: [model.layers.16.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [237] name: [model.layers.16.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [238] name: [model.layers.16.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [239] name: [model.layers.16.mlp.act_fn] type: [SiLU()]\n",
      "idx [240] name: [model.layers.16.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [241] name: [model.layers.16.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [242] name: [model.layers.17] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [243] name: [model.layers.17.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [244] name: [model.layers.17.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [245] name: [model.layers.17.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [246] name: [model.layers.17.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [247] name: [model.layers.17.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [248] name: [model.layers.17.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [249] name: [model.layers.17.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [250] name: [model.layers.17.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [251] name: [model.layers.17.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [252] name: [model.layers.17.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [253] name: [model.layers.17.mlp.act_fn] type: [SiLU()]\n",
      "idx [254] name: [model.layers.17.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [255] name: [model.layers.17.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [256] name: [model.layers.18] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [257] name: [model.layers.18.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [258] name: [model.layers.18.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [259] name: [model.layers.18.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [260] name: [model.layers.18.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [261] name: [model.layers.18.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [262] name: [model.layers.18.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [263] name: [model.layers.18.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [264] name: [model.layers.18.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [265] name: [model.layers.18.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [266] name: [model.layers.18.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [267] name: [model.layers.18.mlp.act_fn] type: [SiLU()]\n",
      "idx [268] name: [model.layers.18.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [269] name: [model.layers.18.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [270] name: [model.layers.19] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [271] name: [model.layers.19.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [272] name: [model.layers.19.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [273] name: [model.layers.19.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [274] name: [model.layers.19.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [275] name: [model.layers.19.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [276] name: [model.layers.19.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [277] name: [model.layers.19.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [278] name: [model.layers.19.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [279] name: [model.layers.19.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [280] name: [model.layers.19.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [281] name: [model.layers.19.mlp.act_fn] type: [SiLU()]\n",
      "idx [282] name: [model.layers.19.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [283] name: [model.layers.19.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [284] name: [model.layers.20] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [285] name: [model.layers.20.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [286] name: [model.layers.20.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [287] name: [model.layers.20.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [288] name: [model.layers.20.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [289] name: [model.layers.20.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [290] name: [model.layers.20.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [291] name: [model.layers.20.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [292] name: [model.layers.20.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [293] name: [model.layers.20.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [294] name: [model.layers.20.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [295] name: [model.layers.20.mlp.act_fn] type: [SiLU()]\n",
      "idx [296] name: [model.layers.20.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [297] name: [model.layers.20.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [298] name: [model.layers.21] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [299] name: [model.layers.21.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [300] name: [model.layers.21.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [301] name: [model.layers.21.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [302] name: [model.layers.21.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [303] name: [model.layers.21.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [304] name: [model.layers.21.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [305] name: [model.layers.21.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [306] name: [model.layers.21.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [307] name: [model.layers.21.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [308] name: [model.layers.21.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [309] name: [model.layers.21.mlp.act_fn] type: [SiLU()]\n",
      "idx [310] name: [model.layers.21.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [311] name: [model.layers.21.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [312] name: [model.layers.22] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [313] name: [model.layers.22.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [314] name: [model.layers.22.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [315] name: [model.layers.22.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [316] name: [model.layers.22.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [317] name: [model.layers.22.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [318] name: [model.layers.22.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [319] name: [model.layers.22.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [320] name: [model.layers.22.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [321] name: [model.layers.22.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [322] name: [model.layers.22.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [323] name: [model.layers.22.mlp.act_fn] type: [SiLU()]\n",
      "idx [324] name: [model.layers.22.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [325] name: [model.layers.22.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [326] name: [model.layers.23] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [327] name: [model.layers.23.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [328] name: [model.layers.23.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [329] name: [model.layers.23.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [330] name: [model.layers.23.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [331] name: [model.layers.23.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [332] name: [model.layers.23.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [333] name: [model.layers.23.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [334] name: [model.layers.23.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [335] name: [model.layers.23.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [336] name: [model.layers.23.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [337] name: [model.layers.23.mlp.act_fn] type: [SiLU()]\n",
      "idx [338] name: [model.layers.23.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [339] name: [model.layers.23.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [340] name: [model.layers.24] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [341] name: [model.layers.24.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [342] name: [model.layers.24.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [343] name: [model.layers.24.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [344] name: [model.layers.24.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [345] name: [model.layers.24.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [346] name: [model.layers.24.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [347] name: [model.layers.24.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [348] name: [model.layers.24.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [349] name: [model.layers.24.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [350] name: [model.layers.24.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [351] name: [model.layers.24.mlp.act_fn] type: [SiLU()]\n",
      "idx [352] name: [model.layers.24.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [353] name: [model.layers.24.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [354] name: [model.layers.25] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [355] name: [model.layers.25.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [356] name: [model.layers.25.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [357] name: [model.layers.25.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [358] name: [model.layers.25.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [359] name: [model.layers.25.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [360] name: [model.layers.25.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [361] name: [model.layers.25.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [362] name: [model.layers.25.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [363] name: [model.layers.25.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [364] name: [model.layers.25.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [365] name: [model.layers.25.mlp.act_fn] type: [SiLU()]\n",
      "idx [366] name: [model.layers.25.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [367] name: [model.layers.25.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [368] name: [model.layers.26] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [369] name: [model.layers.26.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [370] name: [model.layers.26.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [371] name: [model.layers.26.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [372] name: [model.layers.26.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [373] name: [model.layers.26.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [374] name: [model.layers.26.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [375] name: [model.layers.26.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [376] name: [model.layers.26.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [377] name: [model.layers.26.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [378] name: [model.layers.26.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [379] name: [model.layers.26.mlp.act_fn] type: [SiLU()]\n",
      "idx [380] name: [model.layers.26.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [381] name: [model.layers.26.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [382] name: [model.layers.27] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [383] name: [model.layers.27.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [384] name: [model.layers.27.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [385] name: [model.layers.27.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [386] name: [model.layers.27.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [387] name: [model.layers.27.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [388] name: [model.layers.27.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [389] name: [model.layers.27.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [390] name: [model.layers.27.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [391] name: [model.layers.27.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [392] name: [model.layers.27.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [393] name: [model.layers.27.mlp.act_fn] type: [SiLU()]\n",
      "idx [394] name: [model.layers.27.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [395] name: [model.layers.27.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [396] name: [model.layers.28] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [397] name: [model.layers.28.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [398] name: [model.layers.28.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [399] name: [model.layers.28.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [400] name: [model.layers.28.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [401] name: [model.layers.28.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [402] name: [model.layers.28.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [403] name: [model.layers.28.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [404] name: [model.layers.28.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [405] name: [model.layers.28.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [406] name: [model.layers.28.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [407] name: [model.layers.28.mlp.act_fn] type: [SiLU()]\n",
      "idx [408] name: [model.layers.28.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [409] name: [model.layers.28.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [410] name: [model.layers.29] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [411] name: [model.layers.29.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [412] name: [model.layers.29.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [413] name: [model.layers.29.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [414] name: [model.layers.29.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [415] name: [model.layers.29.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [416] name: [model.layers.29.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [417] name: [model.layers.29.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [418] name: [model.layers.29.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [419] name: [model.layers.29.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [420] name: [model.layers.29.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [421] name: [model.layers.29.mlp.act_fn] type: [SiLU()]\n",
      "idx [422] name: [model.layers.29.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [423] name: [model.layers.29.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [424] name: [model.layers.30] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [425] name: [model.layers.30.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [426] name: [model.layers.30.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [427] name: [model.layers.30.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [428] name: [model.layers.30.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [429] name: [model.layers.30.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [430] name: [model.layers.30.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [431] name: [model.layers.30.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [432] name: [model.layers.30.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [433] name: [model.layers.30.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [434] name: [model.layers.30.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [435] name: [model.layers.30.mlp.act_fn] type: [SiLU()]\n",
      "idx [436] name: [model.layers.30.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [437] name: [model.layers.30.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [438] name: [model.layers.31] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [439] name: [model.layers.31.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [440] name: [model.layers.31.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [441] name: [model.layers.31.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [442] name: [model.layers.31.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [443] name: [model.layers.31.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [444] name: [model.layers.31.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [445] name: [model.layers.31.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [446] name: [model.layers.31.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [447] name: [model.layers.31.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [448] name: [model.layers.31.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [449] name: [model.layers.31.mlp.act_fn] type: [SiLU()]\n",
      "idx [450] name: [model.layers.31.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [451] name: [model.layers.31.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [452] name: [model.layers.32] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [453] name: [model.layers.32.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [454] name: [model.layers.32.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [455] name: [model.layers.32.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [456] name: [model.layers.32.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [457] name: [model.layers.32.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [458] name: [model.layers.32.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [459] name: [model.layers.32.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [460] name: [model.layers.32.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [461] name: [model.layers.32.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [462] name: [model.layers.32.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [463] name: [model.layers.32.mlp.act_fn] type: [SiLU()]\n",
      "idx [464] name: [model.layers.32.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [465] name: [model.layers.32.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [466] name: [model.layers.33] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [467] name: [model.layers.33.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [468] name: [model.layers.33.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [469] name: [model.layers.33.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [470] name: [model.layers.33.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [471] name: [model.layers.33.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [472] name: [model.layers.33.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [473] name: [model.layers.33.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [474] name: [model.layers.33.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [475] name: [model.layers.33.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [476] name: [model.layers.33.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [477] name: [model.layers.33.mlp.act_fn] type: [SiLU()]\n",
      "idx [478] name: [model.layers.33.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [479] name: [model.layers.33.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [480] name: [model.layers.34] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [481] name: [model.layers.34.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [482] name: [model.layers.34.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [483] name: [model.layers.34.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [484] name: [model.layers.34.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [485] name: [model.layers.34.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [486] name: [model.layers.34.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [487] name: [model.layers.34.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [488] name: [model.layers.34.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [489] name: [model.layers.34.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [490] name: [model.layers.34.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [491] name: [model.layers.34.mlp.act_fn] type: [SiLU()]\n",
      "idx [492] name: [model.layers.34.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [493] name: [model.layers.34.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [494] name: [model.layers.35] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [495] name: [model.layers.35.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [496] name: [model.layers.35.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [497] name: [model.layers.35.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [498] name: [model.layers.35.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [499] name: [model.layers.35.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [500] name: [model.layers.35.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [501] name: [model.layers.35.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [502] name: [model.layers.35.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [503] name: [model.layers.35.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [504] name: [model.layers.35.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [505] name: [model.layers.35.mlp.act_fn] type: [SiLU()]\n",
      "idx [506] name: [model.layers.35.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [507] name: [model.layers.35.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [508] name: [model.layers.36] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [509] name: [model.layers.36.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [510] name: [model.layers.36.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [511] name: [model.layers.36.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [512] name: [model.layers.36.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [513] name: [model.layers.36.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [514] name: [model.layers.36.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [515] name: [model.layers.36.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [516] name: [model.layers.36.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [517] name: [model.layers.36.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [518] name: [model.layers.36.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [519] name: [model.layers.36.mlp.act_fn] type: [SiLU()]\n",
      "idx [520] name: [model.layers.36.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [521] name: [model.layers.36.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [522] name: [model.layers.37] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [523] name: [model.layers.37.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [524] name: [model.layers.37.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [525] name: [model.layers.37.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [526] name: [model.layers.37.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [527] name: [model.layers.37.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [528] name: [model.layers.37.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [529] name: [model.layers.37.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [530] name: [model.layers.37.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [531] name: [model.layers.37.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [532] name: [model.layers.37.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [533] name: [model.layers.37.mlp.act_fn] type: [SiLU()]\n",
      "idx [534] name: [model.layers.37.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [535] name: [model.layers.37.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [536] name: [model.layers.38] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [537] name: [model.layers.38.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [538] name: [model.layers.38.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [539] name: [model.layers.38.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [540] name: [model.layers.38.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [541] name: [model.layers.38.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [542] name: [model.layers.38.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [543] name: [model.layers.38.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [544] name: [model.layers.38.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [545] name: [model.layers.38.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [546] name: [model.layers.38.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [547] name: [model.layers.38.mlp.act_fn] type: [SiLU()]\n",
      "idx [548] name: [model.layers.38.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [549] name: [model.layers.38.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [550] name: [model.layers.39] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [551] name: [model.layers.39.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [552] name: [model.layers.39.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [553] name: [model.layers.39.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [554] name: [model.layers.39.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [555] name: [model.layers.39.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [556] name: [model.layers.39.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [557] name: [model.layers.39.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [558] name: [model.layers.39.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [559] name: [model.layers.39.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [560] name: [model.layers.39.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [561] name: [model.layers.39.mlp.act_fn] type: [SiLU()]\n",
      "idx [562] name: [model.layers.39.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [563] name: [model.layers.39.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [564] name: [model.layers.40] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [565] name: [model.layers.40.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [566] name: [model.layers.40.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [567] name: [model.layers.40.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [568] name: [model.layers.40.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [569] name: [model.layers.40.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [570] name: [model.layers.40.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [571] name: [model.layers.40.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [572] name: [model.layers.40.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [573] name: [model.layers.40.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [574] name: [model.layers.40.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [575] name: [model.layers.40.mlp.act_fn] type: [SiLU()]\n",
      "idx [576] name: [model.layers.40.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [577] name: [model.layers.40.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [578] name: [model.layers.41] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [579] name: [model.layers.41.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [580] name: [model.layers.41.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [581] name: [model.layers.41.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [582] name: [model.layers.41.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [583] name: [model.layers.41.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [584] name: [model.layers.41.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [585] name: [model.layers.41.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [586] name: [model.layers.41.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [587] name: [model.layers.41.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [588] name: [model.layers.41.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [589] name: [model.layers.41.mlp.act_fn] type: [SiLU()]\n",
      "idx [590] name: [model.layers.41.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [591] name: [model.layers.41.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [592] name: [model.layers.42] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [593] name: [model.layers.42.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [594] name: [model.layers.42.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [595] name: [model.layers.42.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [596] name: [model.layers.42.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [597] name: [model.layers.42.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [598] name: [model.layers.42.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [599] name: [model.layers.42.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [600] name: [model.layers.42.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [601] name: [model.layers.42.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [602] name: [model.layers.42.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [603] name: [model.layers.42.mlp.act_fn] type: [SiLU()]\n",
      "idx [604] name: [model.layers.42.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [605] name: [model.layers.42.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [606] name: [model.layers.43] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [607] name: [model.layers.43.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [608] name: [model.layers.43.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [609] name: [model.layers.43.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [610] name: [model.layers.43.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [611] name: [model.layers.43.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [612] name: [model.layers.43.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [613] name: [model.layers.43.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [614] name: [model.layers.43.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [615] name: [model.layers.43.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [616] name: [model.layers.43.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [617] name: [model.layers.43.mlp.act_fn] type: [SiLU()]\n",
      "idx [618] name: [model.layers.43.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [619] name: [model.layers.43.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [620] name: [model.layers.44] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [621] name: [model.layers.44.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [622] name: [model.layers.44.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [623] name: [model.layers.44.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [624] name: [model.layers.44.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [625] name: [model.layers.44.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [626] name: [model.layers.44.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [627] name: [model.layers.44.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [628] name: [model.layers.44.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [629] name: [model.layers.44.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [630] name: [model.layers.44.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [631] name: [model.layers.44.mlp.act_fn] type: [SiLU()]\n",
      "idx [632] name: [model.layers.44.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [633] name: [model.layers.44.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [634] name: [model.layers.45] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [635] name: [model.layers.45.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [636] name: [model.layers.45.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [637] name: [model.layers.45.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [638] name: [model.layers.45.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [639] name: [model.layers.45.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [640] name: [model.layers.45.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [641] name: [model.layers.45.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [642] name: [model.layers.45.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [643] name: [model.layers.45.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [644] name: [model.layers.45.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [645] name: [model.layers.45.mlp.act_fn] type: [SiLU()]\n",
      "idx [646] name: [model.layers.45.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [647] name: [model.layers.45.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [648] name: [model.layers.46] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [649] name: [model.layers.46.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [650] name: [model.layers.46.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [651] name: [model.layers.46.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [652] name: [model.layers.46.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [653] name: [model.layers.46.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [654] name: [model.layers.46.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [655] name: [model.layers.46.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [656] name: [model.layers.46.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [657] name: [model.layers.46.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [658] name: [model.layers.46.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [659] name: [model.layers.46.mlp.act_fn] type: [SiLU()]\n",
      "idx [660] name: [model.layers.46.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [661] name: [model.layers.46.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [662] name: [model.layers.47] type: [LlamaDecoderLayer(\n",
      "  (self_attn): LlamaFlashAttention2(\n",
      "    (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")]\n",
      "idx [663] name: [model.layers.47.self_attn] type: [LlamaFlashAttention2(\n",
      "  (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")]\n",
      "idx [664] name: [model.layers.47.self_attn.q_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [665] name: [model.layers.47.self_attn.k_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [666] name: [model.layers.47.self_attn.v_proj] type: [Linear4bit(in_features=8192, out_features=1024, bias=False)]\n",
      "idx [667] name: [model.layers.47.self_attn.o_proj] type: [Linear4bit(in_features=8192, out_features=8192, bias=False)]\n",
      "idx [668] name: [model.layers.47.self_attn.rotary_emb] type: [LlamaRotaryEmbedding()]\n",
      "idx [669] name: [model.layers.47.mlp] type: [LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")]\n",
      "idx [670] name: [model.layers.47.mlp.gate_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [671] name: [model.layers.47.mlp.up_proj] type: [Linear4bit(in_features=8192, out_features=22016, bias=False)]\n",
      "idx [672] name: [model.layers.47.mlp.down_proj] type: [Linear4bit(in_features=22016, out_features=8192, bias=False)]\n",
      "idx [673] name: [model.layers.47.mlp.act_fn] type: [SiLU()]\n",
      "idx [674] name: [model.layers.47.input_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [675] name: [model.layers.47.post_attention_layernorm] type: [LlamaRMSNorm()]\n",
      "idx [676] name: [model.norm] type: [LlamaRMSNorm()]\n",
      "idx [677] name: [lm_head] type: [Linear(in_features=8192, out_features=32000, bias=False)]\n"
     ]
    }
   ],
   "source": [
    "modules = base_model.named_modules()\n",
    "for idx, module in enumerate( modules ):\n",
    "    print( f\"idx [{idx}] name: [{module[ 0 ]}] type: [{module[ 1 ]}]\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T18:31:54.255434Z",
     "start_time": "2024-01-12T18:31:54.211878Z"
    }
   },
   "id": "a1674c98d71838c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "def load_adapter( model, adapter_path, use_flash_attention_2=True ):\n",
    "    \n",
    "    return PeftModel.from_pretrained( model, adapter_path, use_flash_attention_2=use_flash_attention_2 )\n",
    "\n",
    "adapter_plus_model = load_adapter( base_model, \"adapters/01-browser-vox-command\", use_flash_attention_2=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:16:01.952095Z",
     "start_time": "2024-01-18T03:16:00.433205Z"
    }
   },
   "id": "34796358a34b99c6"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32000, 8192)\n        (layers): ModuleList(\n          (0-47): 48 x LlamaDecoderLayer(\n            (self_attn): LlamaFlashAttention2(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=22016, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=22016, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=22016, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=22016, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=22016, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=22016, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm()\n            (post_attention_layernorm): LlamaRMSNorm()\n          )\n        )\n        (norm): LlamaRMSNorm()\n      )\n      (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n    )\n  )\n)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_plus_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T19:33:52.794068Z",
     "start_time": "2024-01-12T19:33:52.776734Z"
    }
   },
   "id": "ea35c725b74c782a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.32.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.33.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.34.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.35.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.36.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.37.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.38.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.39.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.40.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.41.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.42.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.43.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.44.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.45.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.46.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.47.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.norm.weight: cuda:1\n",
      "base_model.model.lm_head.weight: cuda:1\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "dupt.print_device_allocation( adapter_plus_model )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:16:18.253831Z",
     "start_time": "2024-01-18T03:16:18.174426Z"
    }
   },
   "id": "4a4d57dcf172749f"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload\n",
    "dupt.is_allocated_to_cpu( adapter_plus_model )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:16:23.780615Z",
     "start_time": "2024-01-18T03:16:23.727066Z"
    }
   },
   "id": "d6f882c31d9ae794"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NaÃ¯vely merge model and adapter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5e2ca02755846c8"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:257: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "adapter_plus_model = adapter_plus_model.merge_and_unload()\n",
    "adapter_plus_model.save_pretrained( \"./merged-naive/\", safe_serialization=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:24:15.209810Z",
     "start_time": "2024-01-18T03:23:12.855106Z"
    }
   },
   "id": "7cb0f9915bf83e22"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "'4.37.0.dev0'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:41:22.306811Z",
     "start_time": "2024-01-18T03:41:22.260911Z"
    }
   },
   "id": "49314f23cc0a03e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dequantize and then merge  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba85999c167bcc9"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dequantizing model ...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- base_model: PRE dequantize\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 8192)\n",
      "    (layers): ModuleList(\n",
      "      (0-47): 48 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaFlashAttention2(\n",
      "          (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
      ")\n",
      "Skipping ``...\n",
      "Skipping `model`...\n",
      "Skipping `model.embed_tokens`...\n",
      "Skipping `model.layers`...\n",
      "Skipping `model.layers.0`...\n",
      "Skipping `model.layers.0.self_attn`...\n",
      "Dequantizing `model.layers.0.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([230, 165,  88,  ..., 193, 174, 202], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0348, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00854f40>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00854e20>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.0.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([255, 204, 208,  ..., 212,  72,  93], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0782, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc008550f0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00855840>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.0.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([43, 24, 59,  ..., 38, 54, 22], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0311, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00855210>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00854f40>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.0.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([204, 210, 178,  ..., 183,  74,  91], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0248, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00855390>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc008550f0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.0.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.0.mlp`...\n",
      "Dequantizing `model.layers.0.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 57, 206,  51,  ..., 212,  62,  45], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0349, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00855540>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc008553f0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.0.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([100,  58,  55,  ..., 190, 170, 160], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0346, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00855630>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc008552d0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.0.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([203, 205,  56,  ...,  54,  60, 202], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0417, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff617b20>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00855570>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.0.mlp.act_fn`...\n",
      "Skipping `model.layers.0.input_layernorm`...\n",
      "Skipping `model.layers.0.post_attention_layernorm`...\n",
      "Skipping `model.layers.1`...\n",
      "Skipping `model.layers.1.self_attn`...\n",
      "Dequantizing `model.layers.1.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([14, 19, 15,  ..., 11, 13, 23], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0438, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff617a60>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00855330>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.1.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([15, 25, 21,  ..., 50, 51, 59], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0795, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff6170d0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff617610>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.1.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([50, 91, 54,  ..., 31, 38, 18], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0312, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff617340>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff617a60>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.1.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([39, 49, 52,  ..., 73, 68, 84], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0315, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff6171f0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff617610>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.1.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.1.mlp`...\n",
      "Dequantizing `model.layers.1.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 55,  86,  44,  ..., 205,  60, 190], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0436, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff617bb0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff617580>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.1.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 32, 196, 162,  ...,  57,  43,  58], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0433, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff616cb0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff617b50>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.1.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 46, 189, 193,  ..., 165, 199,  44], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0466, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00e5fa30>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff616860>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.1.mlp.act_fn`...\n",
      "Skipping `model.layers.1.input_layernorm`...\n",
      "Skipping `model.layers.1.post_attention_layernorm`...\n",
      "Skipping `model.layers.2`...\n",
      "Skipping `model.layers.2.self_attn`...\n",
      "Dequantizing `model.layers.2.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 38,  23,  60,  ..., 192, 181, 170], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0514, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00e5fcd0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff616b90>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.2.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 61,  61,  61,  ..., 219, 209, 203], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0892, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00e5fa90>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00e5fca0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.2.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([234, 161, 152,  ...,  49,  40,  43], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0343, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00a38760>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00e5fcd0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.2.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([169, 197, 205,  ..., 165,  94, 147], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0411, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00a3b070>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00e5fca0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.2.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.2.mlp`...\n",
      "Dequantizing `model.layers.2.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([200,  48,  52,  ...,  62, 220,  87], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0475, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00a38790>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00a39f60>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.2.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([181,  54,  47,  ..., 194,  61, 167], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0463, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00a3bd90>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00a3a8c0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.2.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([170, 190, 197,  ...,  53,  55,  54], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0481, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00a39870>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00a3b4f0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.2.mlp.act_fn`...\n",
      "Skipping `model.layers.2.input_layernorm`...\n",
      "Skipping `model.layers.2.post_attention_layernorm`...\n",
      "Skipping `model.layers.3`...\n",
      "Skipping `model.layers.3.self_attn`...\n",
      "Dequantizing `model.layers.3.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 85,  86,  91,  ..., 211,  60, 240], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0501, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00a388b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00a382e0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.3.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([41, 50, 49,  ..., 57, 65, 73], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0838, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00a3bc10>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00a3bf70>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.3.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([191, 174,  72,  ..., 206, 143, 196], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0351, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc00a3a230>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00a388b0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.3.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([173, 234,  58,  ...,  76,  94,  86], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0411, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdc500122c0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00a3bc10>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.3.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.3.mlp`...\n",
      "Dequantizing `model.layers.3.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 57,  48,  58,  ...,  49,  46, 152], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0489, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff610160>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00a3a470>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.3.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 52, 194,  56,  ...,  75,  54, 200], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0474, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff6102b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdc00a3a800>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.3.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([177, 191, 211,  ..., 169, 207,  57], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0483, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff610460>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff610310>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.3.mlp.act_fn`...\n",
      "Skipping `model.layers.3.input_layernorm`...\n",
      "Skipping `model.layers.3.post_attention_layernorm`...\n",
      "Skipping `model.layers.4`...\n",
      "Skipping `model.layers.4.self_attn`...\n",
      "Dequantizing `model.layers.4.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 54,  45,  57,  ..., 236, 227, 223], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0508, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff610610>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff6102e0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.4.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 49,  28,  30,  ..., 195, 201, 192], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0775, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff610730>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff610670>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.4.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 54,  80, 187,  ..., 195,  61,  86], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0377, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff6108b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff610610>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.4.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 60,  88, 226,  ..., 137, 164, 192], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0429, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff610a30>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff610730>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.4.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.4.mlp`...\n",
      "Dequantizing `model.layers.4.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 50,  30,  49,  ..., 196,  55, 174], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0490, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff610bb0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff610a90>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.4.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([180,  74, 198,  ...,  57, 238,  38], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0475, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff610d30>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff610c10>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.4.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([201,  71,  77,  ...,  43, 197, 196], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0484, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff610ee0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff610d90>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.4.mlp.act_fn`...\n",
      "Skipping `model.layers.4.input_layernorm`...\n",
      "Skipping `model.layers.4.post_attention_layernorm`...\n",
      "Skipping `model.layers.5`...\n",
      "Skipping `model.layers.5.self_attn`...\n",
      "Dequantizing `model.layers.5.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 15,  15,  24,  ..., 208, 204, 204], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0492, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff610e50>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff610f40>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.5.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 12,   6,  27,  ..., 193, 194, 197], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0700, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff6111b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff611090>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.5.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 62,  61, 207,  ..., 105,  42, 156], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0362, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff611330>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff610e50>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.5.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([180, 225,  61,  ..., 181,  69,  76], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0413, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff6114b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff6111b0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.5.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.5.mlp`...\n",
      "Dequantizing `model.layers.5.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([178,  58,  60,  ..., 150, 193, 213], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0494, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff611630>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff611510>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.5.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([160,  90,  87,  ...,  88,  95,  51], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0477, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff6117b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff611690>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.5.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 87, 191,  60,  ..., 208,  58,  50], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0483, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff611930>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff611810>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.5.mlp.act_fn`...\n",
      "Skipping `model.layers.5.input_layernorm`...\n",
      "Skipping `model.layers.5.post_attention_layernorm`...\n",
      "Skipping `model.layers.6`...\n",
      "Skipping `model.layers.6.self_attn`...\n",
      "Dequantizing `model.layers.6.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 13,  15,  17,  ..., 206, 241, 217], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0498, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff611ab0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff611990>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.6.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 15,  29,   7,  ..., 204, 198, 192], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0701, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff611c30>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff611b10>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.6.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 62,  61,  59,  ...,  40, 248,  62], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0369, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff611db0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff611ab0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.6.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 61, 207,  42,  ..., 196,  82,  82], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0422, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff611f30>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff611c30>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.6.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.6.mlp`...\n",
      "Dequantizing `model.layers.6.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 58, 209, 205,  ..., 201, 191,  92], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0496, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff6120b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff611f90>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.6.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 23, 194,  56,  ..., 161, 218, 210], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0479, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff611f60>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff612110>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.6.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([209, 190, 107,  ...,  94,  61,  57], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0484, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff612410>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff612320>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.6.mlp.act_fn`...\n",
      "Skipping `model.layers.6.input_layernorm`...\n",
      "Skipping `model.layers.6.post_attention_layernorm`...\n",
      "Skipping `model.layers.7`...\n",
      "Skipping `model.layers.7.self_attn`...\n",
      "Dequantizing `model.layers.7.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 38,  12,  20,  ..., 194, 204, 194], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0495, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff612590>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff612470>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.7.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 40,  48,  53,  ..., 191, 187, 183], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0705, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff612740>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff6125f0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.7.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([221, 197,  68,  ...,  63,  55, 174], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0404, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff6126b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff612590>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.7.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([212, 237, 210,  ...,  90,  97, 163], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0440, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff612a10>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff612740>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.7.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.7.mlp`...\n",
      "Dequantizing `model.layers.7.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([204,  82,  61,  ...,  54, 193,  27], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0499, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff612bc0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff612a70>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.7.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([216, 165,  72,  ..., 202,  85, 211], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0481, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff612b30>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff612a40>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.7.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([167,  56, 214,  ..., 141,  77, 192], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0485, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff612e90>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff612d70>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.7.mlp.act_fn`...\n",
      "Skipping `model.layers.7.input_layernorm`...\n",
      "Skipping `model.layers.7.post_attention_layernorm`...\n",
      "Skipping `model.layers.8`...\n",
      "Skipping `model.layers.8.self_attn`...\n",
      "Dequantizing `model.layers.8.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 31,  47,   6,  ..., 169, 205, 167], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0504, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff613010>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff612ef0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.8.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 16,  41,  13,  ..., 199,  77,  90], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0692, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff6131c0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff613070>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.8.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([199, 173,  51,  ..., 193, 174, 194], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0376, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff613130>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff613010>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.8.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([201,  94, 186,  ..., 192, 159,  63], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0438, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff613490>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff6131c0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.8.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.8.mlp`...\n",
      "Dequantizing `model.layers.8.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 53,  63, 201,  ...,  60,  60,  54], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0499, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff613640>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff6134f0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.8.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 59,  59,  38,  ...,  62, 194, 191], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0482, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff6135b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff6136a0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.8.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 74,  52, 214,  ..., 181, 181,  60], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0486, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff613910>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff6137f0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.8.mlp.act_fn`...\n",
      "Skipping `model.layers.8.input_layernorm`...\n",
      "Skipping `model.layers.8.post_attention_layernorm`...\n",
      "Skipping `model.layers.9`...\n",
      "Skipping `model.layers.9.self_attn`...\n",
      "Dequantizing `model.layers.9.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([194,  59, 179,  ...,  86, 168, 204], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0508, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff613a90>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff613970>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.9.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 33, 198,  38,  ..., 177, 191, 179], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0706, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff613c10>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff613af0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.9.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 57,  55,  61,  ...,  63, 181,  55], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0404, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff613d90>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff613a90>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.9.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 62,  61,  58,  ...,  58, 192, 192], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0447, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff613f10>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff613c10>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.9.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.9.mlp`...\n",
      "Dequantizing `model.layers.9.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 32,  68, 202,  ...,  30,  52, 186], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0500, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f0100>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff613e50>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.9.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 83,  62,  91,  ...,  38,  53, 205], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0484, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f0250>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff613fd0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.9.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([249, 175, 209,  ...,  91,  59,  45], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0487, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f03d0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f02b0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.9.mlp.act_fn`...\n",
      "Skipping `model.layers.9.input_layernorm`...\n",
      "Skipping `model.layers.9.post_attention_layernorm`...\n",
      "Skipping `model.layers.10`...\n",
      "Skipping `model.layers.10.self_attn`...\n",
      "Dequantizing `model.layers.10.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 49,  31,  45,  ..., 217, 165, 196], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0502, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f0550>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f0430>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.10.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 38,  15,  14,  ..., 201, 194, 196], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0680, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f06d0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f05b0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.10.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 91, 167,  53,  ..., 207,  76,  51], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0384, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f0850>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f0550>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.10.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([215, 160,  79,  ..., 196,  62,  52], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0446, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f09d0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f06d0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.10.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.10.mlp`...\n",
      "Dequantizing `model.layers.10.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 54,  58,  48,  ...,  84, 172, 211], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0499, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f0b80>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f0a30>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.10.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 36, 255,  58,  ...,  81,  87,  50], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0484, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f0cd0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f0910>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.10.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([212, 195,  71,  ...,  52, 198, 203], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0486, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f0e50>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f0d30>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.10.mlp.act_fn`...\n",
      "Skipping `model.layers.10.input_layernorm`...\n",
      "Skipping `model.layers.10.post_attention_layernorm`...\n",
      "Skipping `model.layers.11`...\n",
      "Skipping `model.layers.11.self_attn`...\n",
      "Dequantizing `model.layers.11.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 37,  22,  15,  ..., 168, 186, 211], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0504, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f0fd0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f0eb0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.11.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 18,  19,  17,  ..., 171, 178, 108], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0698, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f1150>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f1030>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.11.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([198,  84, 239,  ...,  63,  87,  92], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0401, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f12d0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f0fd0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.11.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([174, 193, 213,  ..., 151,  61,  73], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0453, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f1450>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f1150>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.11.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.11.mlp`...\n",
      "Dequantizing `model.layers.11.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 57,  60,  43,  ...,  87, 171,  56], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0496, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f15d0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f14b0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.11.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 61, 106,  59,  ..., 174, 191,  58], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0482, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f1780>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f1630>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.11.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 58,  59, 186,  ..., 197, 203,  56], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0487, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f16f0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f1600>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.11.mlp.act_fn`...\n",
      "Skipping `model.layers.11.input_layernorm`...\n",
      "Skipping `model.layers.11.post_attention_layernorm`...\n",
      "Skipping `model.layers.12`...\n",
      "Skipping `model.layers.12.self_attn`...\n",
      "Dequantizing `model.layers.12.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 48,  33,  49,  ..., 208, 201, 216], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0488, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f1a50>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f1930>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.12.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 14,  49,  35,  ..., 198, 210, 197], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0688, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f1c00>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f1ab0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.12.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 53, 193,  90,  ...,  20, 169,  64], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0415, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f1b70>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f1a50>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.12.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([192,  77, 198,  ...,  67,  82,  73], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0452, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f1ed0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f1c00>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.12.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.12.mlp`...\n",
      "Dequantizing `model.layers.12.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([176,  50,  98,  ...,  35,  20, 220], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0495, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f2080>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f1f30>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.12.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 70,  61,  53,  ..., 219,  78, 195], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0484, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f1ff0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f20e0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.12.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([181, 186, 194,  ...,  57,  61,  39], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0488, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f2350>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f2230>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.12.mlp.act_fn`...\n",
      "Skipping `model.layers.12.input_layernorm`...\n",
      "Skipping `model.layers.12.post_attention_layernorm`...\n",
      "Skipping `model.layers.13`...\n",
      "Skipping `model.layers.13.self_attn`...\n",
      "Dequantizing `model.layers.13.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 54,  47,  52,  ..., 243, 203, 207], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0482, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f24d0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f23b0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.13.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 15,  32,  26,  ..., 198, 213, 223], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0657, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f2680>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f2530>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.13.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([213,  74, 197,  ...,  51,  39,  41], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0420, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f27d0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f24d0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.13.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 77,  62, 172,  ...,  69, 211, 208], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0454, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f2950>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f2680>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.13.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.13.mlp`...\n",
      "Dequantizing `model.layers.13.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 27,  50, 206,  ..., 226, 104, 182], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0495, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f2b00>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f29b0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.13.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([184, 173,  89,  ..., 199,  77, 241], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0485, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f2c50>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f2bc0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.13.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([182, 222,  40,  ..., 195, 221,  43], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0489, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f2dd0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f2cb0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.13.mlp.act_fn`...\n",
      "Skipping `model.layers.13.input_layernorm`...\n",
      "Skipping `model.layers.13.post_attention_layernorm`...\n",
      "Skipping `model.layers.14`...\n",
      "Skipping `model.layers.14.self_attn`...\n",
      "Dequantizing `model.layers.14.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 44,  42,  38,  ..., 212, 191, 204], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0484, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f2f50>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f2e30>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.14.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 12,  16,  21,  ..., 217, 205, 222], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0658, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f3100>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f2fb0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.14.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([191,  46,  45,  ...,  46,  39,  24], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0417, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f3250>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f2f50>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.14.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([211,  83, 187,  ...,  54,  63, 169], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0455, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f33d0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f3100>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.14.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.14.mlp`...\n",
      "Dequantizing `model.layers.14.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 57,  92,  75,  ...,  60, 207,  60], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0494, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f3580>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f3430>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.14.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([192, 197,  70,  ...,  55,  40,  87], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0487, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f36d0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f3310>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.14.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([200,  42,  62,  ...,  46, 194, 193], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0491, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f3850>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f3730>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.14.mlp.act_fn`...\n",
      "Skipping `model.layers.14.input_layernorm`...\n",
      "Skipping `model.layers.14.post_attention_layernorm`...\n",
      "Skipping `model.layers.15`...\n",
      "Skipping `model.layers.15.self_attn`...\n",
      "Dequantizing `model.layers.15.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 32,  39,  51,  ..., 198,  55, 194], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0468, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f3a00>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f38b0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.15.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 17,  22,  15,  ..., 207, 224, 203], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0652, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f3b50>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f3700>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.15.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 59,  21,  32,  ...,  38, 218,  94], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0440, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f3cd0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f3a00>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.15.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 89, 199, 214,  ...,  51,  47, 186], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0463, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f3e50>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f3b50>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.15.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.15.mlp`...\n",
      "Dequantizing `model.layers.15.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([171,  66,  63,  ..., 192,  57,  57], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0493, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbff5f3e80>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f3eb0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.15.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 72, 160, 180,  ...,  85, 194,  47], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0486, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd4040>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbff5f3d90>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.15.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([229,  62, 205,  ...,  82,  60, 226], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0491, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd4310>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd4100>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.15.mlp.act_fn`...\n",
      "Skipping `model.layers.15.input_layernorm`...\n",
      "Skipping `model.layers.15.post_attention_layernorm`...\n",
      "Skipping `model.layers.16`...\n",
      "Skipping `model.layers.16.self_attn`...\n",
      "Dequantizing `model.layers.16.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 48,  30,  22,  ..., 186, 161, 177], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0478, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd4490>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd4370>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.16.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 27,   8,  10,  ..., 172, 176, 171], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0635, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd4640>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd44f0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.16.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 49, 212,  26,  ..., 192, 190,  36], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0441, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd4790>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd4490>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.16.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([204, 193,  83,  ..., 209, 169,  58], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0461, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd4910>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd4640>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.16.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.16.mlp`...\n",
      "Dequantizing `model.layers.16.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 50,  57,  60,  ...,  58, 206, 176], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0492, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd4ac0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd4970>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.16.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([219,  41,  42,  ..., 223,  60,  42], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0488, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd4a30>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd4b20>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.16.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 95,  83, 220,  ...,  52, 175, 169], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0492, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd4d90>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd4c70>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.16.mlp.act_fn`...\n",
      "Skipping `model.layers.16.input_layernorm`...\n",
      "Skipping `model.layers.16.post_attention_layernorm`...\n",
      "Skipping `model.layers.17`...\n",
      "Skipping `model.layers.17.self_attn`...\n",
      "Dequantizing `model.layers.17.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 42,  39,  42,  ..., 171,  50, 193], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0466, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd4f10>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd4df0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.17.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 28,  13,  28,  ..., 192, 197, 170], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0649, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd5090>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd4f70>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.17.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 60, 207, 163,  ..., 195, 205, 196], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0454, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd5210>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd4f10>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.17.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([191, 174, 212,  ..., 169,  97, 197], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0464, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd5390>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd5090>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.17.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.17.mlp`...\n",
      "Dequantizing `model.layers.17.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 50, 211, 181,  ...,  51, 192, 116], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0491, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd5540>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd53f0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.17.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([176,  41,  86,  ...,  51,  49, 175], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0488, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd54b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd53c0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.17.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([202, 214,  70,  ..., 173,  51,  63], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0493, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd5810>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd56f0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.17.mlp.act_fn`...\n",
      "Skipping `model.layers.17.input_layernorm`...\n",
      "Skipping `model.layers.17.post_attention_layernorm`...\n",
      "Skipping `model.layers.18`...\n",
      "Skipping `model.layers.18.self_attn`...\n",
      "Dequantizing `model.layers.18.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 27,  13,  26,  ..., 238, 216, 230], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0463, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd5990>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd5870>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.18.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 20,  19,  21,  ...,  92, 196, 190], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0666, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd5b10>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd59f0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.18.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([202,  52,  38,  ...,  19,  22,  16], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0459, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd5c90>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd5990>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.18.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 68,  56,  53,  ..., 212,  61,  58], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0464, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd5e10>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd5b10>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.18.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.18.mlp`...\n",
      "Dequantizing `model.layers.18.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([196,  60, 179,  ...,  53,  41,  53], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0491, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd5fc0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd5e70>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.18.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([171, 192,  58,  ..., 196,  59, 202], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0488, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd5f30>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd5e40>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.18.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([174,  64, 177,  ...,  77,  80, 255], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0493, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd6290>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd6170>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.18.mlp.act_fn`...\n",
      "Skipping `model.layers.18.input_layernorm`...\n",
      "Skipping `model.layers.18.post_attention_layernorm`...\n",
      "Skipping `model.layers.19`...\n",
      "Skipping `model.layers.19.self_attn`...\n",
      "Dequantizing `model.layers.19.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 31,  16,   8,  ..., 206, 206, 189], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0455, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd6410>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd62f0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.19.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 27,  20,   7,  ..., 214, 221, 219], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0636, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd65c0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd6470>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.19.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 60, 190, 183,  ..., 199, 208,  45], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0461, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd6710>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd6410>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.19.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([204, 108, 210,  ...,  60,  66, 173], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0460, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd6890>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd65c0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.19.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.19.mlp`...\n",
      "Dequantizing `model.layers.19.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([178,  56,  78,  ..., 218, 213, 182], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0492, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd6a40>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd68f0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.19.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([206, 174, 178,  ...,  63,  63, 161], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0489, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd6b90>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd6a70>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.19.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 62,  58, 166,  ...,  55,  67, 180], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0494, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd6d10>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd6bf0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.19.mlp.act_fn`...\n",
      "Skipping `model.layers.19.input_layernorm`...\n",
      "Skipping `model.layers.19.post_attention_layernorm`...\n",
      "Skipping `model.layers.20`...\n",
      "Skipping `model.layers.20.self_attn`...\n",
      "Dequantizing `model.layers.20.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([167, 204,   6,  ..., 167, 208, 180], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0438, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd6e90>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd6d70>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.20.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([15,  9, 16,  ..., 57, 56, 61], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0607, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd7040>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd6ef0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.20.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([177, 210,  37,  ...,  75,  94,  54], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0479, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd6fb0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd6e90>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.20.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([255, 244, 193,  ...,  62,  54,  61], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0470, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd7310>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd7040>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.20.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.20.mlp`...\n",
      "Dequantizing `model.layers.20.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 53,  79, 174,  ..., 196,  61, 104], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0492, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd74c0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd7370>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.20.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([193, 195,  54,  ..., 207,  57, 181], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0488, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd7430>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd7340>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.20.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 55,  51,  43,  ..., 165, 199,  57], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0493, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd7790>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd7670>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.20.mlp.act_fn`...\n",
      "Skipping `model.layers.20.input_layernorm`...\n",
      "Skipping `model.layers.20.post_attention_layernorm`...\n",
      "Skipping `model.layers.21`...\n",
      "Skipping `model.layers.21.self_attn`...\n",
      "Dequantizing `model.layers.21.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([  6,  31,  20,  ..., 195, 193,  80], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0451, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd7910>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd77f0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.21.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 30,  12,   8,  ..., 202, 203, 203], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0617, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd7ac0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd7970>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.21.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([81, 46, 42,  ..., 62, 62, 49], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0478, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd7a30>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd7910>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.21.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([166, 199, 141,  ..., 149,  82, 193], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0468, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd7d90>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd7ac0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.21.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.21.mlp`...\n",
      "Dequantizing `model.layers.21.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 70, 164,  57,  ...,  40,  51, 192], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0493, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffcd7f10>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd7df0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.21.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([147,  59,  59,  ...,  60, 145, 162], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0487, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc88100>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd7dc0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.21.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 59, 216,  56,  ...,  60, 171, 191], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0494, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc88250>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffcd7fd0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.21.mlp.act_fn`...\n",
      "Skipping `model.layers.21.input_layernorm`...\n",
      "Skipping `model.layers.21.post_attention_layernorm`...\n",
      "Skipping `model.layers.22`...\n",
      "Skipping `model.layers.22.self_attn`...\n",
      "Dequantizing `model.layers.22.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 19,  28,  13,  ..., 200, 209, 199], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0431, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc883d0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc882b0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.22.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 21,   5,  16,  ..., 190, 195, 197], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0601, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc88580>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc88430>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.22.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([165,  64,  54,  ...,  66, 182,  63], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0502, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc886d0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc883d0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.22.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 43, 204,  54,  ..., 193, 174,  94], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0474, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc88850>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc88580>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.22.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.22.mlp`...\n",
      "Dequantizing `model.layers.22.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 55,  75,  57,  ...,  56,  27, 191], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0496, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc88a00>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc888b0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.22.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([190, 192,  52,  ..., 195, 207, 218], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0487, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc88970>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc88880>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.22.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 57, 216, 204,  ..., 207, 173,  48], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0493, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc88cd0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc88bb0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.22.mlp.act_fn`...\n",
      "Skipping `model.layers.22.input_layernorm`...\n",
      "Skipping `model.layers.22.post_attention_layernorm`...\n",
      "Skipping `model.layers.23`...\n",
      "Skipping `model.layers.23.self_attn`...\n",
      "Dequantizing `model.layers.23.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 15,  21,   1,  ..., 231, 196, 215], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0420, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc88e50>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc88d30>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.23.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 28,  24,  19,  ..., 210, 163, 192], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0604, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc88fd0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc88eb0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.23.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([173, 192, 198,  ...,  57, 225,  58], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0511, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc89150>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc88e50>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.23.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 41,  74,  38,  ...,  79, 113,  93], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0473, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc892d0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc88fd0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.23.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.23.mlp`...\n",
      "Dequantizing `model.layers.23.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([184,  33,  58,  ...,  80,  61,  44], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0497, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc89450>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc89330>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.23.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([63, 59, 31,  ..., 93, 41, 44], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0487, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc895d0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc894b0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.23.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([191,  93, 206,  ...,  54,  46,  45], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0494, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc89750>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc89630>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.23.mlp.act_fn`...\n",
      "Skipping `model.layers.23.input_layernorm`...\n",
      "Skipping `model.layers.23.post_attention_layernorm`...\n",
      "Skipping `model.layers.24`...\n",
      "Skipping `model.layers.24.self_attn`...\n",
      "Dequantizing `model.layers.24.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 46,  62, 169,  ..., 192, 188, 170], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0438, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc898d0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc897b0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.24.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 26,  37,  21,  ..., 187,  74,  77], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0628, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc89a80>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc89930>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.24.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([177, 208, 214,  ...,  54,  57,  60], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0500, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc899f0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc898d0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.24.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 62,  63,  63,  ...,  68,  91, 181], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0467, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc89d80>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc89a80>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.24.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.24.mlp`...\n",
      "Dequantizing `model.layers.24.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 49,  79,  59,  ..., 198,  90, 219], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0498, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc89f30>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc89de0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.24.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([166, 204,  54,  ..., 154,  61, 201], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0487, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8a0b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc89f90>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.24.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([181, 205, 204,  ...,  63,  61,  82], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0493, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8a230>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8a110>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.24.mlp.act_fn`...\n",
      "Skipping `model.layers.24.input_layernorm`...\n",
      "Skipping `model.layers.24.post_attention_layernorm`...\n",
      "Skipping `model.layers.25`...\n",
      "Skipping `model.layers.25.self_attn`...\n",
      "Dequantizing `model.layers.25.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 18,  17,  23,  ..., 197, 194, 197], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0417, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8a3b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8a290>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.25.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 30,  28,  28,  ..., 209, 202, 206], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0569, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8a530>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8a410>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.25.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 59,  89, 111,  ...,  34,  23,  85], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0523, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8a4a0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8a3b0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.25.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([196,  78,  58,  ...,  64, 191, 162], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0473, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8a800>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8a530>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.25.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.25.mlp`...\n",
      "Dequantizing `model.layers.25.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 51,  68,  51,  ..., 196,  67,  79], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0499, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8a9b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8a860>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.25.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([211, 198,  51,  ...,  58,  48, 238], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0487, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8ab30>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8aa10>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.25.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 70,  73,  53,  ..., 218,  49,  99], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0493, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8acb0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8ab90>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.25.mlp.act_fn`...\n",
      "Skipping `model.layers.25.input_layernorm`...\n",
      "Skipping `model.layers.25.post_attention_layernorm`...\n",
      "Skipping `model.layers.26`...\n",
      "Skipping `model.layers.26.self_attn`...\n",
      "Dequantizing `model.layers.26.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 63,  61,  63,  ..., 168, 210, 197], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0416, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8ae30>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8ab60>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.26.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 10,  17,  20,  ..., 183, 181,  91], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0572, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8ada0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8ae90>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.26.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([208, 194, 165,  ...,  55, 186,  55], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0514, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8b100>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8ae30>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.26.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 73,  73, 165,  ...,  74, 190,  91], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0468, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8b280>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8ada0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.26.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.26.mlp`...\n",
      "Dequantizing `model.layers.26.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 48, 148, 173,  ...,  42, 164, 206], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0499, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8b400>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8b2e0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.26.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([198, 152, 191,  ...,  52,  76, 201], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0489, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8b580>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8b460>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.26.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([153, 206, 227,  ..., 199, 186,  36], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0494, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8b730>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8b5e0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.26.mlp.act_fn`...\n",
      "Skipping `model.layers.26.input_layernorm`...\n",
      "Skipping `model.layers.26.post_attention_layernorm`...\n",
      "Skipping `model.layers.27`...\n",
      "Skipping `model.layers.27.self_attn`...\n",
      "Dequantizing `model.layers.27.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 16,  34,  16,  ..., 202, 224, 229], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0445, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8b8b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8b5b0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.27.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([  7,  17,  23,  ..., 212, 195, 206], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0615, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8ba30>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8b910>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.27.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 57,  19, 174,  ..., 192, 203,  51], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0506, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8b9a0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8b8b0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.27.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 56, 233, 209,  ..., 201,  54,  62], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0475, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8bd00>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8ba30>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.27.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.27.mlp`...\n",
      "Dequantizing `model.layers.27.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([185,  58, 218,  ...,  19,  48,  37], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0500, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc8beb0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8bd60>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.27.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([217,  58, 187,  ...,  53, 195, 199], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0488, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc58070>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8bf10>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.27.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([192, 175, 193,  ..., 212,  72, 162], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0495, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc58190>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc8bd30>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.27.mlp.act_fn`...\n",
      "Skipping `model.layers.27.input_layernorm`...\n",
      "Skipping `model.layers.27.post_attention_layernorm`...\n",
      "Skipping `model.layers.28`...\n",
      "Skipping `model.layers.28.self_attn`...\n",
      "Dequantizing `model.layers.28.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 12,  24,  11,  ..., 206, 253, 230], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0398, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc58370>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc58220>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.28.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 24,  28,  26,  ..., 208, 224, 191], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0579, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc584f0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc58070>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.28.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 76,  54, 246,  ...,  37, 217, 188], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0548, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc58460>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc58220>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.28.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 56, 175, 173,  ...,  76,  81,  94], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0483, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc587c0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc58070>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.28.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.28.mlp`...\n",
      "Dequantizing `model.layers.28.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 56, 198, 205,  ...,  49, 196, 187], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0501, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc58970>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc58820>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.28.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 52, 219,  59,  ...,  62, 161,  43], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0489, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc588e0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc589d0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.28.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 77, 189, 180,  ...,  26,  46, 220], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0495, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc58c70>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc58b20>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.28.mlp.act_fn`...\n",
      "Skipping `model.layers.28.input_layernorm`...\n",
      "Skipping `model.layers.28.post_attention_layernorm`...\n",
      "Skipping `model.layers.29`...\n",
      "Skipping `model.layers.29.self_attn`...\n",
      "Dequantizing `model.layers.29.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 34,  36,  28,  ...,  86, 182, 159], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0408, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc58df0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc58af0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.29.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([46, 48, 53,  ..., 82, 63, 67], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0578, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc58f70>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc58670>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.29.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 48,  34, 195,  ...,  43,  74,  52], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0543, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc590c0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc58df0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.29.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 60, 159, 179,  ...,  62,  88,  61], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0476, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc59240>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc58f70>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.29.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.29.mlp`...\n",
      "Dequantizing `model.layers.29.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([234,  57,  76,  ...,  60, 204,  63], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0502, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc593f0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc592a0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.29.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 63,  94,  33,  ..., 202, 223, 212], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0490, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc59570>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc59450>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.29.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 61,  92,  44,  ..., 238,  42, 182], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0496, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc596f0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc595d0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.29.mlp.act_fn`...\n",
      "Skipping `model.layers.29.input_layernorm`...\n",
      "Skipping `model.layers.29.post_attention_layernorm`...\n",
      "Skipping `model.layers.30`...\n",
      "Skipping `model.layers.30.self_attn`...\n",
      "Dequantizing `model.layers.30.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([  7,  23,  24,  ..., 196,  67, 217], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0404, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc59870>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc59750>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.30.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 22,  26,  16,  ..., 184, 192, 163], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0531, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc597e0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc598d0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.30.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([196, 197,  57,  ..., 206,  39,  88], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0558, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc59b40>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc59870>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.30.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 60,  61,  72,  ..., 156,  48,  57], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0480, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc59cc0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc597e0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.30.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.30.mlp`...\n",
      "Dequantizing `model.layers.30.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 53,  63,  77,  ...,  40,  87, 194], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0503, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc59e40>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc59d20>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.30.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 53,  62, 197,  ..., 167, 176, 191], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0491, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc59ff0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc59ea0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.30.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 30, 199, 223,  ..., 213,  53,  31], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0496, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5a170>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5a0b0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.30.mlp.act_fn`...\n",
      "Skipping `model.layers.30.input_layernorm`...\n",
      "Skipping `model.layers.30.post_attention_layernorm`...\n",
      "Skipping `model.layers.31`...\n",
      "Skipping `model.layers.31.self_attn`...\n",
      "Dequantizing `model.layers.31.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 54,  99,  56,  ...,  79, 215, 200], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0405, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5a2f0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5a020>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.31.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 38,  32,  36,  ..., 161, 198, 192], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0568, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5a440>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5a3b0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.31.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([192, 166, 153,  ...,  76, 195,  53], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0564, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5a5c0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5a2f0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.31.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([193,  58,  49,  ..., 164,  67, 159], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0482, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5a740>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5a440>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.31.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.31.mlp`...\n",
      "Dequantizing `model.layers.31.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([215, 197,  63,  ..., 191, 180, 214], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0504, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5a8c0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5a7a0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.31.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 61,  64,  88,  ...,  45,  60, 209], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0492, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5aa70>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5a920>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.31.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([216,  52, 208,  ..., 192, 210,  66], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0497, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5abf0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5aad0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.31.mlp.act_fn`...\n",
      "Skipping `model.layers.31.input_layernorm`...\n",
      "Skipping `model.layers.31.post_attention_layernorm`...\n",
      "Skipping `model.layers.32`...\n",
      "Skipping `model.layers.32.self_attn`...\n",
      "Dequantizing `model.layers.32.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 24,  25,  39,  ..., 195, 164, 184], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0388, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5ad70>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5ac50>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.32.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 16,  30,  22,  ..., 213, 181, 191], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0532, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5aef0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5add0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.32.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([212,  49,  75,  ..., 202, 191,  55], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0565, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5ae60>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5ad70>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.32.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([213, 197, 111,  ...,  50,  54,  46], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0484, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5b1c0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5aef0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.32.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.32.mlp`...\n",
      "Dequantizing `model.layers.32.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([198,  57, 193,  ..., 167, 193,  91], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0505, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5b370>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5b220>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.32.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 78, 213,  71,  ..., 227,  32,  60], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0493, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5b4f0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5b3d0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.32.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([211, 197,  72,  ...,  56, 173, 208], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0497, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5b670>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5b520>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.32.mlp.act_fn`...\n",
      "Skipping `model.layers.32.input_layernorm`...\n",
      "Skipping `model.layers.32.post_attention_layernorm`...\n",
      "Skipping `model.layers.33`...\n",
      "Skipping `model.layers.33.self_attn`...\n",
      "Dequantizing `model.layers.33.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 24,  23,  32,  ...,  59, 225, 192], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0396, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5b7f0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5b730>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.33.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 26,   7,  31,  ...,  90, 205, 191], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0535, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5b940>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5b820>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.33.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([222, 203, 178,  ..., 192,  38,  62], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0580, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5bac0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5b7f0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.33.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([168, 168,  95,  ...,  55, 199, 187], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0482, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5bc40>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5b940>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.33.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.33.mlp`...\n",
      "Dequantizing `model.layers.33.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 43, 193, 185,  ...,  50,  78, 187], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0505, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5bdf0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5bca0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.33.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 88, 150, 200,  ..., 193, 200, 198], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0494, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc5bf70>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5be50>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.33.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([168, 173,  80,  ..., 202,  73,  73], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0497, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc60160>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5bc70>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.33.mlp.act_fn`...\n",
      "Skipping `model.layers.33.input_layernorm`...\n",
      "Skipping `model.layers.33.post_attention_layernorm`...\n",
      "Skipping `model.layers.34`...\n",
      "Skipping `model.layers.34.self_attn`...\n",
      "Dequantizing `model.layers.34.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 15,   3,  12,  ..., 222, 200, 199], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0397, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc60100>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc5bdf0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.34.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 23,  14,  32,  ..., 217, 184, 212], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0532, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc60250>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc60160>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.34.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 70,  42,  91,  ..., 220,  45,  25], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0576, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc60580>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc60100>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.34.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 54,  43,  47,  ...,  55,  57, 195], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0487, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc60700>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc60160>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.34.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.34.mlp`...\n",
      "Dequantizing `model.layers.34.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 55,  37, 195,  ..., 193, 192, 203], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0506, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc608b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc60760>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.34.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([104, 183, 160,  ..., 169, 175, 198], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0495, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc60a30>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc60910>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.34.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 60,  34,  55,  ...,  73, 244,  53], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0497, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc60bb0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc60a90>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.34.mlp.act_fn`...\n",
      "Skipping `model.layers.34.input_layernorm`...\n",
      "Skipping `model.layers.34.post_attention_layernorm`...\n",
      "Skipping `model.layers.35`...\n",
      "Skipping `model.layers.35.self_attn`...\n",
      "Dequantizing `model.layers.35.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([193,  91, 217,  ...,  59, 198, 166], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0387, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc60d30>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc60c70>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.35.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([204,  26,  30,  ..., 212, 191, 162], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0535, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc60e80>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc60b20>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.35.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 30,  44, 217,  ...,  30,  27,  32], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0563, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc61000>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc60d30>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.35.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 42,  83, 193,  ...,  55,  60,  61], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0463, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc61180>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc60e80>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.35.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.35.mlp`...\n",
      "Dequantizing `model.layers.35.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([206,  56,  89,  ..., 213, 162,  52], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0507, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc61300>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc611e0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.35.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([191,  89, 212,  ...,  89, 197,  63], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0496, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc614b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc61360>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.35.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 91, 175, 167,  ...,  59, 222, 230], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0497, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc61630>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc61330>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.35.mlp.act_fn`...\n",
      "Skipping `model.layers.35.input_layernorm`...\n",
      "Skipping `model.layers.35.post_attention_layernorm`...\n",
      "Skipping `model.layers.36`...\n",
      "Skipping `model.layers.36.self_attn`...\n",
      "Dequantizing `model.layers.36.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([18, 27, 88,  ..., 55, 45, 53], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0396, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc617b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc61660>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.36.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 55,  60,  60,  ..., 180,  21,  28], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0504, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc61930>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc614e0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.36.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([193,  31,  26,  ..., 195, 224, 215], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0582, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc618a0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc617b0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.36.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 63,  57,  38,  ...,  67, 105,  86], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0481, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc61c00>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc61930>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.36.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.36.mlp`...\n",
      "Dequantizing `model.layers.36.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([174,  50,  53,  ..., 169, 205, 223], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0508, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc61db0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc61c60>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.36.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 54,  51,  90,  ..., 223,  45,  62], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0496, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc61f60>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc61c30>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.36.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([228, 181, 163,  ..., 191,  40,  87], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0497, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc620e0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc61fc0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.36.mlp.act_fn`...\n",
      "Skipping `model.layers.36.input_layernorm`...\n",
      "Skipping `model.layers.36.post_attention_layernorm`...\n",
      "Skipping `model.layers.37`...\n",
      "Skipping `model.layers.37.self_attn`...\n",
      "Dequantizing `model.layers.37.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 63,  50,  26,  ..., 198, 160, 208], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0402, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc62260>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc62170>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.37.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([193,  34,  17,  ..., 209, 185, 194], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0535, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc62200>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc622c0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.37.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 95, 165, 192,  ..., 168,  46,  46], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0598, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc62530>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc62260>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.37.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([223, 103, 192,  ..., 172,  72,  95], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0487, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc626b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc62200>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.37.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.37.mlp`...\n",
      "Dequantizing `model.layers.37.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([194,  72,  47,  ..., 182,  40,  28], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0508, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc62830>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc61f90>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.37.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([194,  82,  50,  ...,  42, 175, 245], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0497, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc629b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc627a0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.37.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([195,  42, 213,  ..., 191, 204,  61], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0497, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc62b60>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc62920>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.37.mlp.act_fn`...\n",
      "Skipping `model.layers.37.input_layernorm`...\n",
      "Skipping `model.layers.37.post_attention_layernorm`...\n",
      "Skipping `model.layers.38`...\n",
      "Skipping `model.layers.38.self_attn`...\n",
      "Dequantizing `model.layers.38.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([12, 31, 30,  ..., 59, 56, 82], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0415, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc62ce0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc62bc0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.38.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([22, 17, 12,  ..., 15, 36, 37], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0562, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc62c50>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc62d40>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.38.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([195,  52,  46,  ...,  54, 208,  59], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0577, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc62fb0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc62ce0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.38.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([212, 236, 163,  ...,  56, 201,  80], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0478, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc63130>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc62c50>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.38.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.38.mlp`...\n",
      "Dequantizing `model.layers.38.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([218, 194, 166,  ..., 217,  59, 182], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0508, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc632b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc629e0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.38.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 52, 226,  63,  ..., 207, 211,  74], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0497, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc63430>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc63220>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.38.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 70, 242, 199,  ...,  48,  75,  72], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0496, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc635b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc633a0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.38.mlp.act_fn`...\n",
      "Skipping `model.layers.38.input_layernorm`...\n",
      "Skipping `model.layers.38.post_attention_layernorm`...\n",
      "Skipping `model.layers.39`...\n",
      "Skipping `model.layers.39.self_attn`...\n",
      "Dequantizing `model.layers.39.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 28,  36,  22,  ...,  55,  75, 180], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0412, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc63760>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc63520>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.39.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 6, 14, 37,  ..., 29, 23, 24], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0528, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc638e0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc637f0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.39.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 43,  59,  53,  ...,  56, 161, 218], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0562, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc63880>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc63760>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.39.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([202, 176,  66,  ...,  55, 197,  59], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0465, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc63bb0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc638e0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.39.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.39.mlp`...\n",
      "Dequantizing `model.layers.39.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([228,  44, 224,  ...,  94, 120, 194], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0508, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc63d60>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc63940>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.39.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([192, 169,  64,  ...,  54, 212, 195], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0498, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc63ee0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc63af0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.39.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([195,  41, 196,  ...,  35,  48, 208], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0496, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc30040>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc63fd0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.39.mlp.act_fn`...\n",
      "Skipping `model.layers.39.input_layernorm`...\n",
      "Skipping `model.layers.39.post_attention_layernorm`...\n",
      "Skipping `model.layers.40`...\n",
      "Skipping `model.layers.40.self_attn`...\n",
      "Dequantizing `model.layers.40.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 11,  28,  24,  ..., 207, 199, 204], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0405, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc30220>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc63e80>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.40.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 33,  30,  18,  ..., 181, 184, 174], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0548, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc303a0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc302e0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.40.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 63,  48, 184,  ..., 163,  53,  55], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0568, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc304f0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc30220>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.40.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 65, 179, 193,  ..., 212,  85,  65], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0463, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc30670>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc302e0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.40.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.40.mlp`...\n",
      "Dequantizing `model.layers.40.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([223,  57,  78,  ...,  69, 217, 210], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0508, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc30820>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc306d0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.40.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([199, 208,  47,  ...,  78,  53, 196], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0499, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc309a0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc30880>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.40.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([171,  64, 196,  ...,  41, 198, 197], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0496, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc30b20>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc30a00>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.40.mlp.act_fn`...\n",
      "Skipping `model.layers.40.input_layernorm`...\n",
      "Skipping `model.layers.40.post_attention_layernorm`...\n",
      "Skipping `model.layers.41`...\n",
      "Skipping `model.layers.41.self_attn`...\n",
      "Dequantizing `model.layers.41.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 35,  14,  28,  ..., 199, 177, 198], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0414, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc30ca0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc30be0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.41.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 26,  26,  32,  ..., 192, 195, 218], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0552, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc30e20>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc30d00>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.41.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([205,  69,  58,  ..., 193,  43,  77], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0595, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc30f70>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc30ca0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.41.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 60,  80,  72,  ...,  62, 187, 193], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0475, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc310f0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc30e20>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.41.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.41.mlp`...\n",
      "Dequantizing `model.layers.41.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 60, 173, 171,  ...,  76, 170,  48], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0507, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc312a0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc31150>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.41.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 48,  60,  50,  ..., 148,  68, 212], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0499, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc31210>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc31300>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.41.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([189, 163, 179,  ..., 196,  47,  54], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0496, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc31570>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc31450>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.41.mlp.act_fn`...\n",
      "Skipping `model.layers.41.input_layernorm`...\n",
      "Skipping `model.layers.41.post_attention_layernorm`...\n",
      "Skipping `model.layers.42`...\n",
      "Skipping `model.layers.42.self_attn`...\n",
      "Dequantizing `model.layers.42.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 17,  38,  50,  ...,  74,  87, 190], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0403, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc316f0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc315d0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.42.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 22,  30,  19,  ..., 200, 199, 219], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0552, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc318a0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc31750>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.42.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 54, 168,  79,  ..., 245, 219,  82], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0578, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc31810>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc316f0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.42.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 61,  46,  55,  ..., 201,  44, 209], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0462, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc31b70>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc318a0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.42.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.42.mlp`...\n",
      "Dequantizing `model.layers.42.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 81,  44, 202,  ..., 171, 215, 200], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0507, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc31d20>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc31bd0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.42.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 59, 197, 173,  ..., 212,  50, 192], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0500, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc31ea0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc31d80>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.42.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 47, 202,  58,  ...,  54,  61,  95], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0497, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc32020>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc31ed0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.42.mlp.act_fn`...\n",
      "Skipping `model.layers.42.input_layernorm`...\n",
      "Skipping `model.layers.42.post_attention_layernorm`...\n",
      "Skipping `model.layers.43`...\n",
      "Skipping `model.layers.43.self_attn`...\n",
      "Dequantizing `model.layers.43.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 22,  35,  19,  ..., 220, 195, 175], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0425, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc321a0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc31d50>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.43.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 21,  13,  27,  ..., 203, 192, 195], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0544, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc32320>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc32200>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.43.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 59, 199,  51,  ...,  94, 192, 192], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0616, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc32290>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc321a0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.43.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([199, 215, 204,  ..., 106,  89, 197], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0491, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc325f0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc32320>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.43.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.43.mlp`...\n",
      "Dequantizing `model.layers.43.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([195, 232, 201,  ...,  46, 174,  22], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0506, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc32770>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc32650>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.43.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([95, 48, 56,  ..., 40, 42, 51], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0501, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc32920>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc327d0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.43.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 63,  79,  68,  ...,  77,  59, 194], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0498, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc32aa0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc32980>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.43.mlp.act_fn`...\n",
      "Skipping `model.layers.43.input_layernorm`...\n",
      "Skipping `model.layers.43.post_attention_layernorm`...\n",
      "Skipping `model.layers.44`...\n",
      "Skipping `model.layers.44.self_attn`...\n",
      "Dequantizing `model.layers.44.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 16,  25,  12,  ..., 217, 190,  87], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0435, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc32c20>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc32b00>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.44.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 27,  16,  21,  ..., 176, 196, 200], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0565, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc32b90>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc32ad0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.44.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 55,  59, 170,  ...,  66,  49, 161], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0594, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc32ef0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc32c20>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.44.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 58, 169, 193,  ..., 200,  77,  63], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0480, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc33070>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc32b90>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.44.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.44.mlp`...\n",
      "Dequantizing `model.layers.44.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([177, 192, 209,  ...,  61, 196,  19], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0506, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc331f0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc330d0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.44.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 63,  67, 175,  ...,  53, 188, 211], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0501, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc333a0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc33250>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.44.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([174, 188, 194,  ...,  57, 255,  80], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0497, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc33520>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc333d0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.44.mlp.act_fn`...\n",
      "Skipping `model.layers.44.input_layernorm`...\n",
      "Skipping `model.layers.44.post_attention_layernorm`...\n",
      "Skipping `model.layers.45`...\n",
      "Skipping `model.layers.45.self_attn`...\n",
      "Dequantizing `model.layers.45.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 14,   4,  10,  ..., 197, 195, 195], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0427, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc336a0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc33550>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.45.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 35,  23,  63,  ..., 205, 170, 190], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0535, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc33820>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc33700>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.45.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 53, 197,  83,  ..., 200, 175, 199], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0616, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc33790>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc336a0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.45.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([225, 160, 220,  ...,  56, 200,  61], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0484, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc33af0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc33820>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.45.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.45.mlp`...\n",
      "Dequantizing `model.layers.45.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 60, 209, 199,  ...,  45,  57,  42], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0508, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc33ca0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc33b50>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.45.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 50, 193, 199,  ...,  52,  57, 190], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0502, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc33e20>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc33d00>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.45.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 28,  74,  39,  ...,  51, 203, 220], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0497, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffc33fa0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc33e80>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.45.mlp.act_fn`...\n",
      "Skipping `model.layers.45.input_layernorm`...\n",
      "Skipping `model.layers.45.post_attention_layernorm`...\n",
      "Skipping `model.layers.46`...\n",
      "Skipping `model.layers.46.self_attn`...\n",
      "Dequantizing `model.layers.46.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([35, 10,  6,  ..., 62, 43, 60], device='cuda:1', dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0413, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffbf4190>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc33e50>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.46.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 26,  11,  16,  ..., 163, 172, 195], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0531, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffbf4280>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffc33fd0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.46.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 78,  29, 198,  ..., 196, 241, 204], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0608, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffbf4430>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffbf4190>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.46.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 53, 211, 176,  ...,  61, 170, 183], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0467, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffbf45b0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffbf4280>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.46.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.46.mlp`...\n",
      "Dequantizing `model.layers.46.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([208, 206, 208,  ...,  43,  51, 171], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0514, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffbf4760>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffbf4610>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.46.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 53,  56, 232,  ...,  43,  55,  76], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0503, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffbf48e0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffbf45e0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.46.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 82, 181,  78,  ..., 169, 205, 219], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0497, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffbf4a60>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffbf4940>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.46.mlp.act_fn`...\n",
      "Skipping `model.layers.46.input_layernorm`...\n",
      "Skipping `model.layers.46.post_attention_layernorm`...\n",
      "Skipping `model.layers.47`...\n",
      "Skipping `model.layers.47.self_attn`...\n",
      "Dequantizing `model.layers.47.self_attn.q_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 62,  63,  61,  ..., 196, 198, 184], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0396, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffbf4be0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffbf4ac0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.47.self_attn.k_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 60,  58,  61,  ..., 211, 196, 209], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0581, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffbf4b80>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffbf4a90>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.47.self_attn.v_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([164,  60, 161,  ..., 163, 248,  60], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([1024, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0497, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffbf4eb0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffbf4be0>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.47.self_attn.o_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 51,  55,  77,  ...,  84, 196, 191], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0395, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffbf5030>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffbf4b80>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.47.self_attn.rotary_emb`...\n",
      "Skipping `model.layers.47.mlp`...\n",
      "Dequantizing `model.layers.47.mlp.gate_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 34, 156, 192,  ...,  98, 148,  58], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0523, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffbf51e0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffbf5090>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.47.mlp.up_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([ 56, 214, 207,  ...,  53,  50, 107], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([22016, 8192]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0507, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffbf5360>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffbf4f70>] to dtype: [torch.bfloat16]\n",
      "Dequantizing `model.layers.47.mlp.down_proj`...\n",
      "quant_state.__dict__: [{'absmax': tensor([202,  58, 222,  ...,  42, 174,  53], device='cuda:1',\n",
      "       dtype=torch.uint8), 'shape': torch.Size([8192, 22016]), 'code': tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
      "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
      "       device='cuda:1'), 'dtype': torch.float16, 'blocksize': 64, 'quant_type': 'nf4', 'offset': tensor(0.0495, device='cuda:1'), 'state2': <bitsandbytes.functional.QuantState object at 0x7fdbffbf54e0>, 'nested': True}]\n",
      "SUCCESS setting quant_state.dtype [<bitsandbytes.functional.QuantState object at 0x7fdbffbf53c0>] to dtype: [torch.bfloat16]\n",
      "Skipping `model.layers.47.mlp.act_fn`...\n",
      "Skipping `model.layers.47.input_layernorm`...\n",
      "Skipping `model.layers.47.post_attention_layernorm`...\n",
      "Skipping `model.norm`...\n",
      "Skipping `lm_head`...\n",
      "Saving dequantized model & tokenizer...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- base_model: POST dequantize\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 8192)\n",
      "    (layers): ModuleList(\n",
      "      (0-47): 48 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaFlashAttention2(\n",
      "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "          (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
      "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
      "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import peft\n",
    "import json\n",
    "import shutil\n",
    "from peft.utils import _get_submodules\n",
    "import os\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.functional import dequantize_4bit\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "\n",
    "def dequantize_model( model, tokenizer, to='./dequantized-model', dtype=torch.float16, device=\"cpu\" ):\n",
    "    \"\"\"\n",
    "    'model': the peftmodel you loaded with qlora.\n",
    "    'tokenizer': the model's corresponding hf's tokenizer.\n",
    "    'to': directory to save the dequantized model\n",
    "    'dtype': dtype that the model was trained using\n",
    "    'device': device to load the model to\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs( to, exist_ok=True )\n",
    "    \n",
    "    cls = bnb.nn.Linear4bit\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance( module, cls ):\n",
    "                print( f\"Dequantizing `{name}`...\" )\n",
    "                quant_state = copy.deepcopy( module.weight.quant_state )\n",
    "                print( f\"quant_state.__dict__: [{quant_state.__dict__}]\" )\n",
    "                try:\n",
    "                    quant_state.dtype = dtype\n",
    "                    print( f\"SUCCESS setting quant_state.dtype [{quant_state}] to dtype: [{dtype}]\" )\n",
    "                except Exception as e:\n",
    "                    print( f\"FAILed   to set quant_state.dtype [{quant_state}] to dtype: [{dtype}] [{e}]\" )\n",
    "                    # quant_state = None\n",
    "                \n",
    "                weights = dequantize_4bit( module.weight.data, quant_state=quant_state, quant_type=\"nf4\" ).to( dtype )\n",
    "                \n",
    "                new_module = torch.nn.Linear( module.in_features, module.out_features, bias=None, dtype=dtype )\n",
    "                new_module.weight = torch.nn.Parameter( weights )\n",
    "                new_module.to( device=device, dtype=dtype )\n",
    "                \n",
    "                parent, target, target_name = _get_submodules( model, name )\n",
    "                setattr( parent, target_name, new_module )\n",
    "                \n",
    "            else:\n",
    "                print( f\"Skipping `{name}`...\" )\n",
    "        \n",
    "        # a hack, setting this to avoid hf's saving error because hf\n",
    "        # itself does not support saving a model that is registered to be loaded in 4bit.\n",
    "        model.is_loaded_in_4bit = False\n",
    "        \n",
    "        print( \"Saving dequantized model & tokenizer...\" )\n",
    "        model.save_pretrained( to )\n",
    "        tokenizer.save_pretrained( to )\n",
    "        config_data = json.loads( open( os.path.join( to, 'config.json' ), 'r' ).read() )\n",
    "        config_data.pop( \"quantization_config\", None )\n",
    "        config_data.pop( \"pretraining_tp\", None )\n",
    "        with open( os.path.join( to, 'config.json' ), 'w' ) as config:\n",
    "            config.write( json.dumps( config_data, indent=2 ) )\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "#To which precision do you want to dequantize?\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=dtype,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "# )\n",
    "\n",
    "model_name = \".\"\n",
    "try:\n",
    "    print( f\"Dequantizing model {model_name}..\" )\n",
    "    \n",
    "    # model = LlamaForCausalLM.from_pretrained(\n",
    "    #     model_name,\n",
    "    #     quantization_config=quantization_config,\n",
    "    #     device_map={ \"\": 0 }\n",
    "    # )\n",
    "    du.print_banner( \"base_model: PRE dequantize\" )\n",
    "    print( base_model )\n",
    "    base_model = dequantize_model( base_model, tokenizer, to='./dequantized-base-model/', dtype=dtype )\n",
    "    du.print_banner( \"base_model: POST dequantize\" )\n",
    "    print( base_model )\n",
    "    \n",
    "    # base_model_plus_adapter = AutoModelForCausalLM.from_pretrained( base_model, \"adapters/00-browser-vox-command\" )\n",
    "    # du.print_banner( \"PRE merge base_model_plus_adapter\" )\n",
    "    # print( base_model_plus_adapter )\n",
    "    # \n",
    "    # base_model_plus_adapter = base_model_plus_adapter.merge_and_unload()\n",
    "    # du.print_banner( \"POST merged base_model_plus_adapter\" )\n",
    "    # print( base_model_plus_adapter )\n",
    "    # \n",
    "    # base_model_plus_adapter.save_pretrained( \"./merged/\", safe_serialization=True )\n",
    "    # tokenizer.save_pretrained( \"./merged/\" )\n",
    "    \n",
    "except Exception as e:\n",
    "    \n",
    "    print( f\"An error occurred: {e}\" )\n",
    "    # Print Stacktrace\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # # Delete the model object if it exists\n",
    "    # if 'model' in locals():\n",
    "    #     del base_model\n",
    "    # \n",
    "    # # Clear the GPU cache\n",
    "    # torch.cuda.empty_cache()\n",
    "    # \n",
    "    # # Run the garbage collection\n",
    "    # gc.collect()\n",
    "    # \n",
    "    # print( \"Model, GPU cache, and garbage have been cleared.\" )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:22:56.515499Z",
     "start_time": "2024-01-14T20:18:09.226187Z"
    }
   },
   "id": "66420c43f22b33b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- PRE merge base_model_plus_adapter\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 8192)\n",
      "        (layers): ModuleList(\n",
      "          (0-47): 48 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaFlashAttention2(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=8192, out_features=22016, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=22016, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=8192, out_features=22016, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=22016, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=22016, out_features=8192, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=22016, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "base_model_plus_adapter = AutoModelForCausalLM.from_pretrained( base_model, \"adapters/00-browser-vox-command\" )\n",
    "du.print_banner( \"PRE merge base_model_plus_adapter\" )\n",
    "print( base_model_plus_adapter )\n",
    "\n",
    "# base_model_plus_adapter = base_model_plus_adapter.merge_and_unload()\n",
    "# du.print_banner( \"POST merged base_model_plus_adapter\" )\n",
    "# print( base_model_plus_adapter )\n",
    "# \n",
    "# base_model_plus_adapter.save_pretrained( \"./merged/\", safe_serialization=True )\n",
    "# tokenizer.save_pretrained( \"./merged/\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:24:29.043806Z",
     "start_time": "2024-01-14T20:24:25.675186Z"
    }
   },
   "id": "7c8bc14e48ca8177"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- POST merged base_model_plus_adapter\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 8192)\n",
      "    (layers): ModuleList(\n",
      "      (0-47): 48 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaFlashAttention2(\n",
      "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "          (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
      "          (up_proj): Linear(in_features=8192, out_features=22016, bias=False)\n",
      "          (down_proj): Linear(in_features=22016, out_features=8192, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# base_model_plus_adapter = AutoModelForCausalLM.from_pretrained( base_model, \"adapters/00-browser-vox-command\" )\n",
    "# du.print_banner( \"PRE merge base_model_plus_adapter\" )\n",
    "# print( base_model_plus_adapter )\n",
    "\n",
    "base_model_plus_adapter = base_model_plus_adapter.merge_and_unload()\n",
    "du.print_banner( \"POST merged base_model_plus_adapter\" )\n",
    "print( base_model_plus_adapter )\n",
    "# \n",
    "# base_model_plus_adapter.save_pretrained( \"./merged/\", safe_serialization=True )\n",
    "# tokenizer.save_pretrained( \"./merged/\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:38:18.135522Z",
     "start_time": "2024-01-14T20:25:36.000819Z"
    }
   },
   "id": "a535cc5b5026a0e3"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "('./merged/tokenizer_config.json',\n './merged/special_tokens_map.json',\n './merged/tokenizer.model',\n './merged/added_tokens.json',\n './merged/tokenizer.json')"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_plus_adapter.save_pretrained( \"./merged/\", safe_serialization=True )\n",
    "tokenizer.save_pretrained( \"./merged/\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:42:53.274157Z",
     "start_time": "2024-01-14T20:42:03.678749Z"
    }
   },
   "id": "bf6bdc82147199c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## load merged model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "164afc5f27a7c9d5"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'/var/model/Phind-CodeLlama-34B-v2'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir( \"/var/model/Phind-CodeLlama-34B-v2\" )\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:50:29.435182Z",
     "start_time": "2024-01-18T03:50:29.358025Z"
    }
   },
   "id": "1a3d7a9c713d1a72"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f13f0411da5b4191b16ac7acd6d28a1e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# device_map = \"cuda:1\"\n",
    "device_map = \"auto\"\n",
    "merged_model, tokenizer = get_base_model_and_tokenizer( \"./merged-naive/\", \".\", use_quant_config=False, device_map=device_map )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:50:40.268158Z",
     "start_time": "2024-01-18T03:50:34.752760Z"
    }
   },
   "id": "2500909e1d26439f"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def free_gpu( models ):\n",
    "    \n",
    "    for model in models:\n",
    "        del model\n",
    "        model = None\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "free_gpu( [ base_model, adapter_plus_model ] )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T22:49:28.889927Z",
     "start_time": "2024-01-10T22:49:28.889690Z"
    }
   },
   "id": "23550e0e9f04149f"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:51:44.682922Z",
     "start_time": "2024-01-18T03:51:44.673818Z"
    }
   },
   "id": "f16775997e14a03c"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            instruction  \\\n6252  Your job is to discern the intent of a human v...   \n4684  Your job is to discern the intent of a human v...   \n1731  Your job is to discern the intent of a human v...   \n4742  Your job is to discern the intent of a human v...   \n4521  Your job is to discern the intent of a human v...   \n\n                                                  input  \\\n6252  \\n        Below is the raw human voice command...   \n4684  \\n        Below is the raw human voice command...   \n1731  \\n        Below is the raw human voice command...   \n4742  \\n        Below is the raw human voice command...   \n4521  \\n        Below is the raw human voice command...   \n\n                                                 output  \\\n6252  \\n        <response>\\n            <browser-com...   \n4684  \\n        <response>\\n            <browser-com...   \n1731  \\n        <response>\\n            <browser-com...   \n4742  \\n        <response>\\n            <browser-com...   \n4521  \\n        <response>\\n            <browser-com...   \n\n                                                 prompt  \\\n6252  ### Instruction:\\n    Use the Task and Input g...   \n4684  ### Instruction:\\n    Use the Task and Input g...   \n1731  ### Instruction:\\n    Use the Task and Input g...   \n4742  ### Instruction:\\n    Use the Task and Input g...   \n4521  ### Instruction:\\n    Use the Task and Input g...   \n\n                                            gpt_message  \n6252  {'messages': [{'role': 'system', 'content': 'I...  \n4684  {'messages': [{'role': 'system', 'content': 'I...  \n1731  {'messages': [{'role': 'system', 'content': 'I...  \n4742  {'messages': [{'role': 'system', 'content': 'I...  \n4521  {'messages': [{'role': 'system', 'content': 'I...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>input</th>\n      <th>output</th>\n      <th>prompt</th>\n      <th>gpt_message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6252</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\n        Below is the raw human voice command...</td>\n      <td>\\n        &lt;response&gt;\\n            &lt;browser-com...</td>\n      <td>### Instruction:\\n    Use the Task and Input g...</td>\n      <td>{'messages': [{'role': 'system', 'content': 'I...</td>\n    </tr>\n    <tr>\n      <th>4684</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\n        Below is the raw human voice command...</td>\n      <td>\\n        &lt;response&gt;\\n            &lt;browser-com...</td>\n      <td>### Instruction:\\n    Use the Task and Input g...</td>\n      <td>{'messages': [{'role': 'system', 'content': 'I...</td>\n    </tr>\n    <tr>\n      <th>1731</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\n        Below is the raw human voice command...</td>\n      <td>\\n        &lt;response&gt;\\n            &lt;browser-com...</td>\n      <td>### Instruction:\\n    Use the Task and Input g...</td>\n      <td>{'messages': [{'role': 'system', 'content': 'I...</td>\n    </tr>\n    <tr>\n      <th>4742</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\n        Below is the raw human voice command...</td>\n      <td>\\n        &lt;response&gt;\\n            &lt;browser-com...</td>\n      <td>### Instruction:\\n    Use the Task and Input g...</td>\n      <td>{'messages': [{'role': 'system', 'content': 'I...</td>\n    </tr>\n    <tr>\n      <th>4521</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\n        Below is the raw human voice command...</td>\n      <td>\\n        &lt;response&gt;\\n            &lt;browser-com...</td>\n      <td>### Instruction:\\n    Use the Task and Input g...</td>\n      <td>{'messages': [{'role': 'system', 'content': 'I...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df = pd.read_json( \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\", lines=True ).sample( 100, random_state=42 )\n",
    "validation_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:51:45.359993Z",
     "start_time": "2024-01-18T03:51:45.150981Z"
    }
   },
   "id": "21de2f127247b122"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(100, 5)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:51:48.396410Z",
     "start_time": "2024-01-18T03:51:48.378245Z"
    }
   },
   "id": "c5263f96c53b2c09"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands file for [search new tab] exists: True\n",
      "Commands file for [search current tab] exists: True\n",
      "Commands file for [search google new tab] exists: True\n",
      "Commands file for [search google current tab] exists: True\n",
      "Commands file for [search google scholar new tab] exists: True\n",
      "Commands file for [search google scholar current tab] exists: True\n",
      "\n",
      "Using HuggingFace model_name [TGI/Phind-CodeLlama-34B-v2] in memory...\n",
      "\n",
      "Processing call [001] out of [100] = [1.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,929 ms\n",
      "Tokens per second [48.1]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>URLError</args></response>]\n",
      "\n",
      "Processing call [002] out of [100] = [2.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,186 ms\n",
      "Tokens per second [64.3]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [003] out of [100] = [3.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,348 ms\n",
      "Tokens per second [65.1]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>AI in supply chain optimization: How does AI contribute to supply chain optimization?</args></response>]\n",
      "\n",
      "Processing call [004] out of [100] = [4.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,525 ms\n",
      "Tokens per second [57.8]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>OverflowError</args></response>]\n",
      "\n",
      "Processing call [005] out of [100] = [5.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,709 ms\n",
      "Tokens per second [59.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Scan for Connection Aborted Error</args></response>]\n",
      "\n",
      "Processing call [006] out of [100] = [6.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,640 ms\n",
      "Tokens per second [51.9]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>healthy snack ideas</args></response>]\n",
      "\n",
      "Processing call [007] out of [100] = [7.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,028 ms\n",
      "Tokens per second [57.5]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>how to play guitar chords in this tab now</args></response>]\n",
      "\n",
      "Processing call [008] out of [100] = [8.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,514 ms\n",
      "Tokens per second [59.3]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>EnvironmentError</args></response>]\n",
      "\n",
      "Processing call [009] out of [100] = [9.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,463 ms\n",
      "Tokens per second [57.7]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Invalid Operation</args></response>]\n",
      "\n",
      "Processing call [010] out of [100] = [10.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,984 ms\n",
      "Tokens per second [52.3]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>how do i cure dandruff?</args></response>]\n",
      "\n",
      "Processing call [011] out of [100] = [11.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,526 ms\n",
      "Tokens per second [57.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>exchange rates today</args></response>]\n",
      "\n",
      "Processing call [012] out of [100] = [12.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,224 ms\n",
      "Tokens per second [54.9]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Out Of Bounds Timedelta: Out of bounds for timedelta</args></response>]\n",
      "\n",
      "Processing call [013] out of [100] = [13.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,814 ms\n",
      "Tokens per second [57.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Responsible AI practices</args></response>]\n",
      "\n",
      "Processing call [014] out of [100] = [14.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,573 ms\n",
      "Tokens per second [62.1]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>ROC Curve analysis: How is ROC curve analysis utilized to evaluate the performance of binary classifiers</args></response>]\n",
      "\n",
      "Processing call [015] out of [100] = [15.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,695 ms\n",
      "Tokens per second [54.2]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI in gaming, results</args></response>]\n",
      "\n",
      "Processing call [016] out of [100] = [16.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,700 ms\n",
      "Tokens per second [57.0]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>AI ethics in research</args></response>]\n",
      "\n",
      "Processing call [017] out of [100] = [17.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,410 ms\n",
      "Tokens per second [56.0]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Invalid Operation</args></response>]\n",
      "\n",
      "Processing call [018] out of [100] = [18.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,583 ms\n",
      "Tokens per second [59.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Syntax issue warning</args></response>]\n",
      "\n",
      "Processing call [019] out of [100] = [19.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,585 ms\n",
      "Tokens per second [53.8]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>UnicodeTranslateError</args></response>]\n",
      "\n",
      "Processing call [020] out of [100] = [20.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,463 ms\n",
      "Tokens per second [56.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Import Warning</args></response>]\n",
      "\n",
      "Processing call [021] out of [100] = [21.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,879 ms\n",
      "Tokens per second [58.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Joining and concatenating in Pandas</args></response>]\n",
      "\n",
      "Processing call [022] out of [100] = [22.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,639 ms\n",
      "Tokens per second [54.9]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>current news in tech</args></response>]\n",
      "\n",
      "Processing call [023] out of [100] = [23.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,638 ms\n",
      "Tokens per second [58.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>DIY home improvement projects</args></response>]\n",
      "\n",
      "Processing call [024] out of [100] = [24.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,816 ms\n",
      "Tokens per second [58.9]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>what is the capital of Australia?</args></response>]\n",
      "\n",
      "Processing call [025] out of [100] = [25.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,590 ms\n",
      "Tokens per second [57.5]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Cross-Validation techniques</args></response>]\n",
      "\n",
      "Processing call [026] out of [100] = [26.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,643 ms\n",
      "Tokens per second [58.6]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Pandas and regular expressions</args></response>]\n",
      "\n",
      "Processing call [027] out of [100] = [27.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,349 ms\n",
      "Tokens per second [60.9]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>What are user-defined warnings in Python, and how can they be effectively used?</args></response>]\n",
      "\n",
      "Processing call [028] out of [100] = [28.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,874 ms\n",
      "Tokens per second [56.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>shopping for groceries online</args></response>]\n",
      "\n",
      "Processing call [029] out of [100] = [29.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,705 ms\n",
      "Tokens per second [61.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>DIY home improvement projects</args></response>]\n",
      "\n",
      "Processing call [030] out of [100] = [30.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,701 ms\n",
      "Tokens per second [58.9]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Resource Warning, initiate</args></response>]\n",
      "\n",
      "Processing call [031] out of [100] = [31.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,584 ms\n",
      "Tokens per second [56.5]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Apache Spark data processing</args></response>]\n",
      "\n",
      "Processing call [032] out of [100] = [32.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,406 ms\n",
      "Tokens per second [63.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Responsible AI practices: How can organizations implement responsible AI practices?</args></response>]\n",
      "\n",
      "Processing call [033] out of [100] = [33.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,584 ms\n",
      "Tokens per second [54.6]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>news in tech</args></response>]\n",
      "\n",
      "Processing call [034] out of [100] = [34.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,763 ms\n",
      "Tokens per second [49.9]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [035] out of [100] = [35.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,652 ms\n",
      "Tokens per second [59.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Connection Aborted Error</args></response>]\n",
      "\n",
      "Processing call [036] out of [100] = [36.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,418 ms\n",
      "Tokens per second [60.0]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>What are memory-related errors in Python, and how can they be prevented?</args></response>]\n",
      "\n",
      "Processing call [037] out of [100] = [37.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,759 ms\n",
      "Tokens per second [60.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Normalization vs. Standardization</args></response>]\n",
      "\n",
      "Processing call [038] out of [100] = [38.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,521 ms\n",
      "Tokens per second [63.3]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Examine What steps can be taken to address HTTP-related errors in web programming with Python?</args></response>]\n",
      "\n",
      "Processing call [039] out of [100] = [39.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,529 ms\n",
      "Tokens per second [53.4]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>ZeroDivisionError</args></response>]\n",
      "\n",
      "Processing call [040] out of [100] = [40.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,753 ms\n",
      "Tokens per second [63.7]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Find me AI and blockchain integration: What are the benefits of integrating AI with blockchain technology?</args></response>]\n",
      "\n",
      "Processing call [041] out of [100] = [41.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,700 ms\n",
      "Tokens per second [59.6]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>best restaurants near me</args></response>]\n",
      "\n",
      "Processing call [042] out of [100] = [42.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,714 ms\n",
      "Tokens per second [55.3]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Deep Learning in genomics</args></response>]\n",
      "\n",
      "Processing call [043] out of [100] = [43.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,639 ms\n",
      "Tokens per second [54.6]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>NotImplementedError</args></response>]\n",
      "\n",
      "Processing call [044] out of [100] = [44.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,805 ms\n",
      "Tokens per second [57.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>GANs recent advancements</args></response>]\n",
      "\n",
      "Processing call [045] out of [100] = [45.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,636 ms\n",
      "Tokens per second [55.8]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>how to clean your room</args></response>]\n",
      "\n",
      "Processing call [046] out of [100] = [46.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,929 ms\n",
      "Tokens per second [57.7]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Delve for Using Pandas with big data</args></response>]\n",
      "\n",
      "Processing call [047] out of [100] = [47.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,519 ms\n",
      "Tokens per second [54.0]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Hunt OverflowError</args></response>]\n",
      "\n",
      "Processing call [048] out of [100] = [48.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,289 ms\n",
      "Tokens per second [58.4]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Explore best hiking trails near me on Google from this location</args></response>]\n",
      "\n",
      "Processing call [049] out of [100] = [49.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,645 ms\n",
      "Tokens per second [54.8]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Probe for Bytes Warning</args></response>]\n",
      "\n",
      "Processing call [050] out of [100] = [50.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,341 ms\n",
      "Tokens per second [67.6]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Confusion Matrix interpretation: How do you interpret a confusion matrix in a classification problem?</args></response>]\n",
      "\n",
      "Processing call [051] out of [100] = [51.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,577 ms\n",
      "Tokens per second [56.7]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI in human resources</args></response>]\n",
      "\n",
      "Processing call [052] out of [100] = [52.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,751 ms\n",
      "Tokens per second [58.5]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Visualizing data with Pandas</args></response>]\n",
      "\n",
      "Processing call [053] out of [100] = [53.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,806 ms\n",
      "Tokens per second [58.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Aggregation functions in Pandas</args></response>]\n",
      "\n",
      "Processing call [054] out of [100] = [54.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,480 ms\n",
      "Tokens per second [55.6]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Runtime Error</args></response>]\n",
      "\n",
      "Processing call [055] out of [100] = [55.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,822 ms\n",
      "Tokens per second [56.7]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Exporting data from Pandas to Excel</args></response>]\n",
      "\n",
      "Processing call [056] out of [100] = [56.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,114 ms\n",
      "Tokens per second [61.6]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How can you resolve errors related to ZIP file operations in Python</args></response>]\n",
      "\n",
      "Processing call [057] out of [100] = [57.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,819 ms\n",
      "Tokens per second [59.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>User Warning: User-defined warning</args></response>]\n",
      "\n",
      "Processing call [058] out of [100] = [58.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,688 ms\n",
      "Tokens per second [61.8]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>AI in natural language generation</args></response>]\n",
      "\n",
      "Processing call [059] out of [100] = [59.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,213 ms\n",
      "Tokens per second [61.3]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How do you resolve issues when a required module is not found in Python?</args></response>]\n",
      "\n",
      "Processing call [060] out of [100] = [60.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,460 ms\n",
      "Tokens per second [68.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Apache Spark data processing: How does Apache Spark handle large-scale data processing efficiently?</args></response>]\n",
      "\n",
      "Processing call [061] out of [100] = [61.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,955 ms\n",
      "Tokens per second [55.5]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Rolling and expanding windows in Pandas</args></response>]\n",
      "\n",
      "Processing call [062] out of [100] = [62.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,470 ms\n",
      "Tokens per second [64.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Image Recognition technologies: How have image recognition technologies evolved in recent years?</args></response>]\n",
      "\n",
      "Processing call [063] out of [100] = [63.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,642 ms\n",
      "Tokens per second [68.1]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>What techniques can you use to resolve import errors that occur when Python cannot find a module or its members during import?</args></response>]\n",
      "\n",
      "Processing call [064] out of [100] = [64.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,748 ms\n",
      "Tokens per second [63.3]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Runtime Error: Runtime exception occurred</args></response>]\n",
      "\n",
      "Processing call [065] out of [100] = [65.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,692 ms\n",
      "Tokens per second [56.5]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Data imputation with Pandas</args></response>]\n",
      "\n",
      "Processing call [066] out of [100] = [66.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,532 ms\n",
      "Tokens per second [54.9]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>find new music</args></response>]\n",
      "\n",
      "Processing call [067] out of [100] = [67.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,536 ms\n",
      "Tokens per second [55.6]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>InterruptedError</args></response>]\n",
      "\n",
      "Processing call [068] out of [100] = [68.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,168 ms\n",
      "Tokens per second [66.0]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How do you handle arithmetic errors in Python, especially with numeric calculations?</args></response>]\n",
      "\n",
      "Processing call [069] out of [100] = [69.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,527 ms\n",
      "Tokens per second [64.1]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>AI in agriculture technology: How is AI being utilized in modern agricultural technology?</args></response>]\n",
      "\n",
      "Processing call [070] out of [100] = [70.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,575 ms\n",
      "Tokens per second [59.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Unicode Warning</args></response>]\n",
      "\n",
      "Processing call [071] out of [100] = [71.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,475 ms\n",
      "Tokens per second [63.3]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>How do you address errors related to attempting to access non-existent files in Python?</args></response>]\n",
      "\n",
      "Processing call [072] out of [100] = [72.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,699 ms\n",
      "Tokens per second [58.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Google Cloud AI services</args></response>]\n",
      "\n",
      "Processing call [073] out of [100] = [73.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,579 ms\n",
      "Tokens per second [63.2]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Feature Engineering best practices</args></response>]\n",
      "\n",
      "Processing call [074] out of [100] = [74.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,514 ms\n",
      "Tokens per second [55.3]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AssertionError</args></response>]\n",
      "\n",
      "Processing call [075] out of [100] = [75.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,781 ms\n",
      "Tokens per second [55.0]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Performance tuning in Pandas</args></response>]\n",
      "\n",
      "Processing call [076] out of [100] = [76.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,802 ms\n",
      "Tokens per second [55.5]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI-driven chatbots: How are AI-driven chatbots enhancing customer service experiences?</args></response>]\n",
      "\n",
      "Processing call [077] out of [100] = [77.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,690 ms\n",
      "Tokens per second [53.2]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>best sci-fi novels</args></response>]\n",
      "\n",
      "Processing call [078] out of [100] = [78.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,814 ms\n",
      "Tokens per second [61.5]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Frequency conversion in time series data</args></response>]\n",
      "\n",
      "Processing call [079] out of [100] = [79.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,457 ms\n",
      "Tokens per second [55.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>SyntaxError</args></response>]\n",
      "\n",
      "Processing call [080] out of [100] = [80.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,690 ms\n",
      "Tokens per second [54.3]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>why cats purr</args></response>]\n",
      "\n",
      "Processing call [081] out of [100] = [81.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,044 ms\n",
      "Tokens per second [59.1]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>User Warning: User-defined warning in a new tab</args></response>]\n",
      "\n",
      "Processing call [082] out of [100] = [82.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,579 ms\n",
      "Tokens per second [57.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>FileExistsError</args></response>]\n",
      "\n",
      "Processing call [083] out of [100] = [83.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,692 ms\n",
      "Tokens per second [58.3]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Using lambda functions in Pandas</args></response>]\n",
      "\n",
      "Processing call [084] out of [100] = [84.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,348 ms\n",
      "Tokens per second [64.5]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>How do you manage situations leading to an unexpected exit of generators in Python?</args></response>]\n",
      "\n",
      "Processing call [085] out of [100] = [85.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,573 ms\n",
      "Tokens per second [58.5]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Explainable AI: Why is explainable AI important, and how is it achieved?</args></response>]\n",
      "\n",
      "Processing call [086] out of [100] = [86.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,272 ms\n",
      "Tokens per second [65.7]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>What steps can be taken to resolve errors related to aborted connections in Python</args></response>]\n",
      "\n",
      "Processing call [087] out of [100] = [87.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,644 ms\n",
      "Tokens per second [57.9]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>File Not Found Error</args></response>]\n",
      "\n",
      "Processing call [088] out of [100] = [88.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,642 ms\n",
      "Tokens per second [58.7]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Connection Reset Error results</args></response>]\n",
      "\n",
      "Processing call [089] out of [100] = [89.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,531 ms\n",
      "Tokens per second [62.0]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>What are the causes of floating point errors in Python, and how can they be minimized?</args></response>]\n",
      "\n",
      "Processing call [090] out of [100] = [90.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,477 ms\n",
      "Tokens per second [58.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI ethics in research: What are the key ethical considerations in AI research?</args></response>]\n",
      "\n",
      "Processing call [091] out of [100] = [91.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,761 ms\n",
      "Tokens per second [59.8]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>latest breakthroughs in medicine</args></response>]\n",
      "\n",
      "Processing call [092] out of [100] = [92.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,719 ms\n",
      "Tokens per second [55.9]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Data manipulation in Pandas</args></response>]\n",
      "\n",
      "Processing call [093] out of [100] = [93.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,405 ms\n",
      "Tokens per second [66.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>What are the best practices for managing timeout errors in Python, especially in network requests?</args></response>]\n",
      "\n",
      "Processing call [094] out of [100] = [94.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,083 ms\n",
      "Tokens per second [54.5]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI-driven chatbots on Google from this site</args></response>]\n",
      "\n",
      "Processing call [095] out of [100] = [95.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,760 ms\n",
      "Tokens per second [55.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Data anonymization in Pandas</args></response>]\n",
      "\n",
      "Processing call [096] out of [100] = [96.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,359 ms\n",
      "Tokens per second [63.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How do you handle situations where a feature or method is not yet implemented in Python?</args></response>]\n",
      "\n",
      "Processing call [097] out of [100] = [97.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,884 ms\n",
      "Tokens per second [57.6]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Investigate what is the stock market?</args></response>]\n",
      "\n",
      "Processing call [098] out of [100] = [98.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,698 ms\n",
      "Tokens per second [53.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>IBM Watson AI tools</args></response>]\n",
      "\n",
      "Processing call [099] out of [100] = [99.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 2,785 ms\n",
      "Tokens per second [57.5]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>System Error: Internal Python system issue</args></response>]\n",
      "\n",
      "Processing call [100] out of [100] = [100.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 3,289 ms\n",
      "Tokens per second [71.1]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Feature Engineering best practices: How does feature engineering improve the performance of machine learning models?</args></response>]\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "xml_ftp_generator = XmlFineTuningPromptGenerator( path_prefix=\"/var/model/genie-in-the-box\", debug=True, verbose=False )\n",
    "\n",
    "validation_df = xml_ftp_generator.generate_responses( validation_df, tokenizer=tokenizer, model=merged_model, switch=\"huggingface\", model_name=XXX_Agent.PHIND_34B_v2 )\n",
    "validation_df = xml_ftp_generator.validate_responses( validation_df )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:59:28.486243Z",
     "start_time": "2024-01-18T03:54:38.808046Z"
    }
   },
   "id": "9836a5fde028b874"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation Stats\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "          Contains response 100.0%\n",
      "   Contains browser command 100.0%\n",
      "              Contains args 100.0%\n",
      "          Response is exact 47.0%\n",
      "Response has correct values 47.0%\n",
      " Browser command is correct 60.0%\n",
      "            Args is correct 81.0%\n"
     ]
    }
   ],
   "source": [
    "xml_ftp_generator.print_validation_stats( validation_df )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:59:28.549999Z",
     "start_time": "2024-01-18T03:59:28.480491Z"
    }
   },
   "id": "bdd3f0dece4134d2"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "'/var/model/genie-in-the-box/src'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir( \"/var/model/genie-in-the-box/src\" )\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:51:22.286566Z",
     "start_time": "2024-01-14T20:51:22.282992Z"
    }
   },
   "id": "2933dc0ac5dcd1df"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands file for [search new tab] exists: True\n",
      "Commands file for [search current tab] exists: True\n",
      "Commands file for [search google new tab] exists: True\n",
      "Commands file for [search google current tab] exists: True\n",
      "Commands file for [search google scholar new tab] exists: True\n",
      "Commands file for [search google scholar current tab] exists: True\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation Stats\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "          Contains response 100.0%\n",
      "   Contains browser command 100.0%\n",
      "              Contains args 100.0%\n",
      "          Response is exact 41.0%\n",
      "Response has correct values 41.0%\n",
      " Browser command is correct 48.0%\n",
      "            Args is correct 78.0%\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from ephemera.prompts.xml_fine_tuning_prompt_generator import XmlFineTuningPromptGenerator\n",
    "\n",
    "# # Advice from phind when auto reload fails: https://www.phind.com/search?cache=l7c51x1v3tids6l43mgkkm9j\n",
    "# os.chdir( \"/var/model/genie-in-the-box/src\" )\n",
    "# %run \"ephemera/prompts/xml_fine_tuning_prompt_generator.py\"  \n",
    "\n",
    "xml_ftp_generator = XmlFineTuningPromptGenerator( path_prefix=\"/var/model/genie-in-the-box\" )\n",
    "\n",
    "validation_df = xml_ftp_generator.validate_prompts_and_responses( validation_df )\n",
    "\n",
    "xml_ftp_generator.print_validation_stats( validation_df )\n",
    "\n",
    "#  Results from a 1000 sample run:\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation Stats: After training adapter --or-- After loading pre-trained adapter and adapter\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#    Contains browser command 100.0%\n",
    "#               Contains args 100.0%\n",
    "#           Response is exact 99.3%\n",
    "# Response has correct values 99.3%\n",
    "#  Browser command is correct 99.6%\n",
    "#             Args is correct 99.7%\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation Stats: After naÃ¯vely merging adapter and model\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#    Contains browser command 100.0%\n",
    "#               Contains args 100.0%\n",
    "#           Response is exact 54.0%\n",
    "# Response has correct values 54.0%\n",
    "#  Browser command is correct 61.0%\n",
    "#             Args is correct 83.0%\n",
    "# \n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation Stats: Baseline, without adapter or training\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#    Contains browser command 100.0%\n",
    "#               Contains args 100.0%\n",
    "#           Response is exact 46.0%\n",
    "# Response has correct values 46.0%\n",
    "#  Browser command is correct 54.0%\n",
    "#             Args is correct 78.0%\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation Stats: After dequantizing and merging adapter and model\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#    Contains browser command 100.0%\n",
    "#               Contains args 100.0%\n",
    "#           Response is exact 41.0%\n",
    "# Response has correct values 41.0%\n",
    "#  Browser command is correct 48.0%\n",
    "#             Args is correct 78.0%"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:51:22.476145Z",
     "start_time": "2024-01-14T20:51:22.289042Z"
    }
   },
   "id": "1b459e918a397994"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b9ec021e401e8645"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
