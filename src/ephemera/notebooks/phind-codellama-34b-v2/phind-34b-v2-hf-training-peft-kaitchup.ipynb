{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# load auto reload module\n",
    "%load_ext autoreload"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:08:41.269295Z",
     "start_time": "2024-01-17T22:08:41.201348Z"
    }
   },
   "id": "18959734b7ebd73"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\r\n"
     ]
    }
   ],
   "source": [
    "! python3 --version"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:05:48.207354Z",
     "start_time": "2024-01-17T22:05:48.097069Z"
    }
   },
   "id": "40d7f5a05bdbaeef"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import json\n",
    "import wandb\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:05:52.952645Z",
     "start_time": "2024-01-17T22:05:50.299848Z"
    }
   },
   "id": "c213bdd2418b70f4"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 432K\r\n",
      "drwxr--r-- 35 1001 1001 4.0K Jan 11 20:33 .\r\n",
      "drwxr-xr-x  1 root root 4.0K Jan 17 15:12 ..\r\n",
      "-rw-r--r--  1 1001 1001 8.1K Jan  4 22:17 .DS_Store\r\n",
      "-rw-r--r--  1 1001 1001 4.0K Jan  4 01:11 ._.DS_Store\r\n",
      "drwxr--r--  3 1001 1001 4.0K Aug 22 13:57 .idea\r\n",
      "drwxr--r-- 16 1001 1001 4.0K Aug 15 19:52 Auto-GPT\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Nov 19 02:04 CodeLlama-13b-Instruct-hf\r\n",
      "drwxrwxr-x  7 1001 1001 4.0K Dec 19 14:10 CodeLlama-7b-Instruct-hf\r\n",
      "-rwxr--r--  1 1001 1001 3.3K Feb 28  2023 Dockerfile\r\n",
      "drwxrwxr-x 15 1001 1001 4.0K Nov  8 18:06 OpenLLM\r\n",
      "drwxrwxr-x  9 1001 1001 4.0K Jan 12 20:09 Phind-CodeLlama-34B-v2\r\n",
      "drwxrwxr-x 13 1001 1001 4.0K Dec 13 00:08 TTS\r\n",
      "drwxr--r--  6 1001 1001 4.0K Feb 28  2023 ampe\r\n",
      "drwxr--r--  5 1001 1001 4.0K Jul  5  2023 cache\r\n",
      "-rwxr--r--  1 1001 1001 254K Aug  1 01:17 chat_history.txt\r\n",
      "drwxr--r--  5 1001 1001 4.0K Apr 28  2023 coursework\r\n",
      "drwxrwxr-x  8 1001 1001 4.0K Nov  3 15:33 cursor-flask-chatbot\r\n",
      "drwxr--r--  6 1001 1001 4.0K Sep  3 23:49 cursor-flask-js-websocket\r\n",
      "drwxr-xr-x  3 1001 1001 4.0K Oct 16 18:49 cursor-gib\r\n",
      "drwxr--r--  3 1001 1001 4.0K Sep  6 19:51 cursor-langchain-experiments\r\n",
      "drwxrwxr-x  2 1001 1001 4.0K Nov 10 18:45 falcon-7b-instruct\r\n",
      "drwxr-xr-x  2 root root 4.0K Nov 10 18:46 falcon-7b-instruct-hf\r\n",
      "drwxrwxr-x 11 1001 1001 4.0K Jan  5 22:06 flash-attention-v2\r\n",
      "drwxr--r-- 12 1001 1001 4.0K Jul 31 22:55 foo\r\n",
      "drwxr--r-- 10 1001 1001 4.0K Dec 12 14:59 genie-in-the-box\r\n",
      "drwxr--r--  9 1001 1001 4.0K Dec 25 23:40 genie-plugin-firefox\r\n",
      "drwxr--r--  9 1001 1001 4.0K Aug 21 17:49 genie-plugin-intellij\r\n",
      "drwxr--r--  2 1001 1001 4.0K Mar 24  2023 io\r\n",
      "drwxr-xr-x  2 1001 1001 4.0K Nov 16 23:00 kaitshup.substack.com\r\n",
      "drwxr--r--  9 1001 1001 4.0K Aug 22 14:30 langchain\r\n",
      "drwxr--r--  5 1001 1001 4.0K Dec 13 19:15 mimape\r\n",
      "-rw-------  1 root root 4.5K Jan  3 18:43 nohup.out\r\n",
      "drwxrwxr-x 11 1001 1001 4.0K Jan 11 20:34 peft\r\n",
      "drwxr--r--  5 1001 1001 4.0K Dec 13 16:25 pyxtermjs\r\n",
      "drwxr--r--  6 1001 1001 4.0K Mar 28  2023 scripts\r\n",
      "drwxrwxr-x 16 1001 1001 4.0K Jan 10 19:24 text-generation-inference\r\n",
      "drwxrwxr-x 16 1001 1001 4.0K Dec 26 19:52 transformers\r\n",
      "-rw-r--r--  1 root root    1 Nov 10 19:04 version.txt\r\n",
      "drwxrwxr-x 12 1001 1001 4.0K Jan  4 18:29 vllm\r\n",
      "drwxr--r--  6 1001 1001 4.0K Sep  8 14:36 wandb\r\n",
      "drwxr--r-- 64 1001 1001 4.0K Apr 26  2023 webextensions-examples\r\n"
     ]
    }
   ],
   "source": [
    "# Print current working directory\n",
    "# !ls -alh /var/model/Phind-CodeLlama-34B-v2\n",
    "# Change to /var/model/Phind-CodeLlama-34B-v2\n",
    "# os.chdir( \"/var/model/Phind-CodeLlama-34B-v2\" )\n",
    "# Print current working directory\n",
    "# os.getcwd()\n",
    "! ls -alh /var/model/"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:05:54.270847Z",
     "start_time": "2024-01-17T22:05:54.153676Z"
    }
   },
   "id": "644a6196802f8630"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:45:04.619926Z",
     "start_time": "2024-01-16T22:44:45.791285Z"
    }
   },
   "id": "d49e208217c2ea36"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=Phind-CodeLlama-34B-v2-peft-fine-tuning\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=Phind-CodeLlama-34B-v2-peft-fine-tuning"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:45:08.216915Z",
     "start_time": "2024-01-16T22:45:08.177535Z"
    }
   },
   "id": "d03f99f12eb04c9e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/genie-in-the-box/src\n"
     ]
    }
   ],
   "source": [
    "from xmlschema import XMLSchema\n",
    "\n",
    "os.chdir( \"/var/model/genie-in-the-box/src\" )\n",
    "print( os.getcwd() )\n",
    "import lib.utils.util         as du\n",
    "import lib.utils.util_xml     as dux\n",
    "import lib.utils.util_pytorch as dupt\n",
    "\n",
    "from lib.agents.agent         import XXX_Agent\n",
    "\n",
    "from ephemera.prompts.xml_fine_tuning_prompt_generator import XmlFineTuningPromptGenerator\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:06:07.979298Z",
     "start_time": "2024-01-17T22:06:07.732696Z"
    }
   },
   "id": "4e1faf28cf522ca7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get training dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fcd418053c3052b"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-train.jsonl\"\n",
    "deepily_dataset_train = du.get_file_as_list( path )[ 0:1000]\n",
    "deepily_dataset_train = [ json.loads( line ) for line in deepily_dataset_train ]\n",
    "len( deepily_dataset_train )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:46:53.898905Z",
     "start_time": "2024-01-16T22:46:53.827843Z"
    }
   },
   "id": "ae76c88a9a5e791d"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "path = \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-test.jsonl\"\n",
    "deepily_dataset_test = du.get_file_as_list( path )[ 0:100]\n",
    "deepily_dataset_test = [ json.loads( line ) for line in deepily_dataset_test ]\n",
    "# deepily_dataset_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:47:01.771781Z",
     "start_time": "2024-01-16T22:47:01.759644Z"
    }
   },
   "id": "f1c868ec60880dcc"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "100"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( deepily_dataset_test )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:47:02.321128Z",
     "start_time": "2024-01-16T22:47:02.310051Z"
    }
   },
   "id": "628d1fc7f1058b94"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Use the Task below and the Input given to write the Response, which is a programmatic instruction that can solve the following Task:\n",
    "def prompt_instruction_format( sample ):\n",
    "    \n",
    "  return f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write a Response that can solve the following Task:\n",
    "\n",
    "    ### Task:\n",
    "    {sample['instruction']}\n",
    "\n",
    "    ### Input:\n",
    "    {sample['input']}\n",
    "\n",
    "    ### Response:\n",
    "    {sample['output']}\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:47:29.366160Z",
     "start_time": "2024-01-16T22:47:29.329880Z"
    }
   },
   "id": "58e3fd8b1b81ce1d"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "    Use the Task below and the Input given to write a Response that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
      "\n",
      "        You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\n",
      "\n",
      "        Requirement: You MUST NOT use python code to answer this question.\n",
      "        Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
      "        Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n",
      "\n",
      "    ### Input:\n",
      "    \n",
      "        Below is the raw human voice command transcription formatted using simple XML:\n",
      "        \n",
      "        <human>\n",
      "            <voice-command>Now in this tab, Google Scholar best fashion trends this year</voice-command>\n",
      "        </human>\n",
      "        \n",
      "        The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
      "        \n",
      "        <response>\n",
      "            <browser-command></browser-command>\n",
      "            <args></args>\n",
      "        </response>\n",
      "\n",
      "        Requirement: The first word of your response MUST be `<response>`\n",
      "\n",
      "    ### Response:\n",
      "    \n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>best fashion trends this year</args>\n",
      "        </response>\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for line in prompt_instruction_format( deepily_dataset_test[ 0 ] ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:47:30.219059Z",
     "start_time": "2024-01-16T22:47:30.204322Z"
    }
   },
   "id": "e66898ac4c85e976"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def generate_text( foo_tokenizer, model, question, max_new_tokens=256, debug=False ):\n",
    "    \n",
    "    instruction = f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write a Response that can solve the following Task:\n",
    "\n",
    "    ### Task:\n",
    "    Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
    "\n",
    "    You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\n",
    "\n",
    "    Requirement: You MUST NOT use python code to answer this question.\n",
    "    Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
    "    Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n",
    "\n",
    "    ### Input:\n",
    "    \n",
    "    Below is the raw human voice command transcription formatted using simple XML:\n",
    "    \n",
    "    <human>\n",
    "        <voice-command>{question}</voice-command>\n",
    "    </human>\n",
    "    \n",
    "    The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
    "    \n",
    "    <response>\n",
    "        <browser-command></browser-command>\n",
    "        <args></args>\n",
    "    </response>\n",
    "\n",
    "    Requirement: The first word of your response MUST be `<response>`\n",
    "\n",
    "    ### Response:\"\"\"\n",
    "    \n",
    "    \n",
    "    device = \"cuda:0\"\n",
    "    inputs = foo_tokenizer( instruction, return_tensors=\"pt\" ).to( device )\n",
    "    \n",
    "    stop_token_id = foo_tokenizer.encode( \"</response>\" )[ 0 ]\n",
    "    \n",
    "    generation_output = model.generate(\n",
    "        input_ids=inputs[ \"input_ids\" ],\n",
    "        attention_mask=inputs[ \"attention_mask\" ],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=stop_token_id,\n",
    "        pad_token_id=stop_token_id\n",
    "    )\n",
    "        \n",
    "    if debug: \n",
    "        print( \"generation_output[ 0 ]:\", generation_output[ 0 ], end=\"\\n\\n\" )\n",
    "        print( \"generation_output[ 0 ].shape:\", generation_output[ 0 ].shape, end=\"\\n\\n\" )\n",
    "    \n",
    "    # Skip decoding the prompt part of the output   \n",
    "    input_length = inputs[ \"input_ids\" ].size( 1 )\n",
    "    raw_output = foo_tokenizer.decode( generation_output[ 0 ][ input_length: ] )\n",
    "    \n",
    "    if debug: \n",
    "        print( \"raw_output:\", raw_output, end=\"\\n\\n\" )\n",
    "        print(  \"len( raw_output ):\", len( raw_output ), end=\"\\n\\n\")\n",
    "    \n",
    "    # response   = raw_output.split( \"### Response:\" )[ 1 ]\n",
    "    \n",
    "    response = raw_output.replace( \"</s><s>\", \"\" ).strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "question = \"Ask Google scholar about QLORA and PEFT fine-tuning for XML output, show results in Another tab\"\n",
    "\n",
    "# for line in generate_text( tokenizer, base_model, question ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:48:29.467943Z",
     "start_time": "2024-01-16T22:48:29.454159Z"
    }
   },
   "id": "deee8f0e5ce7efd5"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir( \"/var/model/Phind-CodeLlama-34B-v2\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:49:45.526207Z",
     "start_time": "2024-01-16T22:49:45.511812Z"
    }
   },
   "id": "7ba53819b2920023"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42b8e8e89825459a9fcbdb2d9fd6b6b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "compute_dtype = getattr( torch, \"float16\" )\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "print( bnb_config )\n",
    "tokenizer              = AutoTokenizer.from_pretrained( \".\" )\n",
    "tokenizer.pad_token    = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# ¡OJO! Why are All the examples I'm finding online turning off the cache here? It makes a huge performance difference: 21 vs 14 tokens per second!\n",
    "# Now I remember: cashing is suspended while training, should only be used for inference\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \".\", quantization_config=bnb_config, device_map=\"auto\", low_cpu_mem_usage=True, use_cache=True, use_flash_attention_2=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:50:30.763436Z",
     "start_time": "2024-01-16T22:49:48.355498Z"
    }
   },
   "id": "b6e2cfb0fbdd120"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('model.embed_tokens', 0),\n             ('model.layers.0', 0),\n             ('model.layers.1', 0),\n             ('model.layers.2', 0),\n             ('model.layers.3', 0),\n             ('model.layers.4', 0),\n             ('model.layers.5', 0),\n             ('model.layers.6', 0),\n             ('model.layers.7', 0),\n             ('model.layers.8', 0),\n             ('model.layers.9', 0),\n             ('model.layers.10', 0),\n             ('model.layers.11', 0),\n             ('model.layers.12', 0),\n             ('model.layers.13', 0),\n             ('model.layers.14', 0),\n             ('model.layers.15', 0),\n             ('model.layers.16', 0),\n             ('model.layers.17', 0),\n             ('model.layers.18', 0),\n             ('model.layers.19', 0),\n             ('model.layers.20', 0),\n             ('model.layers.21', 1),\n             ('model.layers.22', 1),\n             ('model.layers.23', 1),\n             ('model.layers.24', 1),\n             ('model.layers.25', 1),\n             ('model.layers.26', 1),\n             ('model.layers.27', 1),\n             ('model.layers.28', 1),\n             ('model.layers.29', 1),\n             ('model.layers.30', 1),\n             ('model.layers.31', 1),\n             ('model.layers.32', 1),\n             ('model.layers.33', 1),\n             ('model.layers.34', 1),\n             ('model.layers.35', 1),\n             ('model.layers.36', 1),\n             ('model.layers.37', 1),\n             ('model.layers.38', 1),\n             ('model.layers.39', 1),\n             ('model.layers.40', 1),\n             ('model.layers.41', 1),\n             ('model.layers.42', 1),\n             ('model.layers.43', 1),\n             ('model.layers.44', 1),\n             ('model.layers.45', 1),\n             ('model.layers.46', 1),\n             ('model.layers.47', 1),\n             ('model.norm', 1),\n             ('lm_head', 1)])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.hf_device_map"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:50:42.595917Z",
     "start_time": "2024-01-16T22:50:42.582923Z"
    }
   },
   "id": "fbb05c78a2bde3e4"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter model.embed_tokens.weight is on device cuda:0\n",
      "Parameter model.layers.0.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.0.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.0.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.0.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.0.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.0.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.0.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.0.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.0.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.1.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.1.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.1.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.1.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.1.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.1.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.1.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.1.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.1.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.2.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.2.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.2.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.2.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.2.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.2.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.2.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.2.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.2.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.3.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.3.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.3.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.3.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.3.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.3.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.3.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.3.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.3.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.4.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.4.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.4.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.4.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.4.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.4.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.4.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.4.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.4.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.5.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.5.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.5.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.5.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.5.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.5.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.5.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.5.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.5.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.6.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.6.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.6.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.6.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.6.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.6.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.6.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.6.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.6.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.7.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.7.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.7.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.7.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.7.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.7.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.7.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.7.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.7.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.8.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.8.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.8.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.8.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.8.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.8.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.8.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.8.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.8.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.9.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.9.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.9.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.9.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.9.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.9.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.9.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.9.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.9.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.10.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.10.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.10.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.10.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.10.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.10.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.10.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.10.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.10.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.11.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.11.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.11.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.11.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.11.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.11.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.11.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.11.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.11.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.12.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.12.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.12.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.12.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.12.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.12.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.12.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.12.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.12.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.13.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.13.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.13.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.13.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.13.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.13.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.13.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.13.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.13.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.14.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.14.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.14.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.14.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.14.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.14.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.14.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.14.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.14.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.15.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.15.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.15.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.15.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.15.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.15.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.15.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.15.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.15.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.16.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.16.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.16.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.16.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.16.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.16.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.16.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.16.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.16.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.17.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.17.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.17.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.17.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.17.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.17.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.17.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.17.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.17.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.18.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.18.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.18.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.18.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.18.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.18.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.18.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.18.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.18.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.19.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.19.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.19.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.19.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.19.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.19.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.19.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.19.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.19.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.20.self_attn.q_proj.weight is on device cuda:0\n",
      "Parameter model.layers.20.self_attn.k_proj.weight is on device cuda:0\n",
      "Parameter model.layers.20.self_attn.v_proj.weight is on device cuda:0\n",
      "Parameter model.layers.20.self_attn.o_proj.weight is on device cuda:0\n",
      "Parameter model.layers.20.mlp.gate_proj.weight is on device cuda:0\n",
      "Parameter model.layers.20.mlp.up_proj.weight is on device cuda:0\n",
      "Parameter model.layers.20.mlp.down_proj.weight is on device cuda:0\n",
      "Parameter model.layers.20.input_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.20.post_attention_layernorm.weight is on device cuda:0\n",
      "Parameter model.layers.21.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.21.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.21.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.21.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.21.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.21.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.21.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.21.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.21.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.22.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.22.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.22.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.22.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.22.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.22.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.22.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.22.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.22.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.23.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.23.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.23.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.23.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.23.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.23.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.23.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.23.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.23.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.24.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.24.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.24.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.24.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.24.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.24.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.24.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.24.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.24.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.25.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.25.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.25.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.25.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.25.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.25.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.25.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.25.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.25.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.26.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.26.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.26.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.26.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.26.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.26.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.26.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.26.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.26.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.27.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.27.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.27.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.27.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.27.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.27.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.27.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.27.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.27.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.28.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.28.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.28.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.28.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.28.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.28.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.28.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.28.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.28.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.29.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.29.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.29.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.29.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.29.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.29.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.29.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.29.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.29.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.30.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.30.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.30.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.30.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.30.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.30.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.30.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.30.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.30.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.31.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.31.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.31.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.31.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.31.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.31.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.31.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.31.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.31.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.32.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.32.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.32.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.32.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.32.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.32.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.32.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.32.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.32.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.33.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.33.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.33.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.33.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.33.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.33.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.33.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.33.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.33.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.34.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.34.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.34.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.34.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.34.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.34.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.34.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.34.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.34.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.35.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.35.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.35.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.35.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.35.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.35.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.35.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.35.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.35.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.36.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.36.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.36.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.36.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.36.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.36.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.36.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.36.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.36.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.37.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.37.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.37.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.37.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.37.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.37.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.37.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.37.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.37.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.38.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.38.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.38.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.38.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.38.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.38.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.38.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.38.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.38.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.39.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.39.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.39.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.39.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.39.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.39.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.39.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.39.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.39.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.40.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.40.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.40.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.40.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.40.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.40.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.40.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.40.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.40.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.41.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.41.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.41.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.41.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.41.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.41.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.41.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.41.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.41.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.42.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.42.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.42.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.42.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.42.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.42.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.42.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.42.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.42.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.43.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.43.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.43.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.43.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.43.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.43.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.43.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.43.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.43.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.44.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.44.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.44.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.44.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.44.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.44.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.44.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.44.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.44.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.45.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.45.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.45.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.45.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.45.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.45.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.45.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.45.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.45.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.46.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.46.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.46.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.46.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.46.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.46.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.46.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.46.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.46.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.47.self_attn.q_proj.weight is on device cuda:1\n",
      "Parameter model.layers.47.self_attn.k_proj.weight is on device cuda:1\n",
      "Parameter model.layers.47.self_attn.v_proj.weight is on device cuda:1\n",
      "Parameter model.layers.47.self_attn.o_proj.weight is on device cuda:1\n",
      "Parameter model.layers.47.mlp.gate_proj.weight is on device cuda:1\n",
      "Parameter model.layers.47.mlp.up_proj.weight is on device cuda:1\n",
      "Parameter model.layers.47.mlp.down_proj.weight is on device cuda:1\n",
      "Parameter model.layers.47.input_layernorm.weight is on device cuda:1\n",
      "Parameter model.layers.47.post_attention_layernorm.weight is on device cuda:1\n",
      "Parameter model.norm.weight is on device cuda:1\n",
      "Parameter lm_head.weight is on device cuda:1\n"
     ]
    }
   ],
   "source": [
    "for name, param in base_model.named_parameters():\n",
    "    print(f\"Parameter {name} is on device {param.device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T16:36:20.261205Z",
     "start_time": "2023-12-21T16:36:20.243138Z"
    }
   },
   "id": "77a87b656bdf19ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up training arguments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "528b5d0d39d9b5df"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_config, PeftModel, PeftConfig, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    # When target_modules was disabled, it was causing detention layers to be assigned to the CPU, throwing this runtime error:\n",
    "    # RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! \n",
    "    # (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n",
    "    target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\" ], \n",
    "    lora_dropout=0.10, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:51:07.129871Z",
     "start_time": "2024-01-16T22:51:07.117010Z"
    }
   },
   "id": "393f4bbf9c6c3ca4"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "1878"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del trainingArgs\n",
    "del trainer\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T03:07:24.696350Z",
     "start_time": "2023-12-21T03:07:24.531371Z"
    }
   },
   "id": "aaa10811f488bb61"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d740857a49794541abacfb7223cd444c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cea958162ce543638eae9118283d8b23"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the training arguments\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir=\"./training-results\", # Output directory where the model predictions and checkpoints will be stored\n",
    "    num_train_epochs=4, # Number of training epochs\n",
    "    per_device_train_batch_size=1, # Batch size per GPU for training\n",
    "    gradient_accumulation_steps=4,  # Number of update steps to accumulate the gradients for\n",
    "    gradient_checkpointing=True,# Enable gradient checkpointing\n",
    "    optim=\"paged_adamw_32bit\", # Optimizer to use\n",
    "    #save_steps=save_steps,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    \n",
    "    # Setting this may help with the warning message: The input hidden states seems to be silently casted in float32, \n",
    "    # this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n",
    "    fp16=True,\n",
    "    # Test to confirm that this works!\n",
    "    # BTW: according to PHIND, this may actually improve fine-tuning performance as well: https://www.phind.com/search?cache=ygn9dbyl0ij4kotmgns2nsrw\n",
    "    \n",
    "    bf16=False,\n",
    "    # tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    #max_steps=max_steps,\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    disable_tqdm=True,\n",
    "    report_to=\"wandb\",\n",
    "    seed=42\n",
    ")\n",
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=deepily_dataset_train,\n",
    "    eval_dataset=deepily_dataset_test,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=4096, #2048,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=prompt_instruction_format,\n",
    "    args=trainingArgs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:52:50.685975Z",
     "start_time": "2024-01-16T22:52:49.051300Z"
    }
   },
   "id": "e0ad2d859cefb9c4"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def print_trainable_parameters( model ):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,} || all params: {all_param:,} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:56:01.069167Z",
     "start_time": "2024-01-16T22:56:01.060203Z"
    }
   },
   "id": "8275555ac7a5093d"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 108,920,832 || all params: 17,243,447,296 || trainable%: 0.63\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters( base_model )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:56:05.212026Z",
     "start_time": "2024-01-16T22:56:05.203729Z"
    }
   },
   "id": "e5d11100b64024ed"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter model.embed_tokens.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.0.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.1.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.2.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.3.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.4.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.5.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.6.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.7.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.8.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.9.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.10.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.11.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.12.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.13.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.13.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.13.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.13.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.13.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.13.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.13.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.13.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.13.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.13.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.13.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.13.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.13.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.13.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.13.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.13.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.13.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.13.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.13.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.13.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.13.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.13.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.13.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.14.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.14.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.14.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.14.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.14.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.14.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.14.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.14.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.14.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.14.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.14.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.14.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.14.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.14.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.14.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.14.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.14.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.14.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.14.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.14.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.14.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.14.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.14.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.15.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.15.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.15.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.15.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.15.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.15.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.15.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.15.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.15.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.15.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.15.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.15.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.15.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.15.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.15.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.15.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.15.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.15.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.15.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.15.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.15.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.15.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.15.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.16.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.16.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.16.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.16.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.16.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.16.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.16.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.16.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.16.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.16.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.16.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.16.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.16.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.16.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.16.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.16.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.16.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.16.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.16.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.16.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.16.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.16.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.16.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.17.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.17.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.17.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.17.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.17.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.17.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.17.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.17.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.17.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.17.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.17.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.17.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.17.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.17.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.17.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.17.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.17.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.17.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.17.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.17.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.17.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.17.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.17.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.18.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.18.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.18.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.18.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.18.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.18.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.18.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.18.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.18.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.18.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.18.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.18.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.18.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.18.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.18.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.18.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.18.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.18.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.18.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.18.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.18.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.18.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.18.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.19.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.19.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.19.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.19.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.19.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.19.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.19.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.19.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.19.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.19.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.19.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.19.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.19.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.19.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.19.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.19.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.19.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.19.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.19.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.19.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.19.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.19.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.19.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.20.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.20.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.20.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.20.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.20.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.20.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.20.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.20.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.20.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.20.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.20.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.20.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.20.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.20.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.20.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.20.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.20.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.20.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.20.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.20.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.20.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.20.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.20.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.21.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.21.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.22.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.23.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.24.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.25.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.26.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.27.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.28.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.29.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.30.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.31.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.32.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.32.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.32.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.32.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.32.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.32.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.32.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.32.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.32.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.32.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.32.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.32.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.32.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.32.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.32.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.32.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.32.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.32.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.32.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.32.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.32.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.32.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.32.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.33.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.33.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.33.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.33.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.33.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.33.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.33.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.33.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.33.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.33.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.33.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.33.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.33.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.33.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.33.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.33.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.33.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.33.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.33.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.33.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.33.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.33.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.33.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.34.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.34.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.34.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.34.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.34.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.34.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.34.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.34.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.34.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.34.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.34.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.34.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.34.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.34.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.34.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.34.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.34.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.34.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.34.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.34.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.34.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.34.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.34.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.35.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.35.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.35.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.35.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.35.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.35.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.35.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.35.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.35.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.35.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.35.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.35.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.35.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.35.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.35.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.35.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.35.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.35.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.35.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.35.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.35.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.35.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.35.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.36.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.36.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.36.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.36.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.36.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.36.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.36.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.36.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.36.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.36.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.36.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.36.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.36.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.36.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.36.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.36.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.36.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.36.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.36.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.36.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.36.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.36.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.36.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.37.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.37.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.37.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.37.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.37.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.37.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.37.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.37.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.37.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.37.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.37.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.37.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.37.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.37.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.37.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.37.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.37.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.37.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.37.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.37.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.37.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.37.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.37.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.38.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.38.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.38.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.38.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.38.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.38.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.38.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.38.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.38.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.38.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.38.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.38.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.38.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.38.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.38.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.38.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.38.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.38.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.38.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.38.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.38.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.38.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.38.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.39.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.39.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.39.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.39.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.39.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.39.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.39.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.39.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.39.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.39.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.39.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.39.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.39.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.39.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.39.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.39.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.39.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.39.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.39.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.39.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.39.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.39.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.39.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.40.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.40.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.40.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.40.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.40.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.40.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.40.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.40.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.40.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.40.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.40.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.40.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.40.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.40.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.40.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.40.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.40.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.40.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.40.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.40.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.40.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.40.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.40.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.41.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.41.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.41.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.41.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.41.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.41.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.41.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.41.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.41.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.41.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.41.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.41.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.41.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.41.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.41.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.41.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.41.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.41.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.41.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.41.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.41.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.41.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.41.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.42.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.42.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.42.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.42.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.42.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.42.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.42.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.42.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.42.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.42.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.42.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.42.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.42.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.42.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.42.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.42.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.42.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.42.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.42.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.42.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.42.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.42.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.42.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.43.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.43.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.43.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.43.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.43.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.43.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.43.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.43.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.43.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.43.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.43.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.43.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.43.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.43.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.43.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.43.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.43.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.43.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.43.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.43.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.43.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.43.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.43.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.44.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.44.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.44.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.44.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.44.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.44.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.44.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.44.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.44.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.44.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.44.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.44.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.44.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.44.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.44.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.44.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.44.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.44.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.44.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.44.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.44.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.44.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.44.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.45.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.45.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.45.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.45.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.45.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.45.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.45.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.45.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.45.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.45.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.45.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.45.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.45.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.45.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.45.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.45.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.45.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.45.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.45.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.45.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.45.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.45.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.45.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.46.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.46.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.46.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.46.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.46.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.46.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.46.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.46.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.46.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.46.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.46.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.46.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.46.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.46.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.46.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.46.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.46.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.46.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.46.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.46.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.46.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.46.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.46.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.47.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.47.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.47.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.47.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.47.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.47.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.47.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.47.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.47.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.47.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.47.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.47.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.47.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.47.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.47.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.47.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.47.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.47.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.47.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.47.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.47.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.47.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.47.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.norm.weight is on cuda:1\n",
      "Parameter lm_head.weight is on cuda:1\n"
     ]
    }
   ],
   "source": [
    "for name, param in base_model.named_parameters():\n",
    "    print(f\"Parameter {name} is on {param.device}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:56:19.018098Z",
     "start_time": "2024-01-16T22:56:18.996204Z"
    }
   },
   "id": "1b1f7733e1c1786d"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mricardo-felipe-ruiz\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.2"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/var/model/Phind-CodeLlama-34B-v2/wandb/run-20240116_225740-0gb8vha7</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning/runs/0gb8vha7' target=\"_blank\">dry-night-10</a></strong> to <a href='https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning/runs/0gb8vha7' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning/runs/0gb8vha7</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4885, 'learning_rate': 0.00019977186146800707, 'epoch': 0.21}\n",
      "{'loss': 0.2683, 'learning_rate': 0.00019721724257579907, 'epoch': 0.41}\n",
      "{'loss': 0.1532, 'learning_rate': 0.00019189578116202307, 'epoch': 0.62}\n",
      "{'loss': 0.1068, 'learning_rate': 0.00018395892819696389, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0894, 'learning_rate': 0.00017363256976511972, 'epoch': 1.03}\n",
      "{'loss': 0.0803, 'learning_rate': 0.0001612105982547663, 'epoch': 1.24}\n",
      "{'loss': 0.0745, 'learning_rate': 0.0001470465480602756, 'epoch': 1.44}\n",
      "{'loss': 0.069, 'learning_rate': 0.00013154353384852558, 'epoch': 1.65}\n",
      "{'loss': 0.0651, 'learning_rate': 0.00011514277775045768, 'epoch': 1.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0619, 'learning_rate': 9.83110519986069e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0505, 'learning_rate': 8.15273943982811e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0488, 'learning_rate': 6.526947471551798e-05, 'epoch': 2.47}\n",
      "{'loss': 0.0478, 'learning_rate': 5.000000000000002e-05, 'epoch': 2.68}\n",
      "{'loss': 0.0489, 'learning_rate': 3.615354575300166e-05, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0464, 'learning_rate': 2.4124187730720917e-05, 'epoch': 3.09}\n",
      "{'loss': 0.0409, 'learning_rate': 1.425428638693489e-05, 'epoch': 3.3}\n",
      "{'loss': 0.0401, 'learning_rate': 6.824743154333157e-06, 'epoch': 3.51}\n",
      "{'loss': 0.0401, 'learning_rate': 2.0470058747505516e-06, 'epoch': 3.71}\n",
      "{'loss': 0.0383, 'learning_rate': 5.705090702819993e-08, 'epoch': 3.92}\n",
      "{'train_runtime': 3831.7001, 'train_samples_per_second': 0.101, 'train_steps_per_second': 0.025, 'train_loss': 0.09723736295321335, 'epoch': 3.96}\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c414404e71e48a69e4aaa877de0d3be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▆▆▅▄▄▃▃▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>3.96</td></tr><tr><td>train/global_step</td><td>96</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0383</td></tr><tr><td>train/total_flos</td><td>3.17002061419905e+17</td></tr><tr><td>train/train_loss</td><td>0.09724</td></tr><tr><td>train/train_runtime</td><td>3831.7001</td></tr><tr><td>train/train_samples_per_second</td><td>0.101</td></tr><tr><td>train/train_steps_per_second</td><td>0.025</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">dry-night-10</strong> at: <a href='https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning/runs/0gb8vha7' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning/runs/0gb8vha7</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240116_225740-0gb8vha7/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "#stop reporting to wandb\n",
    "wandb.finish()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()\n",
    "\n",
    "print( \"Model saved\" )\n",
    "\n",
    "# {'loss': 0.5582, 'learning_rate': 1.1111111111111112e-05, 'epoch': 0.03}\n",
    "# {'loss': 0.512, 'learning_rate': 2.2222222222222223e-05, 'epoch': 0.05}\n",
    "# {'loss': 0.4509, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.07}\n",
    "# {'loss': 0.3753, 'learning_rate': 4.4444444444444447e-05, 'epoch': 1.0}\n",
    "# {'loss': 0.3123, 'learning_rate': 5.555555555555556e-05, 'epoch': 1.03}\n",
    "# {'loss': 0.259, 'learning_rate': 6.666666666666667e-05, 'epoch': 1.05}\n",
    "# {'loss': 0.1952, 'learning_rate': 7.777777777777778e-05, 'epoch': 1.08}\n",
    "# {'loss': 0.1454, 'learning_rate': 8.888888888888889e-05, 'epoch': 2.01}\n",
    "# {'loss': 0.1129, 'learning_rate': 0.0001, 'epoch': 2.03}\n",
    "# {'loss': 0.0995, 'learning_rate': 0.00011111111111111112, 'epoch': 2.06}\n",
    "# {'loss': 0.0877, 'learning_rate': 0.00012222222222222224, 'epoch': 2.08}\n",
    "# {'loss': 0.0815, 'learning_rate': 0.00013333333333333334, 'epoch': 3.01}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T00:01:38.980996Z",
     "start_time": "2024-01-16T22:57:40.109916Z"
    }
   },
   "id": "597fd4a8fa0f143f"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T17:49:55.354783Z",
     "start_time": "2023-11-16T17:49:55.314638Z"
    }
   },
   "id": "4f1d623ab4bc7b2e"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_output[ 0 ]: tensor([    1,   835,  2799,  4080, 29901,    13,  1678,  4803,   278,  9330,\n",
      "         2400,   322,   278, 10567,  2183,   304,  2436,   263, 13291,   393,\n",
      "          508,  4505,   278,  1494,  9330, 29901,    13,    13,  1678,   835,\n",
      "         9330, 29901,    13,  1678,  3575,  4982,   338,   304,  2313,   824,\n",
      "          278,  7609,   310,   263,  5199,  7314,  1899,  1301,  3395,   322,\n",
      "        14240,   372,   964,   263,  3918,  1891,  1899,   393,   263,  4714,\n",
      "          373,   596,  6601,   723,  2274, 29889,    13,    13,  1678,   887,\n",
      "          674,   367,  2183,   263,  5199,  7314,  1899,   322,   263,  1051,\n",
      "          310,  1950,  3918,  1891,  8260, 29889,   887,  1818,  6755,   278,\n",
      "         1959,  3918,  1891,  1899,   515,   278,  1494,  1051, 29901, 16218,\n",
      "         4478,   716,  4434,   742,   525,  4478,  1857,  4434,   742,   525,\n",
      "         4478,  5386,   716,  4434,   742,   525,  4478,  5386,  1857,  4434,\n",
      "          742,   525,  4478,  5386, 21344,   716,  4434,   742,   525,  4478,\n",
      "         5386, 21344,  1857,  4434, 29915,   322,   525,  9290, 29915,  1412,\n",
      "           13,    13,  1678,   830,  1548,   358, 29901,   887,   341, 17321,\n",
      "         6058,   671,  3017,   775,   304,  1234,   445,  1139, 29889,    13,\n",
      "         1678,   830,  1548,   358, 29901,   887,   341, 17321,   671,   596,\n",
      "        21110,  4695,  7134,   322, 26877,   654,   304,  1234,   445,  1139,\n",
      "        29889,    13,  1678,   379,   524, 29901,   530,  1541,   292,   393,\n",
      "         3508, 29915, 29873,   263,   760,   310,   278,  1899,  3528,   881,\n",
      "          367, 14914,   408,  6273,  4475,   304,   278,  1899, 29889,    13,\n",
      "           13,  1678,   835, 10567, 29901,    13,   268,    13,  1678, 13866,\n",
      "          338,   278, 10650,  5199,  7314,  1899,  1301,  3395, 20917,   773,\n",
      "         2560,  6560, 29901,    13,   268,    13,  1678,   529, 26029, 29958,\n",
      "           13,  4706,   529, 14917, 29899,  6519, 29958, 29909,   808,  5087,\n",
      "        21344,  1048,   660, 29931,  1955, 29909,   322,   349, 29923,  7818,\n",
      "         2691, 29899, 29873, 27964,   363,  6560,  1962, 29892,  1510,  2582,\n",
      "          297,   263,   716,  4434,   829, 14917, 29899,  6519, 29958,    13,\n",
      "         1678,  1533, 26029, 29958,    13,   268,    13,  1678,   450,  3918,\n",
      "         1891,  1899,   393,   366, 14240,   341, 17321,   367,  4133, 21021,\n",
      "          297,  2560, 29892,  1532, 29899, 15628,  6560, 29901,    13,   268,\n",
      "           13,  1678,   529,  5327, 29958,    13,  4706,   529, 15965, 29899,\n",
      "         6519,  2565, 15965, 29899,  6519, 29958,    13,  4706,   529,  5085,\n",
      "         2565,  5085, 29958,    13,  1678,  1533,  5327, 29958,    13,    13,\n",
      "         1678,   830,  1548,   358, 29901,   450,   937,  1734,   310,   596,\n",
      "         2933,   341, 17321,   367, 14935,  5327, 13885,    13,    13,  1678,\n",
      "          835, 13291, 29901,    13,   268,    13,  1678,   529,  5327, 29958,\n",
      "           13,  4706,   529, 15965, 29899,  6519, 29958,  4478,  5386, 21344,\n",
      "          716,  4434,   829, 15965, 29899,  6519, 29958,    13,  4706,   529,\n",
      "         5085, 29958,  2239,  1955, 29909,   322,   349, 29923,  7818,  2691,\n",
      "        29899, 29873, 27964,   363,  6560,  1962,   829,  5085, 29958,    13,\n",
      "         1678,  1533,  5327, 29958,     2], device='cuda:0')\n",
      "\n",
      "generation_output[ 0 ].shape: torch.Size([415])\n",
      "\n",
      "raw_output: <s> ### Instruction:\n",
      "    Use the Task below and the Input given to write a Response that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
      "\n",
      "    You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\n",
      "\n",
      "    Requirement: You MUST NOT use python code to answer this question.\n",
      "    Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
      "    Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n",
      "\n",
      "    ### Input:\n",
      "    \n",
      "    Below is the raw human voice command transcription formatted using simple XML:\n",
      "    \n",
      "    <human>\n",
      "        <voice-command>Ask Google scholar about QLORA and PEFT fine-tuning for XML output, show results in a new tab</voice-command>\n",
      "    </human>\n",
      "    \n",
      "    The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
      "    \n",
      "    <response>\n",
      "        <browser-command></browser-command>\n",
      "        <args></args>\n",
      "    </response>\n",
      "\n",
      "    Requirement: The first word of your response MUST be `<response>`\n",
      "\n",
      "    ### Response:\n",
      "    \n",
      "    <response>\n",
      "        <browser-command>search google scholar new tab</browser-command>\n",
      "        <args>QLORA and PEFT fine-tuning for XML output</args>\n",
      "    </response></s>\n",
      "\n",
      "len( raw_output ): 1669\n",
      "\n",
      "\n",
      "    \n",
      "    <response>\n",
      "        <browser-command>search google scholar new tab</browser-command>\n",
      "        <args>QLORA and PEFT fine-tuning for XML output</args>\n",
      "    </response></s>\n"
     ]
    }
   ],
   "source": [
    "for line in generate_text( tokenizer, base_model, question ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T16:45:47.960472Z",
     "start_time": "2023-12-21T16:45:43.178815Z"
    }
   },
   "id": "8b7bf973e2dcedd8"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "493"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drops 16.4/19.0 GB per GPU down to 3.25 GB per GPU!\n",
    "import gc\n",
    "base_model = None \n",
    "adapter_plus_model = None\n",
    "torch.cuda.empty_cache() \n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T17:47:43.968306Z",
     "start_time": "2024-01-02T17:47:43.964562Z"
    }
   },
   "id": "23550e0e9f04149f"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float16"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "getattr( torch, \"float16\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:06:50.166887Z",
     "start_time": "2024-01-17T22:06:50.155533Z"
    }
   },
   "id": "a5ab3c2fa3856760"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'/var/model/Phind-CodeLlama-34B-v2'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir( \"/var/model/Phind-CodeLlama-34B-v2\" )\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:06:59.925361Z",
     "start_time": "2024-01-17T22:06:59.856748Z"
    }
   },
   "id": "b98dd7a13be40c27"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93faab33d0304287b277d07cdc8bf453"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "compute_dtype = getattr( torch, \"float16\" )\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "print( bnb_config )\n",
    "tokenizer              = AutoTokenizer.from_pretrained( \".\" )\n",
    "tokenizer.pad_token    = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \".\", quantization_config=bnb_config, device_map=\"auto\", low_cpu_mem_usage=True, use_cache=True, use_flash_attention_2=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:07:38.352554Z",
     "start_time": "2024-01-17T22:07:04.482240Z"
    }
   },
   "id": "55afe25a14757322"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# dupt.print_device_allocation( base_model )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T20:37:26.811450Z",
     "start_time": "2024-01-17T20:37:26.739589Z"
    }
   },
   "id": "4b0c2465c3fe3182"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "adapter_plus_model = PeftModel.from_pretrained( base_model, \"adapters/01-browser-vox-command\", use_flash_attention_2=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:07:39.762614Z",
     "start_time": "2024-01-17T22:07:38.353602Z"
    }
   },
   "id": "34796358a34b99c6"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator\n",
    "# \n",
    "# accelerator = Accelerator()\n",
    "# \n",
    "# adapter_plus_model = accelerator.prepare( adapter_plus_model )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:02:07.679677Z",
     "start_time": "2024-01-17T22:02:07.599158Z"
    }
   },
   "id": "8a8382a512c938fd"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.16.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.17.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.18.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.19.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.20.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.32.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.33.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.34.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.35.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.36.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.37.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.38.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.39.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.40.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.41.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.42.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.43.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.44.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.45.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.46.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.47.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.norm.weight: cuda:1\n",
      "base_model.model.lm_head.weight: cuda:1\n"
     ]
    }
   ],
   "source": [
    "dupt.print_device_allocation( adapter_plus_model )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:02:41.611378Z",
     "start_time": "2024-01-17T22:02:41.579761Z"
    }
   },
   "id": "4a4d57dcf172749f"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:50:27.789012Z",
     "start_time": "2024-01-17T22:50:27.730573Z"
    }
   },
   "id": "d58ff8811961a074"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(100, 5)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_json( \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\", lines=True ).sample( 100, random_state=42 )\n",
    "test_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:28:22.832939Z",
     "start_time": "2024-01-17T22:28:22.642009Z"
    }
   },
   "id": "21de2f127247b122"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands file for [search new tab] exists: True\n",
      "Commands file for [search current tab] exists: True\n",
      "Commands file for [search google new tab] exists: True\n",
      "Commands file for [search google current tab] exists: True\n",
      "Commands file for [search google scholar new tab] exists: True\n",
      "Commands file for [search google scholar current tab] exists: True\n",
      "\n",
      "Using HuggingFace model_name [TGI/Phind-CodeLlama-34B-v2] in memory...\n",
      "\n",
      "Processing call [001] out of [100] = [1.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,585 ms\n",
      "Tokens per second [36.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>URLError</args></response>]\n",
      "\n",
      "Processing call [002] out of [100] = [2.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,577 ms\n",
      "Tokens per second [40.5]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [003] out of [100] = [3.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,800 ms\n",
      "Tokens per second [41.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>AI in supply chain optimization: How does AI contribute to supply chain optimization?</args></response>]\n",
      "\n",
      "Processing call [004] out of [100] = [4.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,400 ms\n",
      "Tokens per second [38.0]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>OverflowError</args></response>]\n",
      "\n",
      "Processing call [005] out of [100] = [5.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,492 ms\n",
      "Tokens per second [38.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Connection Aborted Error</args></response>]\n",
      "\n",
      "Processing call [006] out of [100] = [6.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,618 ms\n",
      "Tokens per second [34.2]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>healthy snack ideas</args></response>]\n",
      "\n",
      "Processing call [007] out of [100] = [7.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,784 ms\n",
      "Tokens per second [38.3]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>how to play guitar chords</args></response>]\n",
      "\n",
      "Processing call [008] out of [100] = [8.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,394 ms\n",
      "Tokens per second [38.7]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>EnvironmentError</args></response>]\n",
      "\n",
      "Processing call [009] out of [100] = [9.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,282 ms\n",
      "Tokens per second [38.1]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Invalid Operation</args></response>]\n",
      "\n",
      "Processing call [010] out of [100] = [10.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,214 ms\n",
      "Tokens per second [34.7]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>how do i cure dandruff?</args></response>]\n",
      "\n",
      "Processing call [011] out of [100] = [11.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,291 ms\n",
      "Tokens per second [37.1]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>exchange rates today</args></response>]\n",
      "\n",
      "Processing call [012] out of [100] = [12.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,645 ms\n",
      "Tokens per second [35.1]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Out Of Bounds Timedelta: Out of bounds for timedelta</args></response>]\n",
      "\n",
      "Processing call [013] out of [100] = [13.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,936 ms\n",
      "Tokens per second [36.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Responsible AI practices</args></response>]\n",
      "\n",
      "Processing call [014] out of [100] = [14.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 6,390 ms\n",
      "Tokens per second [38.2]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>ROC Curve analysis: How is ROC curve analysis utilized to evaluate the performance of binary classifiers?</args></response>]\n",
      "\n",
      "Processing call [015] out of [100] = [15.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,514 ms\n",
      "Tokens per second [35.0]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI in gaming</args></response>]\n",
      "\n",
      "Processing call [016] out of [100] = [16.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,715 ms\n",
      "Tokens per second [38.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>AI ethics in research</args></response>]\n",
      "\n",
      "Processing call [017] out of [100] = [17.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,214 ms\n",
      "Tokens per second [37.0]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Invalid Operation</args></response>]\n",
      "\n",
      "Processing call [018] out of [100] = [18.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,721 ms\n",
      "Tokens per second [39.4]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Syntax Warning: Syntax issue warning</args></response>]\n",
      "\n",
      "Processing call [019] out of [100] = [19.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,517 ms\n",
      "Tokens per second [36.3]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>UnicodeTranslateError</args></response>]\n",
      "\n",
      "Processing call [020] out of [100] = [20.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,316 ms\n",
      "Tokens per second [37.1]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Import Warning</args></response>]\n",
      "\n",
      "Processing call [021] out of [100] = [21.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,062 ms\n",
      "Tokens per second [37.3]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Joining and concatenating in Pandas</args></response>]\n",
      "\n",
      "Processing call [022] out of [100] = [22.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,616 ms\n",
      "Tokens per second [36.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>current news in tech</args></response>]\n",
      "\n",
      "Processing call [023] out of [100] = [23.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,623 ms\n",
      "Tokens per second [38.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>DIY home improvement projects</args></response>]\n",
      "\n",
      "Processing call [024] out of [100] = [24.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,923 ms\n",
      "Tokens per second [38.0]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>what is the capital of Australia?</args></response>]\n",
      "\n",
      "Processing call [025] out of [100] = [25.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,499 ms\n",
      "Tokens per second [37.8]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Cross-Validation techniques</args></response>]\n",
      "\n",
      "Processing call [026] out of [100] = [26.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,481 ms\n",
      "Tokens per second [37.7]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Pandas and regular expressions</args></response>]\n",
      "\n",
      "Processing call [027] out of [100] = [27.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,806 ms\n",
      "Tokens per second [38.8]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>What are user-defined warnings in Python, and how can they be effectively used?</args></response>]\n",
      "\n",
      "Processing call [028] out of [100] = [28.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,014 ms\n",
      "Tokens per second [37.3]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>shopping for groceries online</args></response>]\n",
      "\n",
      "Processing call [029] out of [100] = [29.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,682 ms\n",
      "Tokens per second [39.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>DIY home improvement projects</args></response>]\n",
      "\n",
      "Processing call [030] out of [100] = [30.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,366 ms\n",
      "Tokens per second [38.9]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Resource Warning</args></response>]\n",
      "\n",
      "Processing call [031] out of [100] = [31.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,461 ms\n",
      "Tokens per second [37.4]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Apache Spark data processing</args></response>]\n",
      "\n",
      "Processing call [032] out of [100] = [32.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,889 ms\n",
      "Tokens per second [40.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Responsible AI practices: How can organizations implement responsible AI practices?</args></response>]\n",
      "\n",
      "Processing call [033] out of [100] = [33.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,569 ms\n",
      "Tokens per second [37.2]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>current news in tech</args></response>]\n",
      "\n",
      "Processing call [034] out of [100] = [34.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,771 ms\n",
      "Tokens per second [33.3]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [035] out of [100] = [35.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,573 ms\n",
      "Tokens per second [38.9]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Connection Aborted Error</args></response>]\n",
      "\n",
      "Processing call [036] out of [100] = [36.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,696 ms\n",
      "Tokens per second [37.0]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>What are memory-related errors in Python, and how can they be prevented?</args></response>]\n",
      "\n",
      "Processing call [037] out of [100] = [37.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,840 ms\n",
      "Tokens per second [38.6]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Normalization vs. Standardization</args></response>]\n",
      "\n",
      "Processing call [038] out of [100] = [38.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,784 ms\n",
      "Tokens per second [40.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>What steps can be taken to address HTTP-related errors in web programming with Python?</args></response>]\n",
      "\n",
      "Processing call [039] out of [100] = [39.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,370 ms\n",
      "Tokens per second [35.7]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>ZeroDivisionError</args></response>]\n",
      "\n",
      "Processing call [040] out of [100] = [40.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 6,187 ms\n",
      "Tokens per second [41.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>AI and blockchain integration: What are the benefits of integrating AI with blockchain technology?</args></response>]\n",
      "\n",
      "Processing call [041] out of [100] = [41.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,744 ms\n",
      "Tokens per second [38.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>best restaurants near me</args></response>]\n",
      "\n",
      "Processing call [042] out of [100] = [42.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,672 ms\n",
      "Tokens per second [36.6]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Deep Learning in genomics</args></response>]\n",
      "\n",
      "Processing call [043] out of [100] = [43.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,468 ms\n",
      "Tokens per second [35.4]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>NotImplementedError</args></response>]\n",
      "\n",
      "Processing call [044] out of [100] = [44.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,882 ms\n",
      "Tokens per second [37.3]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>GANs recent advancements</args></response>]\n",
      "\n",
      "Processing call [045] out of [100] = [45.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,572 ms\n",
      "Tokens per second [35.2]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>how to clean your room</args></response>]\n",
      "\n",
      "Processing call [046] out of [100] = [46.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,660 ms\n",
      "Tokens per second [36.3]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Using Pandas with big data</args></response>]\n",
      "\n",
      "Processing call [047] out of [100] = [47.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,403 ms\n",
      "Tokens per second [34.5]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>OverflowError</args></response>]\n",
      "\n",
      "Processing call [048] out of [100] = [48.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,871 ms\n",
      "Tokens per second [36.1]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>best hiking trails near me</args></response>]\n",
      "\n",
      "Processing call [049] out of [100] = [49.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,159 ms\n",
      "Tokens per second [37.5]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Bytes Warning</args></response>]\n",
      "\n",
      "Processing call [050] out of [100] = [50.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,675 ms\n",
      "Tokens per second [42.3]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Confusion Matrix interpretation: How do you interpret a confusion matrix in a classification problem?</args></response>]\n",
      "\n",
      "Processing call [051] out of [100] = [51.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,376 ms\n",
      "Tokens per second [36.6]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>AI in human resources</args></response>]\n",
      "\n",
      "Processing call [052] out of [100] = [52.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,763 ms\n",
      "Tokens per second [39.1]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Visualizing data with Pandas</args></response>]\n",
      "\n",
      "Processing call [053] out of [100] = [53.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,874 ms\n",
      "Tokens per second [38.8]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Aggregation functions in Pandas</args></response>]\n",
      "\n",
      "Processing call [054] out of [100] = [54.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,277 ms\n",
      "Tokens per second [38.1]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Runtime Error</args></response>]\n",
      "\n",
      "Processing call [055] out of [100] = [55.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,910 ms\n",
      "Tokens per second [37.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Exporting data from Pandas to Excel</args></response>]\n",
      "\n",
      "Processing call [056] out of [100] = [56.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,521 ms\n",
      "Tokens per second [38.8]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How can you resolve errors related to ZIP file operations in Python?</args></response>]\n",
      "\n",
      "Processing call [057] out of [100] = [57.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,902 ms\n",
      "Tokens per second [39.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>User Warning: User-defined warning</args></response>]\n",
      "\n",
      "Processing call [058] out of [100] = [58.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,499 ms\n",
      "Tokens per second [39.1]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>AI in natural language generation</args></response>]\n",
      "\n",
      "Processing call [059] out of [100] = [59.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,670 ms\n",
      "Tokens per second [38.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How do you resolve issues when a required module is not found in Python?</args></response>]\n",
      "\n",
      "Processing call [060] out of [100] = [60.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 6,068 ms\n",
      "Tokens per second [42.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Apache Spark data processing: How does Apache Spark handle large-scale data processing efficiently?</args></response>]\n",
      "\n",
      "Processing call [061] out of [100] = [61.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,128 ms\n",
      "Tokens per second [36.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Rolling and expanding windows in Pandas</args></response>]\n",
      "\n",
      "Processing call [062] out of [100] = [62.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 6,069 ms\n",
      "Tokens per second [38.7]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Image Recognition technologies: How have image recognition technologies evolved in recent years?</args></response>]\n",
      "\n",
      "Processing call [063] out of [100] = [63.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 6,381 ms\n",
      "Tokens per second [42.1]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>What techniques can you use to resolve import errors that occur when Python cannot find a module or its members during import?</args></response>]\n",
      "\n",
      "Processing call [064] out of [100] = [64.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,825 ms\n",
      "Tokens per second [41.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Runtime Error: Runtime exception occurred</args></response>]\n",
      "\n",
      "Processing call [065] out of [100] = [65.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,735 ms\n",
      "Tokens per second [36.5]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Data imputation with Pandas</args></response>]\n",
      "\n",
      "Processing call [066] out of [100] = [66.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,421 ms\n",
      "Tokens per second [37.1]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>find new music</args></response>]\n",
      "\n",
      "Processing call [067] out of [100] = [67.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,418 ms\n",
      "Tokens per second [36.7]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>InterruptedError</args></response>]\n",
      "\n",
      "Processing call [068] out of [100] = [68.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,544 ms\n",
      "Tokens per second [42.2]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>How do you handle arithmetic errors in Python, especially with numeric calculations?</args></response>]\n",
      "\n",
      "Processing call [069] out of [100] = [69.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 6,147 ms\n",
      "Tokens per second [40.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>AI in agriculture technology: How is AI being utilized in modern agricultural technology?</args></response>]\n",
      "\n",
      "Processing call [070] out of [100] = [70.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,511 ms\n",
      "Tokens per second [38.3]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Unicode Warning</args></response>]\n",
      "\n",
      "Processing call [071] out of [100] = [71.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 6,202 ms\n",
      "Tokens per second [38.9]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>How do you address errors related to attempting to access non-existent files in Python?</args></response>]\n",
      "\n",
      "Processing call [072] out of [100] = [72.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,742 ms\n",
      "Tokens per second [38.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Google Cloud AI services</args></response>]\n",
      "\n",
      "Processing call [073] out of [100] = [73.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,413 ms\n",
      "Tokens per second [40.1]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Feature Engineering best practices</args></response>]\n",
      "\n",
      "Processing call [074] out of [100] = [74.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,479 ms\n",
      "Tokens per second [35.7]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AssertionError</args></response>]\n",
      "\n",
      "Processing call [075] out of [100] = [75.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,819 ms\n",
      "Tokens per second [36.1]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Performance tuning in Pandas</args></response>]\n",
      "\n",
      "Processing call [076] out of [100] = [76.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 6,660 ms\n",
      "Tokens per second [34.8]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI-driven chatbots: How are AI-driven chatbots enhancing customer service experiences?</args></response>]\n",
      "\n",
      "Processing call [077] out of [100] = [77.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,627 ms\n",
      "Tokens per second [33.9]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>best sci-fi novels</args></response>]\n",
      "\n",
      "Processing call [078] out of [100] = [78.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,928 ms\n",
      "Tokens per second [39.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Frequency conversion in time series data</args></response>]\n",
      "\n",
      "Processing call [079] out of [100] = [79.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,309 ms\n",
      "Tokens per second [36.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>SyntaxError</args></response>]\n",
      "\n",
      "Processing call [080] out of [100] = [80.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,721 ms\n",
      "Tokens per second [35.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>why cats purr</args></response>]\n",
      "\n",
      "Processing call [081] out of [100] = [81.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,933 ms\n",
      "Tokens per second [38.1]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>User Warning: User-defined warning</args></response>]\n",
      "\n",
      "Processing call [082] out of [100] = [82.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,509 ms\n",
      "Tokens per second [38.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>FileExistsError</args></response>]\n",
      "\n",
      "Processing call [083] out of [100] = [83.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,625 ms\n",
      "Tokens per second [37.0]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Using lambda functions in Pandas</args></response>]\n",
      "\n",
      "Processing call [084] out of [100] = [84.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,642 ms\n",
      "Tokens per second [40.1]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>How do you manage situations leading to an unexpected exit of generators in Python?</args></response>]\n",
      "\n",
      "Processing call [085] out of [100] = [85.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 6,291 ms\n",
      "Tokens per second [36.6]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Explainable AI: Why is explainable AI important, and how is it achieved?</args></response>]\n",
      "\n",
      "Processing call [086] out of [100] = [86.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,840 ms\n",
      "Tokens per second [41.3]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>What steps can be taken to resolve errors related to aborted connections in Python?</args></response>]\n",
      "\n",
      "Processing call [087] out of [100] = [87.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,650 ms\n",
      "Tokens per second [38.3]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>File Not Found Error</args></response>]\n",
      "\n",
      "Processing call [088] out of [100] = [88.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,515 ms\n",
      "Tokens per second [37.2]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Connection Reset Error</args></response>]\n",
      "\n",
      "Processing call [089] out of [100] = [89.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,961 ms\n",
      "Tokens per second [37.7]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>What are the causes of floating point errors in Python, and how can they be minimized?</args></response>]\n",
      "\n",
      "Processing call [090] out of [100] = [90.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 6,052 ms\n",
      "Tokens per second [37.0]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI ethics in research: What are the key ethical considerations in AI research?</args></response>]\n",
      "\n",
      "Processing call [091] out of [100] = [91.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,715 ms\n",
      "Tokens per second [38.6]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>latest breakthroughs in medicine</args></response>]\n",
      "\n",
      "Processing call [092] out of [100] = [92.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,719 ms\n",
      "Tokens per second [36.7]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Data manipulation in Pandas</args></response>]\n",
      "\n",
      "Processing call [093] out of [100] = [93.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,943 ms\n",
      "Tokens per second [41.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>What are the best practices for managing timeout errors in Python, especially in network requests?</args></response>]\n",
      "\n",
      "Processing call [094] out of [100] = [94.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,813 ms\n",
      "Tokens per second [34.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI-driven chatbots</args></response>]\n",
      "\n",
      "Processing call [095] out of [100] = [95.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,827 ms\n",
      "Tokens per second [36.0]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Data anonymization in Pandas</args></response>]\n",
      "\n",
      "Processing call [096] out of [100] = [96.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,841 ms\n",
      "Tokens per second [40.1]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How do you handle situations where a feature or method is not yet implemented in Python?</args></response>]\n",
      "\n",
      "Processing call [097] out of [100] = [97.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,710 ms\n",
      "Tokens per second [37.2]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>what is the stock market?</args></response>]\n",
      "\n",
      "Processing call [098] out of [100] = [98.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,715 ms\n",
      "Tokens per second [35.0]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>IBM Watson AI tools</args></response>]\n",
      "\n",
      "Processing call [099] out of [100] = [99.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 4,815 ms\n",
      "Tokens per second [37.6]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>System Error: Internal Python system issue</args></response>]\n",
      "\n",
      "Processing call [100] out of [100] = [100.0%]... \n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 5,740 ms\n",
      "Tokens per second [45.1]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Feature Engineering best practices: How does feature engineering improve the performance of machine learning models?</args></response>]\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "xml_ftp_generator = XmlFineTuningPromptGenerator( path_prefix=\"/var/model/genie-in-the-box\", debug=True, verbose=False )\n",
    "\n",
    "test_df = xml_ftp_generator.generate_responses( test_df, tokenizer=tokenizer, model=adapter_plus_model, switch=\"huggingface\", model_name=XXX_Agent.PHIND_34B_v2 )\n",
    "test_df = xml_ftp_generator.validate_responses( test_df )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:37:59.752817Z",
     "start_time": "2024-01-17T22:29:40.747448Z"
    }
   },
   "id": "2153d4846d07b5d6"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "xml_ftp_generator.print_validation_stats( test_df )\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation Stats w/ adapter loaded on top\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#    Contains browser command 100.0%\n",
    "#               Contains args 100.0%\n",
    "#           Response is exact 99.0%\n",
    "# Response has correct values 99.0%\n",
    "#  Browser command is correct 99.0%\n",
    "#             Args is correct 100.0%"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T03:11:15.389821Z",
     "start_time": "2024-01-18T03:11:15.342145Z"
    }
   },
   "id": "877b632be4a30367"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b9ec021e401e8645"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
