{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import json\n",
    "import wandb\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:39:09.024727Z",
     "start_time": "2023-12-19T16:39:06.002014Z"
    }
   },
   "id": "c213bdd2418b70f4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'/var/model/Phind-CodeLlama-34B-v2'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print current working directory\n",
    "# !ls -alh /var/model/Phind-CodeLlama-34B-v2\n",
    "# Change to /var/model/Phind-CodeLlama-34B-v2\n",
    "os.chdir( \"/var/model/Phind-CodeLlama-34B-v2\" )\n",
    "# Print current working directory\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:39:09.033115Z",
     "start_time": "2023-12-19T16:39:09.027118Z"
    }
   },
   "id": "644a6196802f8630"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mricardo-felipe-ruiz\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:39:15.086132Z",
     "start_time": "2023-12-19T16:39:14.312514Z"
    }
   },
   "id": "d49e208217c2ea36"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=Phind-CodeLlama-34B-v2-peft-fine-tuning\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=Phind-CodeLlama-34B-v2-peft-fine-tuning"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:39:15.842448Z",
     "start_time": "2023-12-19T16:39:15.832792Z"
    }
   },
   "id": "d03f99f12eb04c9e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dataset_name = \"iamtarun/python_code_instructions_18k_alpaca\"\n",
    "split = \"train[:10%]\"\n",
    "# finetunes_model_name = \"output/codellama-7b-finetuned-int4-python-18k-alpaca\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:40:04.295904Z",
     "start_time": "2023-12-19T16:40:04.286710Z"
    }
   },
   "id": "90fba6c4ff2875f3"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset( dataset_name, split=split )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:40:05.672506Z",
     "start_time": "2023-12-19T16:40:04.836497Z"
    }
   },
   "id": "ccedc3adeb45dcb8"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "{'instruction': 'Create a function to calculate the sum of a sequence of integers.',\n 'input': '[1, 2, 3, 4, 5]',\n 'output': '# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum',\n 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate a function to calculate the sum of a sequence of integers.\\n\\n### Input:\\n[1, 2, 3, 4, 5]\\n\\n### Output:\\n# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum'}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[ 0 ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:43:34.202410Z",
     "start_time": "2023-12-19T16:43:34.158863Z"
    }
   },
   "id": "d4f6af0454318fbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Delete a row from dataset\n",
    "# del dataset[ 0 ]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f027c8563f604e5"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Use the Task below and the Input given to write the Response, which is a programmatic instruction that can solve the following Task:\n",
    "def prompt_instruction_format( sample ):\n",
    "    \n",
    "  return f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write a Response that can solve the following Task:\n",
    "\n",
    "    ### Task:\n",
    "    {sample['instruction']}\n",
    "\n",
    "    ### Input:\n",
    "    {sample['input']}\n",
    "\n",
    "    ### Response:\n",
    "    {sample['output']}\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:40:09.443781Z",
     "start_time": "2023-12-19T16:40:09.433123Z"
    }
   },
   "id": "58e3fd8b1b81ce1d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eb51e4483b75da5d"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Create a function to calculate the sum of a sequence of integers.\n",
      "\n",
      "    ### Input:\n",
      "    [1, 2, 3, 4, 5]\n",
      "\n",
      "    ### Response:\n",
      "    # Python code\n",
      "def sum_sequence(sequence):\n",
      "  sum = 0\n",
      "  for num in sequence:\n",
      "    sum += num\n",
      "  return sum\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for line in prompt_instruction_format( dataset[ 0 ] ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T17:46:10.446773Z",
     "start_time": "2023-12-19T17:46:10.403529Z"
    }
   },
   "id": "e66898ac4c85e976"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def generate_text( foo_tokenizer, model, product, max_new_tokens=128 ):\n",
    "    \n",
    "    instruction = f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
    "\n",
    "    ### Task:\n",
    "    Create a detailed description for the following product\n",
    "\n",
    "    ### Input:\n",
    "    {product}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    device = \"cuda:0\"\n",
    "    inputs = foo_tokenizer( instruction, return_tensors=\"pt\" ).to( device )\n",
    "    \n",
    "    generation_output = model.generate(\n",
    "        input_ids=inputs[ \"input_ids\" ],\n",
    "        attention_mask=inputs[ \"attention_mask\" ],\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "        \n",
    "    print( \"generation_output[ 0 ]:\", generation_output[ 0 ], end=\"\\n\\n\" )\n",
    "    print( \"generation_output[ 0 ].shape:\", generation_output[ 0 ].shape, end=\"\\n\\n\" )\n",
    "    \n",
    "    raw_output = foo_tokenizer.decode( generation_output[ 0 ] )\n",
    "    \n",
    "    print( \"raw_output:\", raw_output, end=\"\\n\\n\" )\n",
    "    print(  \"len( raw_output ):\", len( raw_output ), end=\"\\n\\n\")\n",
    "    \n",
    "    response   = raw_output.split( \"### Response:\" )[ 1 ]\n",
    "    \n",
    "    return response\n",
    "\n",
    "product = \"Corelogic Smooth Mouse, belonging to category: Optical Mouse\"\n",
    "\n",
    "# for line in generate_text( tokenizer, base_model, product ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:29:36.219200Z",
     "start_time": "2023-12-19T15:29:36.167771Z"
    }
   },
   "id": "deee8f0e5ce7efd5"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2e941df8f614ca3bc7016bc4b93b3aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "print( bnb_config )\n",
    "tokenizer              = AutoTokenizer.from_pretrained( \".\" )\n",
    "tokenizer.pad_token    = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# ¡OJO! Why are All the examples I'm finding online turning off the cache here? It makes a huge performance difference: 21 vs 14 tokens per second!\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \".\", quantization_config=bnb_config, device_map=\"auto\", low_cpu_mem_usage=True, use_cache=True, use_flash_attention_2=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:41:16.633146Z",
     "start_time": "2023-12-19T16:40:42.706495Z"
    }
   },
   "id": "b6e2cfb0fbdd120"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for line in generate_text( tokenizer, base_model, product ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcaaf8f2e4c38178"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def ppl_model( model, tokenizer, dataset ):\n",
    "    nlls = [ ]\n",
    "    max_length = 4096 #2048\n",
    "    stride = 512\n",
    "    for s in tqdm( range( len( dataset[ 'prompt' ] ) ) ):\n",
    "        encodings = tokenizer( dataset[ 'prompt' ][ s ], return_tensors=\"pt\", truncation=True, max_length=max_length )\n",
    "        seq_len = encodings.input_ids.size( 1 )\n",
    "        prev_end_loc = 0\n",
    "        for begin_loc in range( 0, seq_len, stride ):\n",
    "            end_loc = min( begin_loc + max_length, seq_len )\n",
    "            trg_len = end_loc - prev_end_loc\n",
    "            input_ids = encodings.input_ids[ :, begin_loc:end_loc ].to( \"cuda\" )\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[ :, :-trg_len ] = -100\n",
    "            with torch.no_grad():\n",
    "                outputs = model( input_ids, labels=target_ids )\n",
    "                neg_log_likelihood = outputs.loss\n",
    "            nlls.append( neg_log_likelihood )\n",
    "            prev_end_loc = end_loc\n",
    "            if end_loc == seq_len:\n",
    "                break\n",
    "    ppl = torch.exp( torch.stack( nlls ).mean() )\n",
    "    return ppl.item()\n",
    "\n",
    "# 4.7532477378845215"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:04:23.076516Z",
     "start_time": "2023-12-19T16:04:23.024881Z"
    }
   },
   "id": "fa2e407f1f9b833e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppl_model( base_model, tokenizer, dataset )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b32d8cac687b71db"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "1861"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( dataset )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:41:54.835829Z",
     "start_time": "2023-12-19T16:41:54.824274Z"
    }
   },
   "id": "5f09cc2f2ce1ad10"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7976\n",
      "5733\n",
      "13286\n",
      "9491\n",
      "4192\n",
      "10394\n",
      "8083\n",
      "6716\n",
      "5412\n",
      "9311\n",
      "9028\n",
      "8722\n",
      "4850\n",
      "5961\n",
      "5733\n",
      "8359\n",
      "6524\n",
      "6241\n",
      "13248\n",
      "9596\n",
      "13128\n",
      "15029\n",
      "10664\n",
      "13881\n",
      "24\n"
     ]
    },
    {
     "data": {
      "text/plain": "1837"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate dataset line by line and find all strings longer than 4096  \n",
    "count = 0\n",
    "temp = [ ]\n",
    "for s in range( len( dataset[ 'prompt' ] ) ):\n",
    "    \n",
    "    if len( dataset[ 'prompt' ][ s ] ) > 4096:\n",
    "        count += 1\n",
    "        print( len( dataset[ 'prompt' ][ s ] ) )\n",
    "    else:\n",
    "        temp.append( dataset[ s ] )\n",
    "        \n",
    "print( count )\n",
    "len( temp )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:46:27.172422Z",
     "start_time": "2023-12-19T16:46:24.517167Z"
    }
   },
   "id": "df80604c8fedf530"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "dataset = temp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:47:31.029500Z",
     "start_time": "2023-12-19T16:47:31.017244Z"
    }
   },
   "id": "6948145d1adefb1e"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "2802"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del base_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:25:52.209542Z",
     "start_time": "2023-12-19T16:25:52.011386Z"
    }
   },
   "id": "a6e40707f4760dd2"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "{'model.embed_tokens': 0,\n 'model.layers.0': 0,\n 'model.layers.1': 0,\n 'model.layers.2': 0,\n 'model.layers.3': 0,\n 'model.layers.4': 0,\n 'model.layers.5': 0,\n 'model.layers.6': 0,\n 'model.layers.7': 0,\n 'model.layers.8': 0,\n 'model.layers.9': 0,\n 'model.layers.10': 0,\n 'model.layers.11': 0,\n 'model.layers.12': 0,\n 'model.layers.13': 0,\n 'model.layers.14': 0,\n 'model.layers.15': 0,\n 'model.layers.16': 0,\n 'model.layers.17': 0,\n 'model.layers.18': 0,\n 'model.layers.19': 0,\n 'model.layers.20': 0,\n 'model.layers.21': 1,\n 'model.layers.22': 1,\n 'model.layers.23': 1,\n 'model.layers.24': 1,\n 'model.layers.25': 1,\n 'model.layers.26': 1,\n 'model.layers.27': 1,\n 'model.layers.28': 1,\n 'model.layers.29': 1,\n 'model.layers.30': 1,\n 'model.layers.31': 1,\n 'model.layers.32': 1,\n 'model.layers.33': 1,\n 'model.layers.34': 1,\n 'model.layers.35': 1,\n 'model.layers.36': 1,\n 'model.layers.37': 1,\n 'model.layers.38': 1,\n 'model.layers.39': 1,\n 'model.layers.40': 1,\n 'model.layers.41': 1,\n 'model.layers.42': 1,\n 'model.layers.43': 1,\n 'model.layers.44': 1,\n 'model.layers.45': 1,\n 'model.layers.46': 1,\n 'model.layers.47': 1,\n 'model.norm': 1,\n 'lm_head': 1}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.hf_device_map"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:29:14.092033Z",
     "start_time": "2023-12-19T16:29:14.044150Z"
    }
   },
   "id": "fbb05c78a2bde3e4"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# for name, param in base_model.named_parameters():\n",
    "#     print(f\"Parameter {name} is on device {param.device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T20:01:04.820757Z",
     "start_time": "2023-11-15T20:01:04.764619Z"
    }
   },
   "id": "77a87b656bdf19ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up training arguments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "528b5d0d39d9b5df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# base_model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a96c25af2cd7894"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_config, PeftModel, PeftConfig, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    # When target_modules was disabled, it was causing detention layers to be assigned to the CPU, throwing this runtime error:\n",
    "    # RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! \n",
    "    # (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n",
    "    target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\" ], \n",
    "    lora_dropout=0.10, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:47:55.120950Z",
     "start_time": "2023-12-19T16:47:55.113050Z"
    }
   },
   "id": "393f4bbf9c6c3ca4"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir=\"./training-results\", # Output directory where the model predictions and checkpoints will be stored\n",
    "    num_train_epochs=1, # Number of training epochs\n",
    "    per_device_train_batch_size=1, # Batch size per GPU for training\n",
    "    gradient_accumulation_steps=4,  # Number of update steps to accumulate the gradients for\n",
    "    gradient_checkpointing=True,# Enable gradient checkpointing\n",
    "    optim=\"paged_adamw_32bit\", # Optimizer to use\n",
    "    #save_steps=save_steps,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    # fp16=True,\n",
    "    bf16=False,\n",
    "    # tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    #max_steps=max_steps,\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    disable_tqdm=True,\n",
    "    report_to=\"wandb\",\n",
    "    seed=42\n",
    ")\n",
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=4096, #2048,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=prompt_instruction_format,\n",
    "    args=trainingArgs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:48:10.326147Z",
     "start_time": "2023-12-19T16:48:08.905571Z"
    }
   },
   "id": "e0ad2d859cefb9c4"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def print_trainable_parameters( model ):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,} || all params: {all_param:,} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:48:11.496784Z",
     "start_time": "2023-12-19T16:48:11.485104Z"
    }
   },
   "id": "8275555ac7a5093d"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 108,920,832 || all params: 17,243,447,296 || trainable%: 0.63\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters( base_model )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:48:13.035791Z",
     "start_time": "2023-12-19T16:48:13.020252Z"
    }
   },
   "id": "e5d11100b64024ed"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# for name, param in base_model.named_parameters():\n",
    "#     print(f\"Parameter {name} is on {param.device}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:23:18.392164Z",
     "start_time": "2023-12-19T16:23:18.334387Z"
    }
   },
   "id": "1b1f7733e1c1786d"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.0"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/var/model/Phind-CodeLlama-34B-v2/wandb/run-20231219_164854-8bq7ng79</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning/runs/8bq7ng79' target=\"_blank\">legendary-microwave-3</a></strong> to <a href='https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning/runs/8bq7ng79' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning/runs/8bq7ng79</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8568, 'learning_rate': 7.142857142857143e-05, 'epoch': 0.01}\n",
      "{'loss': 0.6902, 'learning_rate': 0.00014285714285714287, 'epoch': 0.02}\n",
      "{'loss': 0.5357, 'learning_rate': 0.00019999750800065415, 'epoch': 0.03}\n",
      "{'loss': 0.4925, 'learning_rate': 0.00019991030106398364, 'epoch': 0.04}\n",
      "{'train_runtime': 1982.9587, 'train_samples_per_second': 0.926, 'train_steps_per_second': 0.231, 'train_loss': 0.6177679002285004, 'epoch': 0.05}\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7393c56cfc74146a8857387aec2da2c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▃▄▆█</td></tr><tr><td>train/global_step</td><td>▁▃▅▇█</td></tr><tr><td>train/learning_rate</td><td>▁▅██</td></tr><tr><td>train/loss</td><td>█▅▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>0.05</td></tr><tr><td>train/global_step</td><td>24</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>0.4925</td></tr><tr><td>train/total_flos</td><td>8.00760415565906e+16</td></tr><tr><td>train/train_loss</td><td>0.61777</td></tr><tr><td>train/train_runtime</td><td>1982.9587</td></tr><tr><td>train/train_samples_per_second</td><td>0.926</td></tr><tr><td>train/train_steps_per_second</td><td>0.231</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">legendary-microwave-3</strong> at: <a href='https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning/runs/8bq7ng79' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/Phind-CodeLlama-34B-v2-peft-fine-tuning/runs/8bq7ng79</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20231219_164854-8bq7ng79/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "#stop reporting to wandb\n",
    "wandb.finish()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()\n",
    "\n",
    "print( \"Model saved\" )\n",
    "\n",
    "# {'loss': 0.8568, 'learning_rate': 7.142857142857143e-05, 'epoch': 0.01}\n",
    "# {'loss': 0.6902, 'learning_rate': 0.00014285714285714287, 'epoch': 0.02}\n",
    "# {'loss': 0.5357, 'learning_rate': 0.00019999750800065415, 'epoch': 0.03}\n",
    "# {'loss': 0.4925, 'learning_rate': 0.00019991030106398364, 'epoch': 0.04}\n",
    "# {'train_runtime': 1982.9587, 'train_samples_per_second': 0.926, 'train_steps_per_second': 0.231, 'train_loss': 0.6177679002285004, 'epoch': 0.05}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T17:22:03.889832Z",
     "start_time": "2023-12-19T16:48:54.424753Z"
    }
   },
   "id": "597fd4a8fa0f143f"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T17:49:55.354783Z",
     "start_time": "2023-11-16T17:49:55.314638Z"
    }
   },
   "id": "4f1d623ab4bc7b2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for line in generate_text( tokenizer, base_model, product ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b7bf973e2dcedd8"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "566"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drops 16.4/19.0 GB per GPU down to 3.25 GB per GPU!\n",
    "import gc\n",
    "# del base_model\n",
    "torch.cuda.empty_cache() \n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T17:24:15.942142Z",
     "start_time": "2023-12-19T17:24:15.933277Z"
    }
   },
   "id": "6db763ceb8f0bce3"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ec10342464b4fc281960d63d8642a02"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "print( bnb_config )\n",
    "tokenizer              = AutoTokenizer.from_pretrained( \".\" )\n",
    "tokenizer.pad_token    = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# ¡OJO! Why were we turning off the cash here? \n",
    "# We're not! It makes a huge performance difference: 21 vs 14 tokens per second!\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \".\", quantization_config=bnb_config, device_map=\"auto\", low_cpu_mem_usage=True, use_cache=True, use_flash_attention_2=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:49:20.704159Z",
     "start_time": "2023-11-17T17:49:16.934739Z"
    }
   },
   "id": "55afe25a14757322"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_output[ 0 ]: tensor([    1,   835,  2799,  4080, 29901,    13,  1678,  4803,   278,  9330,\n",
      "         2400,   322,   278, 10567,  2183,   304,  2436,   278, 13291, 29892,\n",
      "          607,   338,  1824, 29885,  2454, 15278,   393,   508,  4505,   278,\n",
      "         1494,  9330, 29901,    13,    13,  1678,   835,  9330, 29901,    13,\n",
      "         1678,  6204,   263, 13173,  6139,   363,   278,  1494,  3234,    13,\n",
      "           13,  1678,   835, 10567, 29901,    13,  1678,  2994,   295,   468,\n",
      "          293,  4116,  6983, 25992, 29892, 23329,   304,  7663, 29901, 20693,\n",
      "          936, 25992,    13,    13,  1678,   835, 13291, 29901,    13,   268,\n",
      "        29896, 29889, 10969,  4408, 29901,  2994,   295,   468,   293,  4116,\n",
      "         6983, 25992,    13,   268, 29906, 29889, 10969, 12953, 29901,   450,\n",
      "         2994,   295,   468,   293,  4116,  6983, 25992,   338,   263,  1880,\n",
      "        29899, 29567, 27070,  9495,  8688,   304,  3867,   263, 10597,   322,\n",
      "        18378,  7271,   363,  4160, 29889,   739,   338,   263,  1224, 24285,\n",
      "         9495,   393,   508,   367,  1304,   363,  5164,  8324, 29892,  3704,\n",
      "          330, 11500, 29892,  4863, 16278, 29892,   322,  1856,  3347,  2976,\n",
      "        29889,    13,   268, 29941, 29889, 10969, 17943, 29901, 20693,   936,\n",
      "        25992,    13,   268, 29946, 29889, 10969,  5169,  3698, 29901,    13,\n",
      "         4706,   334,  4116,  6983,   322, 18378, 10298,    13,  4706,   334,\n",
      "         5057, 29899, 29567, 27070, 23530,    13,  4706,   334,  2087,  5143,\n",
      "          519,   360,  2227,    13,  4706,   334,  3831,   271,  1821,   411,\n",
      "         3852,   322,  4326,    13,  4706,   334,  7073,   519],\n",
      "       device='cuda:0')\n",
      "\n",
      "generation_output[ 0 ].shape: torch.Size([208])\n",
      "\n",
      "raw_output: <s> ### Instruction:\n",
      "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Create a detailed description for the following product\n",
      "\n",
      "    ### Input:\n",
      "    Corelogic Smooth Mouse, belonging to category: Optical Mouse\n",
      "\n",
      "    ### Response:\n",
      "    1. Product Name: Corelogic Smooth Mouse\n",
      "    2. Product Description: The Corelogic Smooth Mouse is a high-quality optical mouse designed to provide a smooth and precise experience for users. It is a versatile mouse that can be used for various applications, including gaming, video editing, and web browsing.\n",
      "    3. Product Category: Optical Mouse\n",
      "    4. Product Features:\n",
      "        * Smooth and precise movement\n",
      "        * High-quality optical sensor\n",
      "        * Adjustable DPI\n",
      "        * Compatible with Windows and Mac\n",
      "        * Durable\n",
      "\n",
      "len( raw_output ): 867\n",
      "\n",
      "\n",
      "    1. Product Name: Corelogic Smooth Mouse\n",
      "    2. Product Description: The Corelogic Smooth Mouse is a high-quality optical mouse designed to provide a smooth and precise experience for users. It is a versatile mouse that can be used for various applications, including gaming, video editing, and web browsing.\n",
      "    3. Product Category: Optical Mouse\n",
      "    4. Product Features:\n",
      "        * Smooth and precise movement\n",
      "        * High-quality optical sensor\n",
      "        * Adjustable DPI\n",
      "        * Compatible with Windows and Mac\n",
      "        * Durable\n"
     ]
    }
   ],
   "source": [
    "for line in generate_text( tokenizer, base_model, product ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:49:29.101186Z",
     "start_time": "2023-11-17T17:49:23.142465Z"
    }
   },
   "id": "2bbddb04a5a998aa"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "48878"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drops 16.4/19.0 GB per GPU down to 3.25 GB per GPU!\n",
    "import gc\n",
    "del adapter_plus_model\n",
    "torch.cuda.empty_cache() \n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:48:59.572552Z",
     "start_time": "2023-11-17T17:48:59.562021Z"
    }
   },
   "id": "648a08233ae9bd2e"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "adapter_plus_model = PeftModel.from_pretrained( base_model, \"training-results/checkpoint-85\", use_flash_attention_2=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:49:42.509922Z",
     "start_time": "2023-11-17T17:49:41.967645Z"
    }
   },
   "id": "34796358a34b99c6"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_output[ 0 ]: tensor([    1,   835,  2799,  4080, 29901,    13,  1678,  4803,   278,  9330,\n",
      "         2400,   322,   278, 10567,  2183,   304,  2436,   278, 13291, 29892,\n",
      "          607,   338,  1824, 29885,  2454, 15278,   393,   508,  4505,   278,\n",
      "         1494,  9330, 29901,    13,    13,  1678,   835,  9330, 29901,    13,\n",
      "         1678,  6204,   263, 13173,  6139,   363,   278,  1494,  3234,    13,\n",
      "           13,  1678,   835, 10567, 29901,    13,  1678,  2994,   295,   468,\n",
      "          293,  4116,  6983, 25992, 29892, 23329,   304,  7663, 29901, 20693,\n",
      "          936, 25992,    13,    13,  1678,   835, 13291, 29901,    13,   268,\n",
      "           13,  1678,   450,  2994,   295,   468,   293,  4116,  6983, 25992,\n",
      "          338,   263,  1880, 29899, 29567, 27070,  9495,  8688,   363, 18378,\n",
      "          322, 10597, 10298, 29889,   739,  5680,   263, 12844,  1416,   322,\n",
      "        11071,  2874, 29892,  3907,   372,  4780,   304,  8677,  2820, 29889,\n",
      "          450,  9495,   756,   263, 29871, 29896, 29906, 29899, 22466,  6355,\n",
      "        18875,   322,   263, 29871, 29896, 29889, 29947, 29899, 22466, 27070,\n",
      "        23530,   393,  8128, 16232,   322, 18378, 10298, 29889,   739,   884,\n",
      "          756,   263, 29871, 29896, 29900, 29899,  3092,  2874,   411,   263,\n",
      "        29871, 29941, 29899,  3092,  6355, 18875, 29892, 14372,   363,   263,\n",
      "         9377,  3464,   310,  2888,  2133,  3987, 29889,   450,  9495,   338,\n",
      "        15878,   411,  3852,   322,  4326, 13598,  6757,   322,   338,  8688,\n",
      "          304,  3867,   263, 10597,   322,   409,   314,  2222,  1404,  7271,\n",
      "        29889,    13,   268,     2], device='cuda:0')\n",
      "\n",
      "generation_output[ 0 ].shape: torch.Size([204])\n",
      "\n",
      "raw_output: <s> ### Instruction:\n",
      "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Create a detailed description for the following product\n",
      "\n",
      "    ### Input:\n",
      "    Corelogic Smooth Mouse, belonging to category: Optical Mouse\n",
      "\n",
      "    ### Response:\n",
      "    \n",
      "    The Corelogic Smooth Mouse is a high-quality optical mouse designed for precise and smooth movement. It features a sleek and compact design, making it easy to carry around. The mouse has a 12-inch scroll wheel and a 1.8-inch optical sensor that provides accurate and precise movement. It also has a 10-button design with a 3-button scroll wheel, allowing for a wide range of customization options. The mouse is compatible with Windows and Mac operating systems and is designed to provide a smooth and seamless user experience.\n",
      "    </s>\n",
      "\n",
      "len( raw_output ): 875\n",
      "\n",
      "\n",
      "    \n",
      "    The Corelogic Smooth Mouse is a high-quality optical mouse designed for precise and smooth movement. It features a sleek and compact design, making it easy to carry around. The mouse has a 12-inch scroll wheel and a 1.8-inch optical sensor that provides accurate and precise movement. It also has a 10-button design with a 3-button scroll wheel, allowing for a wide range of customization options. The mouse is compatible with Windows and Mac operating systems and is designed to provide a smooth and seamless user experience.\n",
      "    </s>\n"
     ]
    }
   ],
   "source": [
    "for line in generate_text( tokenizer, adapter_plus_model, product ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:49:57.395913Z",
     "start_time": "2023-11-17T17:49:48.579002Z"
    }
   },
   "id": "d84a5998d65cb738"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dcb9efffadd2d939"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
