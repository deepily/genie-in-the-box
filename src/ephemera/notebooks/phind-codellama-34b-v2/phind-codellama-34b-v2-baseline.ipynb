{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:50:29.479838Z",
     "start_time": "2023-11-14T15:50:29.453680Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, pipeline, LlamaForCausalLM #AutoModelForCausalLM, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\r\n",
      "Built on Tue_Aug_15_22:02:13_PDT_2023\r\n",
      "Cuda compilation tools, release 12.2, V12.2.140\r\n",
      "Build cuda_12.2.r12.2/compiler.33191640_0\r\n"
     ]
    }
   ],
   "source": [
    "! nvcc --version"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:50:31.691587Z",
     "start_time": "2023-11-14T15:50:31.568321Z"
    }
   },
   "id": "4a5c8e542a172ae5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 63G\r\n",
      "drwxrwxr-x 5 1001 1001 4.0K Nov 13 19:09 .\r\n",
      "drwxr-xr-x 1 root root 4.0K Nov 14 15:49 ..\r\n",
      "-rw-r--r-- 1 1001 1001 6.1K Nov 13 19:09 .DS_Store\r\n",
      "-rw-r--r-- 1 1001 1001 4.0K Nov 13 19:09 ._.DS_Store\r\n",
      "drwxr-xr-x 2 root root 4.0K Nov 11 02:21 .ipynb_checkpoints\r\n",
      "-rw-rw-r-- 1 1001 1001 3.7K Nov  6 17:14 README.md\r\n",
      "drwxr-xr-x 2 root root 4.0K Nov 11 02:56 checkpoints\r\n",
      "-rw-rw-r-- 1 1001 1001  638 Nov  6 17:14 config.json\r\n",
      "-rw-rw-r-- 1 1001 1001  116 Nov  6 17:14 generation_config.json\r\n",
      "-rw-r--r-- 1 1001 1001  984 Nov  7 18:12 model.bak\r\n",
      "-rw-r--r-- 1 1001 1001  984 Nov  6 21:02 model.yaml\r\n",
      "drwxr-xr-x 3 root root 4.0K Nov 11 03:06 output\r\n",
      "-rw-rw-r-- 1 1001 1001 9.2G Nov  6 17:28 pytorch_model-00001-of-00007.bin\r\n",
      "-rw-rw-r-- 1 1001 1001 9.1G Nov  6 17:28 pytorch_model-00002-of-00007.bin\r\n",
      "-rw-rw-r-- 1 1001 1001 9.1G Nov  6 17:28 pytorch_model-00003-of-00007.bin\r\n",
      "-rw-rw-r-- 1 1001 1001 9.1G Nov  6 17:28 pytorch_model-00004-of-00007.bin\r\n",
      "-rw-rw-r-- 1 1001 1001 9.1G Nov  6 17:28 pytorch_model-00005-of-00007.bin\r\n",
      "-rw-rw-r-- 1 1001 1001 9.1G Nov  6 17:28 pytorch_model-00006-of-00007.bin\r\n",
      "-rw-rw-r-- 1 1001 1001 8.6G Nov  6 17:27 pytorch_model-00007-of-00007.bin\r\n",
      "-rw-rw-r-- 1 1001 1001  35K Nov  6 17:14 pytorch_model.bin.index.json\r\n",
      "-rw-rw-r-- 1 1001 1001  434 Nov  6 17:14 special_tokens_map.json\r\n",
      "-rw-rw-r-- 1 1001 1001 489K Nov  6 17:14 tokenizer.model\r\n",
      "-rw-rw-r-- 1 1001 1001  824 Nov  6 17:14 tokenizer_config.json\r\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /var/model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:50:40.809718Z",
     "start_time": "2023-11-14T15:50:40.687594Z"
    }
   },
   "id": "70e90a0602cf507a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "os.chdir( \"/var/model\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:50:42.841136Z",
     "start_time": "2023-11-14T15:50:42.836866Z"
    }
   },
   "id": "2a03ad86c37f7bc8"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a265eb673ec482cb7d6c89c2e3d6835"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"Phind/Phind-CodeLlama-34B-v2\"\n",
    "\n",
    "# bnb_8bit_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,\n",
    "#     llm_int8_threshold = 6.0,\n",
    "#     bnb_8bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "bnb_4bit_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    use_flash_attention_2=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained( \".\", use_fast=True )\n",
    "model_4bit = LlamaForCausalLM.from_pretrained( \".\", quantization_config=bnb_4bit_config, device_map=\"auto\", low_cpu_mem_usage=True )  \n",
    "# model_8bit = LlamaForCausalLM.from_pretrained( \".\", quantization_config=bnb_8bit_config, device_map=\"auto\", low_cpu_mem_usage=True )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:52:02.280025Z",
     "start_time": "2023-11-14T15:51:19.547941Z"
    }
   },
   "id": "ed0904132b945f19"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "text_generator_4bit = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_4bit, \n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# text_generator_4bit\n",
    "# text_generator_8bit = pipeline(\n",
    "#     \"text-generation\", \n",
    "#     model=model_8bit, \n",
    "#     tokenizer=tokenizer,\n",
    "#     torch_dtype=torch.float16, \n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# text_generator_8bit"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:52:02.287349Z",
     "start_time": "2023-11-14T15:52:02.281342Z"
    }
   },
   "id": "2e6bc8dfd3d1061c"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{'model.embed_tokens': 0,\n 'model.layers.0': 0,\n 'model.layers.1': 0,\n 'model.layers.2': 0,\n 'model.layers.3': 0,\n 'model.layers.4': 0,\n 'model.layers.5': 0,\n 'model.layers.6': 0,\n 'model.layers.7': 0,\n 'model.layers.8': 0,\n 'model.layers.9': 0,\n 'model.layers.10': 0,\n 'model.layers.11': 0,\n 'model.layers.12': 0,\n 'model.layers.13': 0,\n 'model.layers.14': 0,\n 'model.layers.15': 0,\n 'model.layers.16': 0,\n 'model.layers.17': 0,\n 'model.layers.18': 0,\n 'model.layers.19': 0,\n 'model.layers.20': 0,\n 'model.layers.21': 1,\n 'model.layers.22': 1,\n 'model.layers.23': 1,\n 'model.layers.24': 1,\n 'model.layers.25': 1,\n 'model.layers.26': 1,\n 'model.layers.27': 1,\n 'model.layers.28': 1,\n 'model.layers.29': 1,\n 'model.layers.30': 1,\n 'model.layers.31': 1,\n 'model.layers.32': 1,\n 'model.layers.33': 1,\n 'model.layers.34': 1,\n 'model.layers.35': 1,\n 'model.layers.36': 1,\n 'model.layers.37': 1,\n 'model.layers.38': 1,\n 'model.layers.39': 1,\n 'model.layers.40': 1,\n 'model.layers.41': 1,\n 'model.layers.42': 1,\n 'model.layers.43': 1,\n 'model.layers.44': 1,\n 'model.layers.45': 1,\n 'model.layers.46': 1,\n 'model.layers.47': 1,\n 'model.norm': 1,\n 'lm_head': 1}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4bit.hf_device_map\n",
    "# model_8bit.hf_device_map"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:52:02.299155Z",
     "start_time": "2023-11-14T15:52:02.283971Z"
    }
   },
   "id": "9d7c9e1309f95609"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 13 tokens/s w/ 4-bit, before peft fine tuning\n",
    "## 7 tokens/s w/ 8-bit, before peft fine tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e93a09dd637b4d6f"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_haversine(lat1, lon1, lat2, lon2):\n",
      "    import math\n",
      "\n",
      "    # convert decimal degrees to radians\n",
      "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
      "\n",
      "    # haversine formula\n",
      "    dlat = lat2 - lat1\n",
      "    dlon = lon2 - lon1\n",
      "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
      "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
      "\n",
      "    # 6371 km is the earth's radius\n",
      "    km = 6371 * c\n",
      "    return km\n",
      "\n",
      "def get_distance_matrix(locations):\n",
      "    import numpy as np\n",
      "\n",
      "    n = len(locations)\n",
      "    matrix = np.zeros((n, n))\n",
      "\n",
      "    for i in range(n):\n",
      "        for j in range(i+1, n):\n",
      "            lat1, lon1 = locations[i]\n",
      "            lat2, lon2 = locations[j]\n",
      "            d = get_haversine(lat1, lon1, lat2, lon2)\n",
      "            matrix[i, j] = d\n",
      "            matrix[j, i] = d\n",
      "\n",
      "    return matrix\n",
      "\n",
      "locations = [(40.7128, -74.0060), (34.0522, -118.2437), (51.5074, -0.1278)]\n",
      "matrix = get_distance_matrix(locations)\n",
      "print(matrix)\n",
      "CPU times: user 28.9 s, sys: 526 ms, total: 29.4 s\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text = \"def get_haversine(\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model_4bit.generate(**inputs, max_new_tokens=400)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:52:31.837072Z",
     "start_time": "2023-11-14T15:52:02.298813Z"
    }
   },
   "id": "c56db05d68f9e3c4"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\n",
      "\n",
      "### Response:\n",
      "The Corelogic Smooth Mouse is an optical mouse designed for seamless navigation and precision control. This sleek and compact gadget belongs to the category of optical mice, which use light sensors to detect movement and clicks. It is an ideal choice for users who require a high-quality mouse for their computer or laptop, offering both comfort and functionality.\n",
      "\n",
      "Key Features:\n",
      "\n",
      "1. Ergonomic Design: The Corelogic Smooth Mouse features a slim and lightweight design, ensuring a comfortable grip for extended use. The mouse is designed to fit perfectly in\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "prompt= \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\n",
    "\n",
    "### Response:\"\"\"\n",
    "# input_ids = tokenizer( prompt, return_tensors=\"pt\" ).input_ids\n",
    "\n",
    "device = \"cuda:0\"\n",
    "input_ids = tokenizer( prompt, return_tensors=\"pt\" ).to( device ).input_ids\n",
    "\n",
    "generation_output = model_4bit.generate(\n",
    "    input_ids=input_ids, max_new_tokens=128\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(generation_output[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T21:03:52.154875Z",
     "start_time": "2023-11-13T21:03:42.724242Z"
    }
   },
   "id": "823be7f75be62e83"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "transformers.models.llama.modeling_llama.LlamaForCausalLM"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type( model_4bit )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:54:48.215786Z",
     "start_time": "2023-11-14T15:54:48.187495Z"
    }
   },
   "id": "507b26c073d85c43"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "del model_4bit"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:56:42.152763Z",
     "start_time": "2023-11-14T15:56:42.125435Z"
    }
   },
   "id": "50f895cd6ba0cd97"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "1496"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T15:57:03.961312Z",
     "start_time": "2023-11-14T15:57:03.953843Z"
    }
   },
   "id": "56ac638066e87316"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ae1c40138a453493"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "afd66caef823211e"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# From https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "#Load the dataset from the HuggingFace Hub\n",
    "rd_ds = load_dataset( \"xiyuez/red-dot-design-award-product-description\" )\n",
    "\n",
    "#Convert to pandas dataframe for convenient processing\n",
    "rd_df = pd.DataFrame( rd_ds[ 'train' ] )\n",
    "\n",
    "#Combine the two attributes into an instruction string\n",
    "rd_df[ 'instruction' ] = 'Create a detailed description for the following product: ' + rd_df[ 'product' ] + ', belonging to category: ' + rd_df[ 'category' ]\n",
    "\n",
    "rd_df = rd_df[ [ 'instruction', 'description' ] ]\n",
    "\n",
    "#Get a 5000 sample subset for fine-tuning purposes\n",
    "rd_df_sample = rd_df.sample( n=5000, random_state=42 )\n",
    "\n",
    "#Define template and format data into the template for supervised fine-tuning\n",
    "template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "\n",
    "{}\n",
    "\n",
    "### Response:\\n\"\"\"\n",
    "\n",
    "rd_df_sample[ 'prompt' ] = rd_df_sample[ \"instruction\" ].apply( lambda x: template.format( x ) )\n",
    "rd_df_sample.rename( columns={ 'description': 'response' }, inplace=True )\n",
    "rd_df_sample[ 'response' ] = rd_df_sample[ 'response' ] + \"\\n### End\"\n",
    "rd_df_sample = rd_df_sample[ [ 'prompt', 'response' ] ]\n",
    "\n",
    "rd_df_sample[ 'text' ] = rd_df_sample[ \"prompt\" ] + rd_df_sample[ \"response\" ]\n",
    "rd_df_sample.drop( columns=[ 'prompt', 'response' ], inplace=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-13T21:00:23.604563Z"
    }
   },
   "id": "982036c4bfb4520b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "670f4024257719ec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
