{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import json\n",
    "import wandb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:06:33.188706Z",
     "start_time": "2023-12-19T15:06:30.101082Z"
    }
   },
   "id": "c213bdd2418b70f4"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'/var/model/Phind-CodeLlama-34B-v2'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print current working directory\n",
    "# !ls -alh /var/model/Phind-CodeLlama-34B-v2\n",
    "# Change to /var/model/Phind-CodeLlama-34B-v2\n",
    "os.chdir( \"/var/model/Phind-CodeLlama-34B-v2\" )\n",
    "# Print current working directory\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:12:19.582276Z",
     "start_time": "2023-12-19T15:12:19.554514Z"
    }
   },
   "id": "644a6196802f8630"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 39\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n\u001B[1;32m     37\u001B[0m product \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCorelogic Smooth Mouse, belonging to category: Optical Mouse\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 39\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m generate_text( tokenizer, base_model, product )\u001B[38;5;241m.\u001B[39msplit( \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m ): \u001B[38;5;28mprint\u001B[39m( line )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_text( foo_tokenizer, model, product, max_new_tokens=128 ):\n",
    "    \n",
    "    instruction = f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
    "\n",
    "    ### Task:\n",
    "    Create a detailed description for the following product\n",
    "\n",
    "    ### Input:\n",
    "    {product}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    device = \"cuda:0\"\n",
    "    inputs = foo_tokenizer( instruction, return_tensors=\"pt\" ).to( device )\n",
    "    \n",
    "    generation_output = model.generate(\n",
    "        input_ids=inputs[ \"input_ids\" ],\n",
    "        attention_mask=inputs[ \"attention_mask\" ],\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "        \n",
    "    print( \"generation_output[ 0 ]:\", generation_output[ 0 ], end=\"\\n\\n\" )\n",
    "    print( \"generation_output[ 0 ].shape:\", generation_output[ 0 ].shape, end=\"\\n\\n\" )\n",
    "    \n",
    "    raw_output = foo_tokenizer.decode( generation_output[ 0 ] )\n",
    "    \n",
    "    print( \"raw_output:\", raw_output, end=\"\\n\\n\" )\n",
    "    print(  \"len( raw_output ):\", len( raw_output ), end=\"\\n\\n\")\n",
    "    \n",
    "    response   = raw_output.split( \"### Response:\" )[ 1 ]\n",
    "    \n",
    "    return response\n",
    "\n",
    "product = \"Corelogic Smooth Mouse, belonging to category: Optical Mouse\"\n",
    "\n",
    "for line in generate_text( tokenizer, base_model, product ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:09:50.173041Z",
     "start_time": "2023-11-17T16:09:50.150487Z"
    }
   },
   "id": "d00502e6da1117f6"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "os.chdir( \"/var/model\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:09:51.420475Z",
     "start_time": "2023-11-17T16:09:51.412762Z"
    }
   },
   "id": "24974f39bed76473"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14G\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Nov 17 15:51 .\r\n",
      "drwxr-xr-x  1 root root 4.0K Nov 17 15:36 ..\r\n",
      "drwxr-xr-x  2 root root 4.0K Nov 11 03:01 .ipynb_checkpoints\r\n",
      "-rw-rw-r--  1 1001 1001 6.9K Nov  5 17:36 LICENSE\r\n",
      "-rw-rw-r--  1 1001 1001 6.1K Nov  5 17:36 README.md\r\n",
      "-rw-rw-r--  1 1001 1001 4.7K Nov  5 17:36 USE_POLICY.md\r\n",
      "-rw-r--r--  1 root root 9.3K Nov 11 02:12 code-llama-instruct-7b-peft.ipynb\r\n",
      "-rw-r--r--  1 1001 1001  97K Nov 11 03:08 code-llama-instruct-7b.ipynb\r\n",
      "-rw-rw-r--  1 1001 1001  646 Nov  5 17:36 config.json\r\n",
      "-rw-rw-r--  1 1001 1001  116 Nov  5 17:36 generation_config.json\r\n",
      "drwxr-xr-x  2 root root 4.0K Nov 17 15:47 merged\r\n",
      "-rw-rw-r--  1 1001 1001 9.3G Nov  5 17:42 model-00001-of-00002.safetensors\r\n",
      "-rw-rw-r--  1 1001 1001 3.3G Nov  5 17:40 model-00002-of-00002.safetensors\r\n",
      "-rw-rw-r--  1 1001 1001  25K Nov  5 17:36 model.safetensors.index.json\r\n",
      "-rw-rw-r--  1 1001 1001  24K Nov  5 17:36 pytorch_model.bin.index.json\r\n",
      "-rw-rw-r--  1 1001 1001  411 Nov  5 17:36 special_tokens_map.json\r\n",
      "-rw-r--r--  1 1001 1001  27K Nov 10 21:15 text-generation.ipynb\r\n",
      "-rw-------  1 1001 1001 1.1G Nov 10 18:44 tmpdknh986h\r\n",
      "-rw-rw-r--  1 1001 1001 1.8M Nov  5 17:36 tokenizer.json\r\n",
      "-rw-rw-r--  1 1001 1001 489K Nov  5 17:36 tokenizer.model\r\n",
      "-rw-rw-r--  1 1001 1001  749 Nov  5 17:36 tokenizer_config.json\r\n",
      "drwxr-xr-x  2 root root 4.0K Nov 17 15:51 training-results\r\n",
      "-rw-r--r--  1 1001 1001    1 Nov 10 18:43 version.txt\r\n",
      "drwxr-xr-x 11 root root 4.0K Nov 17 16:01 wandb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alh"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:09:53.165024Z",
     "start_time": "2023-11-17T16:09:53.038508Z"
    }
   },
   "id": "630b82887ec1f8c7"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:09:55.253165Z",
     "start_time": "2023-11-17T16:09:55.246628Z"
    }
   },
   "id": "d49e208217c2ea36"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=codellama-7b-instruct-hf-peft-fine-tuning\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=codellama-7b-instruct-hf-peft-fine-tuning"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:09:57.446223Z",
     "start_time": "2023-11-17T16:09:57.439020Z"
    }
   },
   "id": "d03f99f12eb04c9e"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "dataset_name = \"iamtarun/python_code_instructions_18k_alpaca\"\n",
    "split = \"train[:10%]\"\n",
    "finetunes_model_name = \"output/codellama-7b-finetuned-int4-python-18k-alpaca\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:09:58.340797Z",
     "start_time": "2023-11-17T16:09:58.338311Z"
    }
   },
   "id": "90fba6c4ff2875f3"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset( dataset_name, split=split )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:09:59.999257Z",
     "start_time": "2023-11-17T16:09:59.037328Z"
    }
   },
   "id": "ccedc3adeb45dcb8"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def prompt_instruction_format( sample ):\n",
    "    \n",
    "  return f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
    "\n",
    "    ### Task:\n",
    "    {sample['instruction']}\n",
    "\n",
    "    ### Input:\n",
    "    {sample['input']}\n",
    "\n",
    "    ### Response:\n",
    "    {sample['output']}\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:10:02.864735Z",
     "start_time": "2023-11-17T16:10:02.859941Z"
    }
   },
   "id": "58e3fd8b1b81ce1d"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e5cf184a3f341b5aa4c6de6c2a45291"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "print( bnb_config )\n",
    "tokenizer              = AutoTokenizer.from_pretrained( \".\" )\n",
    "tokenizer.pad_token    = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Â¡OJO! Why are we turning off the cash here? \n",
    "# We're not! It makes a huge performance difference: 21 vs 14 tokens per second!\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \".\", quantization_config=bnb_config, device_map=\"auto\", low_cpu_mem_usage=True, use_cache=True, use_flash_attention_2=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:10:09.458753Z",
     "start_time": "2023-11-17T16:10:04.672185Z"
    }
   },
   "id": "b6e2cfb0fbdd120"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_output[ 0 ]: tensor([    1,   835,  2799,  4080, 29901,    13,  1678,  4803,   278,  9330,\n",
      "         2400,   322,   278, 10567,  2183,   304,  2436,   278, 13291, 29892,\n",
      "          607,   338,  1824, 29885,  2454, 15278,   393,   508,  4505,   278,\n",
      "         1494,  9330, 29901,    13,    13,  1678,   835,  9330, 29901,    13,\n",
      "         1678,  6204,   263, 13173,  6139,   363,   278,  1494,  3234,    13,\n",
      "           13,  1678,   835, 10567, 29901,    13,  1678,  2994,   295,   468,\n",
      "          293,  4116,  6983, 25992, 29892, 23329,   304,  7663, 29901, 20693,\n",
      "          936, 25992,    13,    13,  1678,   835, 13291, 29901,    13,   268,\n",
      "        29896, 29889, 10969,  4408, 29901,  2994,   295,   468,   293,  4116,\n",
      "         6983, 25992,    13,   268, 29906, 29889, 10969, 12953, 29901,   450,\n",
      "         2994,   295,   468,   293,  4116,  6983, 25992,   338,   263,  1880,\n",
      "        29899, 29567, 27070,  9495,  8688,   304,  3867,   263, 10597,   322,\n",
      "        18378,  7271,   363,  4160, 29889,   739,   338,   263,  1224, 24285,\n",
      "         9495,   393,   508,   367,  1304,   363,  5164,  8324, 29892,  3704,\n",
      "          330, 11500, 29892,  4863, 16278, 29892,   322,  1856,  3347,  2976,\n",
      "        29889,    13,   268, 29941, 29889, 10969, 17943, 29901, 20693,   936,\n",
      "        25992,    13,   268, 29946, 29889, 10969,  5169,  3698, 29901,    13,\n",
      "         4706,   334,  4116,  6983,   322, 18378, 10298,    13,  4706,   334,\n",
      "         5057, 29899, 29567, 27070, 23530,    13,  4706,   334,  2087,  5143,\n",
      "          519,   360,  2227,    13,  4706,   334,  3831,   271,  1821,   411,\n",
      "         3852,   322,  4326,    13,  4706,   334,  7073,   519],\n",
      "       device='cuda:0')\n",
      "\n",
      "generation_output[ 0 ].shape: torch.Size([208])\n",
      "\n",
      "raw_output: <s> ### Instruction:\n",
      "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Create a detailed description for the following product\n",
      "\n",
      "    ### Input:\n",
      "    Corelogic Smooth Mouse, belonging to category: Optical Mouse\n",
      "\n",
      "    ### Response:\n",
      "    1. Product Name: Corelogic Smooth Mouse\n",
      "    2. Product Description: The Corelogic Smooth Mouse is a high-quality optical mouse designed to provide a smooth and precise experience for users. It is a versatile mouse that can be used for various applications, including gaming, video editing, and web browsing.\n",
      "    3. Product Category: Optical Mouse\n",
      "    4. Product Features:\n",
      "        * Smooth and precise movement\n",
      "        * High-quality optical sensor\n",
      "        * Adjustable DPI\n",
      "        * Compatible with Windows and Mac\n",
      "        * Durable\n",
      "\n",
      "len( raw_output ): 867\n",
      "\n",
      "\n",
      "    1. Product Name: Corelogic Smooth Mouse\n",
      "    2. Product Description: The Corelogic Smooth Mouse is a high-quality optical mouse designed to provide a smooth and precise experience for users. It is a versatile mouse that can be used for various applications, including gaming, video editing, and web browsing.\n",
      "    3. Product Category: Optical Mouse\n",
      "    4. Product Features:\n",
      "        * Smooth and precise movement\n",
      "        * High-quality optical sensor\n",
      "        * Adjustable DPI\n",
      "        * Compatible with Windows and Mac\n",
      "        * Durable\n"
     ]
    }
   ],
   "source": [
    "for line in generate_text( tokenizer, base_model, product ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:10:18.112949Z",
     "start_time": "2023-11-17T16:10:11.138166Z"
    }
   },
   "id": "dcaaf8f2e4c38178"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del model\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-16T17:29:37.686678Z"
    }
   },
   "id": "a6e40707f4760dd2"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "{'model.embed_tokens': 0,\n 'model.layers.0': 0,\n 'model.layers.1': 0,\n 'model.layers.2': 0,\n 'model.layers.3': 0,\n 'model.layers.4': 0,\n 'model.layers.5': 0,\n 'model.layers.6': 0,\n 'model.layers.7': 0,\n 'model.layers.8': 0,\n 'model.layers.9': 0,\n 'model.layers.10': 0,\n 'model.layers.11': 0,\n 'model.layers.12': 0,\n 'model.layers.13': 1,\n 'model.layers.14': 1,\n 'model.layers.15': 1,\n 'model.layers.16': 1,\n 'model.layers.17': 1,\n 'model.layers.18': 1,\n 'model.layers.19': 1,\n 'model.layers.20': 1,\n 'model.layers.21': 1,\n 'model.layers.22': 1,\n 'model.layers.23': 1,\n 'model.layers.24': 1,\n 'model.layers.25': 1,\n 'model.layers.26': 1,\n 'model.layers.27': 1,\n 'model.layers.28': 1,\n 'model.layers.29': 1,\n 'model.layers.30': 1,\n 'model.layers.31': 1,\n 'model.norm': 1,\n 'lm_head': 1}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.hf_device_map"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T17:30:05.213834Z",
     "start_time": "2023-11-16T17:30:05.205993Z"
    }
   },
   "id": "fbb05c78a2bde3e4"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# for name, param in base_model.named_parameters():\n",
    "#     print(f\"Parameter {name} is on device {param.device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T20:01:04.820757Z",
     "start_time": "2023-11-15T20:01:04.764619Z"
    }
   },
   "id": "77a87b656bdf19ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up training arguments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "528b5d0d39d9b5df"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_config, PeftModel, PeftConfig, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    # When target_modules was disabled, it was causing detention layers to be assigned to the CPU, throwing this runtime error:\n",
    "    # RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! \n",
    "    # (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n",
    "    target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\" ], \n",
    "    lora_dropout=0.10, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:11:25.809256Z",
     "start_time": "2023-11-17T16:11:25.795325Z"
    }
   },
   "id": "393f4bbf9c6c3ca4"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir=\"./training-results\", # Output directory where the model predictions and checkpoints will be stored\n",
    "    num_train_epochs=3, # Number of training epochs\n",
    "    per_device_train_batch_size=4, # Batch size per GPU for training\n",
    "    gradient_accumulation_steps=2,  # Number of update steps to accumulate the gradients for\n",
    "    gradient_checkpointing=True,# Enable gradient checkpointing\n",
    "    optim=\"paged_adamw_32bit\", # Optimizer to use\n",
    "    #save_steps=save_steps,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    # fp16=True,\n",
    "    bf16=False,\n",
    "    # tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    #max_steps=max_steps,\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    disable_tqdm=True,\n",
    "    report_to=\"wandb\",\n",
    "    seed=42\n",
    ")\n",
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=prompt_instruction_format,\n",
    "    args=trainingArgs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:12:47.103112Z",
     "start_time": "2023-11-17T16:12:46.578329Z"
    }
   },
   "id": "e0ad2d859cefb9c4"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'print_trainable_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mbase_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprint_trainable_parameters\u001B[49m()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1695\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1693\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1694\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1695\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'LlamaForCausalLM' object has no attribute 'print_trainable_parameters'"
     ]
    }
   ],
   "source": [
    "base_model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:12:48.813820Z",
     "start_time": "2023-11-17T16:12:48.718097Z"
    }
   },
   "id": "e5d11100b64024ed"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter model.embed_tokens.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.0.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.1.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.2.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.3.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.4.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.5.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.6.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.7.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.8.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.9.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.10.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.11.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.12.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.13.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.13.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.13.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.14.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.14.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.15.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.15.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.16.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.16.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.17.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.17.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.18.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.18.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.19.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.19.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.20.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.20.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.21.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.22.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.23.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.24.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.25.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.26.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.27.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.28.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.29.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.30.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.31.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.norm.weight is on cuda:1\n",
      "Parameter lm_head.weight is on cuda:1\n"
     ]
    }
   ],
   "source": [
    "for name, param in base_model.named_parameters():\n",
    "    print(f\"Parameter {name} is on {param.device}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:12:50.464711Z",
     "start_time": "2023-11-17T16:12:50.446123Z"
    }
   },
   "id": "1b1f7733e1c1786d"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.0"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/var/model/wandb/run-20231117_161258-1f92denm</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning/runs/1f92denm' target=\"_blank\">lunar-field-9</a></strong> to <a href='https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning/runs/1f92denm' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning/runs/1f92denm</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CodeLlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7425, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.02}\n",
      "{'loss': 0.6891, 'learning_rate': 9.523809523809524e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6339, 'learning_rate': 0.00014285714285714287, 'epoch': 0.06}\n",
      "{'loss': 0.6216, 'learning_rate': 0.00019047619047619048, 'epoch': 0.09}\n",
      "{'loss': 0.5063, 'learning_rate': 0.00019998282416292055, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5331, 'learning_rate': 0.00019991305743680013, 'epoch': 1.01}\n",
      "{'loss': 0.5162, 'learning_rate': 0.00019978966374934254, 'epoch': 1.03}\n",
      "{'loss': 0.5119, 'learning_rate': 0.00019961270933041477, 'epoch': 1.05}\n",
      "{'loss': 0.4702, 'learning_rate': 0.0001993822891578708, 'epoch': 1.07}\n",
      "{'loss': 0.5098, 'learning_rate': 0.00019909852690657359, 'epoch': 1.09}\n",
      "{'loss': 0.4685, 'learning_rate': 0.00019876157488201424, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.485, 'learning_rate': 0.0001983716139385641, 'epoch': 2.01}\n",
      "{'loss': 0.4565, 'learning_rate': 0.00019792885338240374, 'epoch': 2.03}\n",
      "{'loss': 0.4662, 'learning_rate': 0.0001974335308591806, 'epoch': 2.06}\n",
      "{'loss': 0.4473, 'learning_rate': 0.00019688591222645607, 'epoch': 2.08}\n",
      "{'loss': 0.414, 'learning_rate': 0.00019628629141101012, 'epoch': 2.1}\n",
      "{'loss': 0.4145, 'learning_rate': 0.00019563499025107998, 'epoch': 2.12}\n",
      "{'train_runtime': 1330.6939, 'train_samples_per_second': 4.196, 'train_steps_per_second': 0.525, 'train_loss': 0.5255529887535992, 'epoch': 2.12}\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38ce378f44d943fa82e5c893aa535cb7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>ââââââââââââââââââ</td></tr><tr><td>train/global_step</td><td>ââââââââââââââââââ</td></tr><tr><td>train/learning_rate</td><td>âââââââââââââââââ</td></tr><tr><td>train/loss</td><td>âââââââââââââââââ</td></tr><tr><td>train/total_flos</td><td>â</td></tr><tr><td>train/train_loss</td><td>â</td></tr><tr><td>train/train_runtime</td><td>â</td></tr><tr><td>train/train_samples_per_second</td><td>â</td></tr><tr><td>train/train_steps_per_second</td><td>â</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>2.12</td></tr><tr><td>train/global_step</td><td>85</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>0.4145</td></tr><tr><td>train/total_flos</td><td>5.538112860900557e+16</td></tr><tr><td>train/train_loss</td><td>0.52555</td></tr><tr><td>train/train_runtime</td><td>1330.6939</td></tr><tr><td>train/train_samples_per_second</td><td>4.196</td></tr><tr><td>train/train_steps_per_second</td><td>0.525</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">lunar-field-9</strong> at: <a href='https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning/runs/1f92denm' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning/runs/1f92denm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20231117_161258-1f92denm/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "#stop reporting to wandb\n",
    "wandb.finish()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()\n",
    "\n",
    "print(\"Model saved\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:35:15.253197Z",
     "start_time": "2023-11-17T16:12:58.127185Z"
    }
   },
   "id": "597fd4a8fa0f143f"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T17:49:55.354783Z",
     "start_time": "2023-11-16T17:49:55.314638Z"
    }
   },
   "id": "4f1d623ab4bc7b2e"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_output[ 0 ]: tensor([    1,   835,  2799,  4080, 29901,    13,  1678,  4803,   278,  9330,\n",
      "         2400,   322,   278, 10567,  2183,   304,  2436,   278, 13291, 29892,\n",
      "          607,   338,  1824, 29885,  2454, 15278,   393,   508,  4505,   278,\n",
      "         1494,  9330, 29901,    13,    13,  1678,   835,  9330, 29901,    13,\n",
      "         1678,  6204,   263, 13173,  6139,   363,   278,  1494,  3234,    13,\n",
      "           13,  1678,   835, 10567, 29901,    13,  1678,  2994,   295,   468,\n",
      "          293,  4116,  6983, 25992, 29892, 23329,   304,  7663, 29901, 20693,\n",
      "          936, 25992,    13,    13,  1678,   835, 13291, 29901,    13,   268,\n",
      "           13,  1678,   450,  2994,   295,   468,   293,  4116,  6983, 25992,\n",
      "          338,   263,  1880, 29899, 29567, 27070,  9495,  8688,   363, 18378,\n",
      "          322, 10597, 10298, 29889,   739,  5680,   263, 12844,  1416,   322,\n",
      "        11071,  2874, 29892,  3907,   372,  4780,   304,  8677,  2820, 29889,\n",
      "          450,  9495,   756,   263, 29871, 29896, 29906, 29899, 22466,  6355,\n",
      "        18875,   322,   263, 29871, 29896, 29889, 29947, 29899, 22466, 27070,\n",
      "        23530,   393,  8128, 16232,   322, 18378, 10298, 29889,   739,   884,\n",
      "          756,   263, 29871, 29896, 29900, 29899,  3092,  2874,   411,   263,\n",
      "        29871, 29941, 29899,  3092,  6355, 18875, 29892, 14372,   363,   263,\n",
      "         9377,  3464,   310,  2888,  2133,  3987, 29889,   450,  9495,   338,\n",
      "        15878,   411,  3852,   322,  4326, 13598,  6757,   322,   338,  8688,\n",
      "          304,  3867,   263, 10597,   322,   409,   314,  2222,  1404,  7271,\n",
      "        29889,    13,   268,     2], device='cuda:0')\n",
      "\n",
      "generation_output[ 0 ].shape: torch.Size([204])\n",
      "\n",
      "raw_output: <s> ### Instruction:\n",
      "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Create a detailed description for the following product\n",
      "\n",
      "    ### Input:\n",
      "    Corelogic Smooth Mouse, belonging to category: Optical Mouse\n",
      "\n",
      "    ### Response:\n",
      "    \n",
      "    The Corelogic Smooth Mouse is a high-quality optical mouse designed for precise and smooth movement. It features a sleek and compact design, making it easy to carry around. The mouse has a 12-inch scroll wheel and a 1.8-inch optical sensor that provides accurate and precise movement. It also has a 10-button design with a 3-button scroll wheel, allowing for a wide range of customization options. The mouse is compatible with Windows and Mac operating systems and is designed to provide a smooth and seamless user experience.\n",
      "    </s>\n",
      "\n",
      "len( raw_output ): 875\n",
      "\n",
      "\n",
      "    \n",
      "    The Corelogic Smooth Mouse is a high-quality optical mouse designed for precise and smooth movement. It features a sleek and compact design, making it easy to carry around. The mouse has a 12-inch scroll wheel and a 1.8-inch optical sensor that provides accurate and precise movement. It also has a 10-button design with a 3-button scroll wheel, allowing for a wide range of customization options. The mouse is compatible with Windows and Mac operating systems and is designed to provide a smooth and seamless user experience.\n",
      "    </s>\n"
     ]
    }
   ],
   "source": [
    "for line in generate_text( tokenizer, base_model, product ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:31:32.284619Z",
     "start_time": "2023-11-17T17:31:21.199315Z"
    }
   },
   "id": "8b7bf973e2dcedd8"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "1020"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drops 16.4/19.0 GB per GPU down to 3.25 GB per GPU!\n",
    "import gc\n",
    "del base_model\n",
    "torch.cuda.empty_cache() \n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:48:43.325338Z",
     "start_time": "2023-11-17T17:48:43.324957Z"
    }
   },
   "id": "6db763ceb8f0bce3"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ec10342464b4fc281960d63d8642a02"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "print( bnb_config )\n",
    "tokenizer              = AutoTokenizer.from_pretrained( \".\" )\n",
    "tokenizer.pad_token    = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Â¡OJO! Why were we turning off the cash here? \n",
    "# We're not! It makes a huge performance difference: 21 vs 14 tokens per second!\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \".\", quantization_config=bnb_config, device_map=\"auto\", low_cpu_mem_usage=True, use_cache=True, use_flash_attention_2=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:49:20.704159Z",
     "start_time": "2023-11-17T17:49:16.934739Z"
    }
   },
   "id": "55afe25a14757322"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_output[ 0 ]: tensor([    1,   835,  2799,  4080, 29901,    13,  1678,  4803,   278,  9330,\n",
      "         2400,   322,   278, 10567,  2183,   304,  2436,   278, 13291, 29892,\n",
      "          607,   338,  1824, 29885,  2454, 15278,   393,   508,  4505,   278,\n",
      "         1494,  9330, 29901,    13,    13,  1678,   835,  9330, 29901,    13,\n",
      "         1678,  6204,   263, 13173,  6139,   363,   278,  1494,  3234,    13,\n",
      "           13,  1678,   835, 10567, 29901,    13,  1678,  2994,   295,   468,\n",
      "          293,  4116,  6983, 25992, 29892, 23329,   304,  7663, 29901, 20693,\n",
      "          936, 25992,    13,    13,  1678,   835, 13291, 29901,    13,   268,\n",
      "        29896, 29889, 10969,  4408, 29901,  2994,   295,   468,   293,  4116,\n",
      "         6983, 25992,    13,   268, 29906, 29889, 10969, 12953, 29901,   450,\n",
      "         2994,   295,   468,   293,  4116,  6983, 25992,   338,   263,  1880,\n",
      "        29899, 29567, 27070,  9495,  8688,   304,  3867,   263, 10597,   322,\n",
      "        18378,  7271,   363,  4160, 29889,   739,   338,   263,  1224, 24285,\n",
      "         9495,   393,   508,   367,  1304,   363,  5164,  8324, 29892,  3704,\n",
      "          330, 11500, 29892,  4863, 16278, 29892,   322,  1856,  3347,  2976,\n",
      "        29889,    13,   268, 29941, 29889, 10969, 17943, 29901, 20693,   936,\n",
      "        25992,    13,   268, 29946, 29889, 10969,  5169,  3698, 29901,    13,\n",
      "         4706,   334,  4116,  6983,   322, 18378, 10298,    13,  4706,   334,\n",
      "         5057, 29899, 29567, 27070, 23530,    13,  4706,   334,  2087,  5143,\n",
      "          519,   360,  2227,    13,  4706,   334,  3831,   271,  1821,   411,\n",
      "         3852,   322,  4326,    13,  4706,   334,  7073,   519],\n",
      "       device='cuda:0')\n",
      "\n",
      "generation_output[ 0 ].shape: torch.Size([208])\n",
      "\n",
      "raw_output: <s> ### Instruction:\n",
      "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Create a detailed description for the following product\n",
      "\n",
      "    ### Input:\n",
      "    Corelogic Smooth Mouse, belonging to category: Optical Mouse\n",
      "\n",
      "    ### Response:\n",
      "    1. Product Name: Corelogic Smooth Mouse\n",
      "    2. Product Description: The Corelogic Smooth Mouse is a high-quality optical mouse designed to provide a smooth and precise experience for users. It is a versatile mouse that can be used for various applications, including gaming, video editing, and web browsing.\n",
      "    3. Product Category: Optical Mouse\n",
      "    4. Product Features:\n",
      "        * Smooth and precise movement\n",
      "        * High-quality optical sensor\n",
      "        * Adjustable DPI\n",
      "        * Compatible with Windows and Mac\n",
      "        * Durable\n",
      "\n",
      "len( raw_output ): 867\n",
      "\n",
      "\n",
      "    1. Product Name: Corelogic Smooth Mouse\n",
      "    2. Product Description: The Corelogic Smooth Mouse is a high-quality optical mouse designed to provide a smooth and precise experience for users. It is a versatile mouse that can be used for various applications, including gaming, video editing, and web browsing.\n",
      "    3. Product Category: Optical Mouse\n",
      "    4. Product Features:\n",
      "        * Smooth and precise movement\n",
      "        * High-quality optical sensor\n",
      "        * Adjustable DPI\n",
      "        * Compatible with Windows and Mac\n",
      "        * Durable\n"
     ]
    }
   ],
   "source": [
    "for line in generate_text( tokenizer, base_model, product ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:49:29.101186Z",
     "start_time": "2023-11-17T17:49:23.142465Z"
    }
   },
   "id": "2bbddb04a5a998aa"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "48878"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drops 16.4/19.0 GB per GPU down to 3.25 GB per GPU!\n",
    "import gc\n",
    "del adapter_plus_model\n",
    "torch.cuda.empty_cache() \n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:48:59.572552Z",
     "start_time": "2023-11-17T17:48:59.562021Z"
    }
   },
   "id": "648a08233ae9bd2e"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "adapter_plus_model = PeftModel.from_pretrained( base_model, \"training-results/checkpoint-85\", use_flash_attention_2=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:49:42.509922Z",
     "start_time": "2023-11-17T17:49:41.967645Z"
    }
   },
   "id": "34796358a34b99c6"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_output[ 0 ]: tensor([    1,   835,  2799,  4080, 29901,    13,  1678,  4803,   278,  9330,\n",
      "         2400,   322,   278, 10567,  2183,   304,  2436,   278, 13291, 29892,\n",
      "          607,   338,  1824, 29885,  2454, 15278,   393,   508,  4505,   278,\n",
      "         1494,  9330, 29901,    13,    13,  1678,   835,  9330, 29901,    13,\n",
      "         1678,  6204,   263, 13173,  6139,   363,   278,  1494,  3234,    13,\n",
      "           13,  1678,   835, 10567, 29901,    13,  1678,  2994,   295,   468,\n",
      "          293,  4116,  6983, 25992, 29892, 23329,   304,  7663, 29901, 20693,\n",
      "          936, 25992,    13,    13,  1678,   835, 13291, 29901,    13,   268,\n",
      "           13,  1678,   450,  2994,   295,   468,   293,  4116,  6983, 25992,\n",
      "          338,   263,  1880, 29899, 29567, 27070,  9495,  8688,   363, 18378,\n",
      "          322, 10597, 10298, 29889,   739,  5680,   263, 12844,  1416,   322,\n",
      "        11071,  2874, 29892,  3907,   372,  4780,   304,  8677,  2820, 29889,\n",
      "          450,  9495,   756,   263, 29871, 29896, 29906, 29899, 22466,  6355,\n",
      "        18875,   322,   263, 29871, 29896, 29889, 29947, 29899, 22466, 27070,\n",
      "        23530,   393,  8128, 16232,   322, 18378, 10298, 29889,   739,   884,\n",
      "          756,   263, 29871, 29896, 29900, 29899,  3092,  2874,   411,   263,\n",
      "        29871, 29941, 29899,  3092,  6355, 18875, 29892, 14372,   363,   263,\n",
      "         9377,  3464,   310,  2888,  2133,  3987, 29889,   450,  9495,   338,\n",
      "        15878,   411,  3852,   322,  4326, 13598,  6757,   322,   338,  8688,\n",
      "          304,  3867,   263, 10597,   322,   409,   314,  2222,  1404,  7271,\n",
      "        29889,    13,   268,     2], device='cuda:0')\n",
      "\n",
      "generation_output[ 0 ].shape: torch.Size([204])\n",
      "\n",
      "raw_output: <s> ### Instruction:\n",
      "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Create a detailed description for the following product\n",
      "\n",
      "    ### Input:\n",
      "    Corelogic Smooth Mouse, belonging to category: Optical Mouse\n",
      "\n",
      "    ### Response:\n",
      "    \n",
      "    The Corelogic Smooth Mouse is a high-quality optical mouse designed for precise and smooth movement. It features a sleek and compact design, making it easy to carry around. The mouse has a 12-inch scroll wheel and a 1.8-inch optical sensor that provides accurate and precise movement. It also has a 10-button design with a 3-button scroll wheel, allowing for a wide range of customization options. The mouse is compatible with Windows and Mac operating systems and is designed to provide a smooth and seamless user experience.\n",
      "    </s>\n",
      "\n",
      "len( raw_output ): 875\n",
      "\n",
      "\n",
      "    \n",
      "    The Corelogic Smooth Mouse is a high-quality optical mouse designed for precise and smooth movement. It features a sleek and compact design, making it easy to carry around. The mouse has a 12-inch scroll wheel and a 1.8-inch optical sensor that provides accurate and precise movement. It also has a 10-button design with a 3-button scroll wheel, allowing for a wide range of customization options. The mouse is compatible with Windows and Mac operating systems and is designed to provide a smooth and seamless user experience.\n",
      "    </s>\n"
     ]
    }
   ],
   "source": [
    "for line in generate_text( tokenizer, adapter_plus_model, product ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:49:57.395913Z",
     "start_time": "2023-11-17T17:49:48.579002Z"
    }
   },
   "id": "d84a5998d65cb738"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dcb9efffadd2d939"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
