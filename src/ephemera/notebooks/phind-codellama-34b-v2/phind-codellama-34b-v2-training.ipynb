{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab791885-0ebf-4c82-aa0f-574973c5f9e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:01:11.373353Z",
     "start_time": "2023-11-14T16:01:11.358292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'/'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb00e63-c0c8-497c-ba11-33a7325907da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:01:11.481197Z",
     "start_time": "2023-11-14T16:01:11.370316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 63G\r\n",
      "drwxrwxr-x 5 1001 1001 4.0K Nov 13 19:09 .\r\n",
      "drwxr-xr-x 1 root root 4.0K Nov 14 15:49 ..\r\n",
      "-rw-r--r-- 1 1001 1001 6.1K Nov 13 19:09 .DS_Store\r\n",
      "-rw-r--r-- 1 1001 1001 4.0K Nov 13 19:09 ._.DS_Store\r\n",
      "drwxr-xr-x 2 root root 4.0K Nov 11 02:21 .ipynb_checkpoints\r\n",
      "-rw-rw-r-- 1 1001 1001 3.7K Nov  6 17:14 README.md\r\n",
      "drwxr-xr-x 2 root root 4.0K Nov 11 02:56 checkpoints\r\n",
      "-rw-rw-r-- 1 1001 1001  638 Nov  6 17:14 config.json\r\n",
      "-rw-rw-r-- 1 1001 1001  116 Nov  6 17:14 generation_config.json\r\n",
      "-rw-r--r-- 1 1001 1001  984 Nov  7 18:12 model.bak\r\n",
      "-rw-r--r-- 1 1001 1001  984 Nov  6 21:02 model.yaml\r\n",
      "drwxr-xr-x 3 root root 4.0K Nov 11 03:06 output\r\n",
      "-rw-rw-r-- 1 1001 1001 9.2G Nov  6 17:28 pytorch_model-00001-of-00007.bin\r\n",
      "-rw-rw-r-- 1 1001 1001 9.1G Nov  6 17:28 pytorch_model-00002-of-00007.bin\r\n",
      "-rw-rw-r-- 1 1001 1001 9.1G Nov  6 17:28 pytorch_model-00003-of-00007.bin\r\n",
      "-rw-rw-r-- 1 1001 1001 9.1G Nov  6 17:28 pytorch_model-00004-of-00007.bin\r\n",
      "-rw-rw-r-- 1 1001 1001 9.1G Nov  6 17:28 pytorch_model-00005-of-00007.bin\r\n",
      "-rw-rw-r-- 1 1001 1001 9.1G Nov  6 17:28 pytorch_model-00006-of-00007.bin\r\n",
      "-rw-rw-r-- 1 1001 1001 8.6G Nov  6 17:27 pytorch_model-00007-of-00007.bin\r\n",
      "-rw-rw-r-- 1 1001 1001  35K Nov  6 17:14 pytorch_model.bin.index.json\r\n",
      "-rw-rw-r-- 1 1001 1001  434 Nov  6 17:14 special_tokens_map.json\r\n",
      "-rw-rw-r-- 1 1001 1001 489K Nov  6 17:14 tokenizer.model\r\n",
      "-rw-rw-r-- 1 1001 1001  824 Nov  6 17:14 tokenizer_config.json\r\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /var/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "os.chdir( \"/var/model\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T16:01:12.626005Z",
     "start_time": "2023-11-14T16:01:12.623098Z"
    }
   },
   "id": "549f5690f48e9525"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e0c6bca-7964-4373-9801-bb9ac9f95138",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:02:03.183875Z",
     "start_time": "2023-11-14T16:01:28.552605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ad31ac39b8640bba5d360a4ed673e24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained( \".\" )\n",
    "model     = AutoModelForCausalLM.from_pretrained( \".\", quantization_config=bnb_config, low_cpu_mem_usage=True, device_map=\"auto\", low_cpu_mem_usage=Tru ) # , local_files_only=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32dc9de4-f041-4937-93f3-963dbea1f01e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:02:29.606185Z",
     "start_time": "2023-11-14T16:02:29.539820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "transformers.models.llama.modeling_llama.LlamaForCausalLM"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training( model )\n",
    "type( model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c74ffb2-5cc8-49ea-b3d4-62398d3402e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:02:35.710183Z",
     "start_time": "2023-11-14T16:02:35.701730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 8192)\n    (layers): ModuleList(\n      (0-47): 48 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n          (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n          (up_proj): Linear4bit(in_features=8192, out_features=22016, bias=False)\n          (down_proj): Linear4bit(in_features=22016, out_features=8192, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34828a67-6d2b-4211-b627-68b16547923d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T20:46:01.186001Z",
     "start_time": "2023-12-19T20:46:01.182134Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters( model ):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a94744d5-fb8d-41a4-a4f3-520af939eceb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:04:39.606760Z",
     "start_time": "2023-11-14T16:04:38.321415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 108920832 || all params: 17243447296 || trainable%: 0.6316650616913858\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\" ], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d9d981e-fc94-4823-822a-cac731cc4507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:04:34.499670Z",
     "start_time": "2023-11-14T16:04:34.468981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "transformers.models.llama.modeling_llama.LlamaForCausalLM"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "# \n",
    "# data = load_dataset(\"Abirate/english_quotes\")\n",
    "# data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# \n",
    "# yelp_dataset = load_dataset( \"yelp_review_full\" )\n",
    "# \n",
    "# yelp_dataset[ \"train\" ][ 100 ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T20:44:08.170897Z",
     "start_time": "2023-11-13T20:44:08.137878Z"
    }
   },
   "id": "ac29c562c48113d8"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# def tokenize_function( examples ):\n",
    "#     return tokenizer( examples[ \"text\" ], padding=\"max_length\", truncation=True )\n",
    "# \n",
    "# \n",
    "# tokenized_yelp_datasets = yelp_dataset.map( tokenize_function, batched=True )\n",
    "# \n",
    "# small_yelp_train_dataset = tokenized_yelp_datasets[ \"train\" ].shuffle( seed=42 ).select( range( 5000 ) )\n",
    "# small_yelp_test_dataset  = tokenized_yelp_datasets[ \"test\" ].shuffle( seed=42 ).select( range( 5000 ) )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T20:44:20.364647Z",
     "start_time": "2023-11-13T20:44:20.333313Z"
    }
   },
   "id": "b5d82f609e5d1cfb"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/5.55k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "406989cba4bb4471b1b2946b0b25f6e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05512272ab18443782493ee8b5a0b527"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/647k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7242b154eb14fcfad974c7053d79606"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f78108fddc44b80b3b7ce3e97e617e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee31808d16c84c8db88d69a9f6b1b061"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2508 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e02f528333b649ccb2fd68a5beee1100"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T16:05:51.512557Z",
     "start_time": "2023-11-14T16:05:49.294518Z"
    }
   },
   "id": "943ed857d51b90fe"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1553: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four score and seven years ago our fathers brought forth on this continent a new nation, conceived in liberty, and dedicated to the proposition that all men are created equal.\n",
      "\n",
      "Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battlefield of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this.\n",
      "\n",
      "But, in a larger sense, we can not dedicate, we can not consecrate, we can not hallow this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It\n",
      "CPU times: user 1min 12s, sys: 40.9 s, total: 1min 52s\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "#Pass in a prompt and infer with the model\n",
    "prompt = 'Q: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\\nA:'\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T16:26:19.073629Z",
     "start_time": "2023-11-14T16:24:26.191131Z"
    }
   },
   "id": "83b6d5ec95895940"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained( \".\", use_fast=True )\n",
    "text_generator_4bit = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T20:52:42.699117Z",
     "start_time": "2023-11-13T20:52:42.425877Z"
    }
   },
   "id": "8573174775bf6914"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e5d270f-1c05-4164-acb0-43367f6ceb59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:13:52.341956Z",
     "start_time": "2023-11-14T16:13:51.951635Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "num_training_steps = 20\n",
    "# progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# this is a fairly common redefinition of the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=num_training_steps,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"checkpoints\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## using trl SFTTrainer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0a4e6030ace0607"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cbd63c2ef2f49c7e"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db38bb0f-b56a-4bf3-83bf-1f61178ceed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained( \"output\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "584b1cfb-92fd-4963-8d0b-597983096976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1553: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four score and seven years ago our fathers brought forth on this continent a new nation, conceived in liberty, and dedicated to the proposition that all men are created equal.\n",
      "\n",
      "Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battlefield of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this.\n",
      "\n",
      "But, in a larger sense, we can not dedicate, we can not consecrate, we can not hallow this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It\n",
      "CPU times: user 1min 11s, sys: 40.1 s, total: 1min 51s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"four score and seven years ago\"\n",
    "device = \"cuda:1\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f0cbe-2f66-441a-a702-aa394ead9ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
