{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/site-packages (0.16.0)\r\n",
      "Collecting wandb\r\n",
      "  Downloading wandb-0.16.1-py3-none-any.whl.metadata (9.8 kB)\r\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from wandb) (3.1.40)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/site-packages (from wandb) (2.31.0)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/site-packages (from wandb) (5.9.6)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.11/site-packages (from wandb) (1.37.1)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/site-packages (from wandb) (6.0.1)\r\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from wandb) (65.5.0)\r\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.11/site-packages (from wandb) (1.4.4)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.11/site-packages (from wandb) (4.25.1)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n",
      "Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m27.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: wandb\r\n",
      "  Attempting uninstall: wandb\r\n",
      "    Found existing installation: wandb 0.16.0\r\n",
      "    Uninstalling wandb-0.16.0:\r\n",
      "      Successfully uninstalled wandb-0.16.0\r\n",
      "Successfully installed wandb-0.16.1\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U wandb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:31:36.899224Z",
     "start_time": "2023-12-18T21:31:32.628271Z"
    }
   },
   "id": "bc83d401587bae4c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb.sdk' has no attribute 'lib'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwandb\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/wandb/__init__.py:31\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwandb\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sdk \u001B[38;5;28;01mas\u001B[39;00m wandb_sdk\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwandb\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m wandb\u001B[38;5;241m.\u001B[39mwandb_lib \u001B[38;5;241m=\u001B[39m \u001B[43mwandb_sdk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlib\u001B[49m\n\u001B[1;32m     33\u001B[0m init \u001B[38;5;241m=\u001B[39m wandb_sdk\u001B[38;5;241m.\u001B[39minit\n\u001B[1;32m     34\u001B[0m setup \u001B[38;5;241m=\u001B[39m wandb_sdk\u001B[38;5;241m.\u001B[39msetup\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'wandb.sdk' has no attribute 'lib'"
     ]
    }
   ],
   "source": [
    "import wandb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:31:39.937637Z",
     "start_time": "2023-12-18T21:31:39.898926Z"
    }
   },
   "id": "aefd40292f671cab"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mwandb\u001B[49m\u001B[38;5;241m.\u001B[39mlogin()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T21:31:21.394166Z",
     "start_time": "2023-12-18T21:31:21.390561Z"
    }
   },
   "id": "d49e208217c2ea36"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=codellama-7b-instruct-hf-peft-fine-tuning\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=codellama-7b-instruct-hf-peft-fine-tuning"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T21:43:38.986676Z",
     "start_time": "2023-11-14T21:43:38.977639Z"
    }
   },
   "id": "d03f99f12eb04c9e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dataset_name = \"iamtarun/python_code_instructions_18k_alpaca\"\n",
    "split = \"train[:10%]\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T21:43:41.728384Z",
     "start_time": "2023-11-14T21:43:41.716778Z"
    }
   },
   "id": "90fba6c4ff2875f3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset( dataset_name, split=split )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T21:43:44.818814Z",
     "start_time": "2023-11-14T21:43:43.355823Z"
    }
   },
   "id": "ccedc3adeb45dcb8"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def prompt_instruction_format( sample ):\n",
    "    \n",
    "  return f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
    "\n",
    "    ### Task:\n",
    "    {sample['instruction']}\n",
    "\n",
    "    ### Input:\n",
    "    {sample['input']}\n",
    "\n",
    "    ### Response:\n",
    "    {sample['output']}\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T21:43:45.660130Z",
     "start_time": "2023-11-14T21:43:45.658276Z"
    }
   },
   "id": "58e3fd8b1b81ce1d"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from datasets import load_dataset, Dataset\n",
    "# \n",
    "# #Load the dataset from the HuggingFace Hub\n",
    "# rd_ds = load_dataset( \"xiyuez/red-dot-design-award-product-description\" )\n",
    "# \n",
    "# #Convert to pandas dataframe for convenient processing\n",
    "# rd_df = pd.DataFrame( rd_ds[ 'train' ] )\n",
    "# \n",
    "# #Combine the two attributes into an instruction string\n",
    "# rd_df[ 'instruction' ] = 'Create a detailed description for the following product: ' + rd_df[ 'product' ] + ', belonging to category: ' + rd_df[ 'category' ]\n",
    "# \n",
    "# rd_df = rd_df[ [ 'instruction', 'description' ] ]\n",
    "# \n",
    "# #Get a 5000 sample subset for fine-tuning purposes\n",
    "# rd_df_sample = rd_df.sample( n=5000, random_state=42 )\n",
    "# \n",
    "# #Define template and format data into the template for supervised fine-tuning\n",
    "# template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "# \n",
    "# ### Instruction:\n",
    "# \n",
    "# {}\n",
    "# \n",
    "# ### Response:\\n\"\"\"\n",
    "# \n",
    "# rd_df_sample[ 'prompt' ] = rd_df_sample[ \"instruction\" ].apply( lambda x: template.format( x ) )\n",
    "# rd_df_sample.rename( columns={ 'description': 'response' }, inplace=True )\n",
    "# rd_df_sample[ 'response' ] = rd_df_sample[ 'response' ] + \"\\n### End\"\n",
    "# rd_df_sample = rd_df_sample[ [ 'prompt', 'response' ] ]\n",
    "# \n",
    "# rd_df_sample[ 'text' ] = rd_df_sample[ \"prompt\" ] + rd_df_sample[ \"response\" ]\n",
    "# rd_df_sample.drop( columns=[ 'prompt', 'response' ], inplace=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T21:38:48.361173Z",
     "start_time": "2023-11-14T21:38:48.307196Z"
    }
   },
   "id": "e5696501729dea07"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# \n",
    "# dataset = Dataset.from_pandas( rd_df_sample ).train_test_split( test_size=0.05, seed=42 )\n",
    "# dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T21:39:00.608459Z",
     "start_time": "2023-11-14T21:39:00.572862Z"
    }
   },
   "id": "3f8e7bdb59156209"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir( \"/var/model\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T21:42:35.368451Z",
     "start_time": "2023-11-14T21:42:35.358040Z"
    }
   },
   "id": "24974f39bed76473"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 31G\r\n",
      "drwxrwxr-x 5 1001 1001 4.0K Nov 11 03:08 .\r\n",
      "drwxr-xr-x 1 root root 4.0K Nov 14 18:07 ..\r\n",
      "drwxr-xr-x 2 root root 4.0K Nov 11 03:01 .ipynb_checkpoints\r\n",
      "-rw-rw-r-- 1 1001 1001 6.9K Nov  5 17:36 LICENSE\r\n",
      "-rw-rw-r-- 1 1001 1001 6.1K Nov  5 17:36 README.md\r\n",
      "-rw-rw-r-- 1 1001 1001 4.7K Nov  5 17:36 USE_POLICY.md\r\n",
      "drwxr-xr-x 2 root root 4.0K Nov 10 21:49 checkpoints\r\n",
      "-rw-r--r-- 1 root root 9.3K Nov 11 02:12 code-llama-instruct-7b-peft.ipynb\r\n",
      "-rw-r--r-- 1 1001 1001  97K Nov 11 03:08 code-llama-instruct-7b.ipynb\r\n",
      "-rw-rw-r-- 1 1001 1001  646 Nov  5 17:36 config.json\r\n",
      "-rw-r--r-- 1 root root 4.1G Jun 26 07:24 cuda_12.2.0_535.54.03_linux.run\r\n",
      "-rw-rw-r-- 1 1001 1001  116 Nov  5 17:36 generation_config.json\r\n",
      "-rw-rw-r-- 1 1001 1001 9.3G Nov  5 17:42 model-00001-of-00002.safetensors\r\n",
      "-rw-rw-r-- 1 1001 1001 3.3G Nov  5 17:40 model-00002-of-00002.safetensors\r\n",
      "-rw-rw-r-- 1 1001 1001  25K Nov  5 17:36 model.safetensors.index.json\r\n",
      "drwxr-xr-x 5 root root 4.0K Nov 14 18:34 output\r\n",
      "-rw-rw-r-- 1 1001 1001 4.6G Nov  5 17:41 pytorch_model-00001-of-00003.bin\r\n",
      "-rw-rw-r-- 1 1001 1001 4.7G Nov  5 17:41 pytorch_model-00002-of-00003.bin\r\n",
      "-rw-rw-r-- 1 1001 1001 3.4G Nov  5 17:40 pytorch_model-00003-of-00003.bin\r\n",
      "-rw-rw-r-- 1 1001 1001  24K Nov  5 17:36 pytorch_model.bin.index.json\r\n",
      "-rw-rw-r-- 1 1001 1001  411 Nov  5 17:36 special_tokens_map.json\r\n",
      "-rw-r--r-- 1 1001 1001  27K Nov 10 21:15 text-generation.ipynb\r\n",
      "-rw------- 1 1001 1001 1.1G Nov 10 18:44 tmpdknh986h\r\n",
      "-rw-rw-r-- 1 1001 1001 1.8M Nov  5 17:36 tokenizer.json\r\n",
      "-rw-rw-r-- 1 1001 1001 489K Nov  5 17:36 tokenizer.model\r\n",
      "-rw-rw-r-- 1 1001 1001  749 Nov  5 17:36 tokenizer_config.json\r\n",
      "-rw-r--r-- 1 1001 1001    1 Nov 10 18:43 version.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alh"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T21:42:36.415402Z",
     "start_time": "2023-11-14T21:42:36.293770Z"
    }
   },
   "id": "630b82887ec1f8c7"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf79f1fca88946cb8c11c0a0c984361f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model_path = 'codellama/CodeLlama-7b-Instruct-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained( \".\" )\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# *DOES NOT* force the model to be loaded entirely onto the second GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \".\", quantization_config=bnb_config, device_map=\"auto\", low_cpu_mem_usage=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T21:42:44.739024Z",
     "start_time": "2023-11-14T21:42:39.491896Z"
    }
   },
   "id": "b6e2cfb0fbdd120"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Q: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\n",
      "A: Corelogic Smooth Mouse is a high-quality optical mouse that is designed to provide a smooth and precise experience for users. It is a wireless mouse that is designed to be used with a computer or other device. The mouse has a sleek and modern design that is easy to use and looks great on any desk. It has a 12,000-dot optical sensor that allows for precise movement and a 12-inch optical sensor that provides a smooth and precise experience. The mouse also has a 12-inch optical sensor that provides a smooth and precise experience. It has a 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Pass in a prompt and infer with the model\n",
    "prompt = 'Q: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\\nA:'\n",
    "\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer( prompt, return_tensors=\"pt\" ).to( device )\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=inputs[ \"input_ids\" ],\n",
    "    attention_mask=inputs[ \"attention_mask\" ],\n",
    "    max_new_tokens=128\n",
    ")\n",
    "\n",
    "print( tokenizer.decode( generation_output[ 0 ] ) )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:09:26.034078Z",
     "start_time": "2023-11-14T18:09:20.172224Z"
    }
   },
   "id": "6731c2c2aa9676b6"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_config, PeftModel, PeftConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\" ], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:09:39.089645Z",
     "start_time": "2023-11-14T18:09:39.061026Z"
    }
   },
   "id": "393f4bbf9c6c3ca4"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,976,960 || all params: 6,778,523,648 || trainable%: 0.5897590991188056\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model( model, lora_config )\n",
    "model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:09:41.007638Z",
     "start_time": "2023-11-14T18:09:40.522474Z"
    }
   },
   "id": "9f5efb1e4de343e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up training arguments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "528b5d0d39d9b5df"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = 'adamw_hf'\n",
    "learning_rate = 1e-5\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"linear\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:09:47.074721Z",
     "start_time": "2023-11-14T18:09:47.074022Z"
    }
   },
   "id": "a6da24f319b59523"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs = 3.0,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:09:49.562116Z",
     "start_time": "2023-11-14T18:09:49.535423Z"
    }
   },
   "id": "e0ad2d859cefb9c4"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:09:51.145132Z",
     "start_time": "2023-11-14T18:09:51.068380Z"
    }
   },
   "id": "adbf34af6b284034"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/4750 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "803095f787f742e790dd29ade6c4f343"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/250 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ce978858e464d4a98fccc851b7277b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up casting to float32: base_model.model.model.layers.0.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.0.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.1.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.1.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.2.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.2.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.3.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.3.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.4.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.4.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.5.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.5.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.6.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.6.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.7.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.7.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.8.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.8.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.9.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.9.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.10.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.10.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.11.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.11.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.12.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.12.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.13.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.13.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.14.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.14.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.15.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.15.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.16.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.16.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.17.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.17.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.18.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.18.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.19.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.19.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.20.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.20.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.21.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.21.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.22.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.22.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.23.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.23.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.24.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.24.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.25.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.25.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.26.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.26.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.27.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.27.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.28.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.28.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.29.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.29.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.30.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.30.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.31.input_layernorm\n",
      "Up casting to float32: base_model.model.model.layers.31.post_attention_layernorm\n",
      "Up casting to float32: base_model.model.model.norm\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=256,\n",
    "    args=training_args,\n",
    ")\n",
    "#Upcast layer norms to torch.bfloat16 for stability\n",
    "for name, module in trainer.model.named_modules():\n",
    "  if \"norm\" in name:\n",
    "    module = module.to(torch.bfloat16)\n",
    "    print( \"Up casting to float32:\", name )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:11:12.009055Z",
     "start_time": "2023-11-14T18:11:11.508663Z"
    }
   },
   "id": "2a6f95af2267818f"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:423: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a CodeLlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='891' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/891 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=891, training_loss=1.8714779769263818, metrics={'train_runtime': 1413.0161, 'train_samples_per_second': 10.085, 'train_steps_per_second': 0.631, 'total_flos': 1.0319356998043238e+17, 'train_loss': 1.8714779769263818, 'epoch': 3.0})"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:34:51.854944Z",
     "start_time": "2023-11-14T18:11:18.807661Z"
    }
   },
   "id": "597fd4a8fa0f143f"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'save_adapter'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:508\u001B[0m, in \u001B[0;36mPeftModel.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    507\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 508\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattr__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# defer to nn.Module's logic\u001B[39;00m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1695\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1694\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1695\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'PeftModelForCausalLM' object has no attribute 'save_adapter'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/model.py:328\u001B[0m, in \u001B[0;36mLoraModel.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattr__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# defer to nn.Module's logic\u001B[39;00m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1695\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1694\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1695\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'LoraModel' object has no attribute 'save_adapter'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_adapter\u001B[49m( \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfoo_adapter\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:510\u001B[0m, in \u001B[0;36mPeftModel.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    508\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattr__\u001B[39m(name)  \u001B[38;5;66;03m# defer to nn.Module's logic\u001B[39;00m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m:\n\u001B[0;32m--> 510\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/model.py:330\u001B[0m, in \u001B[0;36mLoraModel.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattr__\u001B[39m(name)  \u001B[38;5;66;03m# defer to nn.Module's logic\u001B[39;00m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m:\n\u001B[0;32m--> 330\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1695\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1693\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1694\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1695\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'LlamaForCausalLM' object has no attribute 'save_adapter'"
     ]
    }
   ],
   "source": [
    "model.save_adapter( \"output\", \"foo_adapter\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T19:39:08.243176Z",
     "start_time": "2023-11-14T19:39:07.994565Z"
    }
   },
   "id": "ba3dafc8154b872a"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m inputs \u001B[38;5;241m=\u001B[39m tokenizer( prompt, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m )\u001B[38;5;241m.\u001B[39mto( device )\n\u001B[0;32m----> 7\u001B[0m generation_output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mattention_mask\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\n\u001B[1;32m     11\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m( tokenizer\u001B[38;5;241m.\u001B[39mdecode( generation_output[ \u001B[38;5;241m0\u001B[39m ] ) )\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:1060\u001B[0m, in \u001B[0;36mPeftModelForCausalLM.generate\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m   1058\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model\u001B[38;5;241m.\u001B[39mgeneration_config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration_config\n\u001B[1;32m   1059\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1060\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1061\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m   1062\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model\u001B[38;5;241m.\u001B[39mprepare_inputs_for_generation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1753\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massisted_decoding(\n\u001B[1;32m   1737\u001B[0m         input_ids,\n\u001B[1;32m   1738\u001B[0m         assistant_model\u001B[38;5;241m=\u001B[39massistant_model,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   1750\u001B[0m     )\n\u001B[1;32m   1751\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mGREEDY_SEARCH:\n\u001B[1;32m   1752\u001B[0m     \u001B[38;5;66;03m# 11. run greedy search\u001B[39;00m\n\u001B[0;32m-> 1753\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgreedy_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1754\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1755\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1756\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1757\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1758\u001B[0m \u001B[43m        \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1759\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_scores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1760\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1761\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1762\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1763\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1764\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1766\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mCONTRASTIVE_SEARCH:\n\u001B[1;32m   1767\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2614\u001B[0m, in \u001B[0;36mGenerationMixin.greedy_search\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[1;32m   2611\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_inputs_for_generation(input_ids, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n\u001B[1;32m   2613\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[0;32m-> 2614\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2615\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2616\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   2617\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2618\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2619\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2621\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synced_gpus \u001B[38;5;129;01mand\u001B[39;00m this_peer_finished:\n\u001B[1;32m   2622\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# don't waste resources running the code we don't need\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:164\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:1052\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1050\u001B[0m     logits \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(logits, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m   1051\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1052\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlm_head\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1053\u001B[0m logits \u001B[38;5;241m=\u001B[39m logits\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m   1055\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:164\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: expected mat1 and mat2 to have the same dtype, but got: float != c10::Half"
     ]
    }
   ],
   "source": [
    "\n",
    "#Pass in a prompt and infer with the model\n",
    "prompt = 'Q: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\\nA:'\n",
    "\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer( prompt, return_tensors=\"pt\" ).to( device )\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=inputs[ \"input_ids\" ],\n",
    "    attention_mask=inputs[ \"attention_mask\" ],\n",
    "    max_new_tokens=128\n",
    ")\n",
    "\n",
    "print( tokenizer.decode( generation_output[ 0 ] ) )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T19:50:29.012245Z",
     "start_time": "2023-11-14T19:50:28.440732Z"
    }
   },
   "id": "3556dfe90765c54b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "68f6e0ece848fd7d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
