{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\r\n",
      "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.10.3)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\r\n",
      "Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m26.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: tiktoken\r\n",
      "Successfully installed tiktoken-0.5.2\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# ! pip install xmlschema\n",
    "# ! pip install scikit-learn\n",
    "# ! pip install openai\n",
    "! pip install tiktoken\n",
    "# ! pip install trl\n",
    "# ! pip install lzma"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T16:40:22.156430Z",
     "start_time": "2024-01-02T16:40:19.940662Z"
    }
   },
   "id": "913059d75bfb9b60"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\r\n"
     ]
    }
   ],
   "source": [
    "! python3 --version"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T17:50:31.610125Z",
     "start_time": "2024-01-02T17:50:31.496744Z"
    }
   },
   "id": "40d7f5a05bdbaeef"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:35:33.583691Z",
     "start_time": "2024-01-02T18:35:32.450024Z"
    }
   },
   "id": "c213bdd2418b70f4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 31M\r\n",
      "drwxr--r-- 4 1001 1001 4.0K Dec 20 03:40 .\r\n",
      "drwxr--r-- 4 1001 1001 4.0K Jan  2 16:18 ..\r\n",
      "-rwxr--r-- 1 1001 1001 6.1K Jun 20  2023 .DS_Store\r\n",
      "-rwxr--r-- 1 1001 1001 4.0K Jun 20  2023 ._.DS_Store\r\n",
      "-rw-rw-r-- 1 1001 1001    0 Oct 13 18:48 __init__.py\r\n",
      "drwxrwxr-x 2 1001 1001 4.0K Oct 13 18:48 fine-tuning-results\r\n",
      "drwxrwxr-x 2 1001 1001 4.0K Oct 13 18:48 jsonl\r\n",
      "-rw-rw-r-- 1 1001 1001 1.6K Oct 13 18:48 munger.py\r\n",
      "-rw-rw-r-- 1 1001 1001  36K Dec 18 19:16 search-terms.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 7.0K Oct 13 18:48 synthetic-data-load-url-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 7.3K Oct 13 18:48 synthetic-data-load-url-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  13K Oct 13 18:48 synthetic-data-none-of-the-above.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 8.5K Dec 17 00:42 synthetic-data-search-google-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  11K Oct 13 18:48 synthetic-data-search-google-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 8.5K Oct 13 18:48 synthetic-data-search-google-scholar-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  11K Oct 13 18:48 synthetic-data-search-google-scholar-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 7.6K Oct 13 18:48 synthetic-data-search-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 8.9K Dec 17 02:09 synthetic-data-search-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  723 Oct 13 18:48 training-commands.map\r\n",
      "-rw-rw-rw- 1 1001 1001 3.1M Dec 21 22:34 voice-commands-xml-test.jsonl\r\n",
      "-rw-rw-rw- 1 1001 1001  25M Dec 21 20:33 voice-commands-xml-train.jsonl\r\n",
      "-rw-rw-rw- 1 1001 1001 3.1M Dec 21 22:34 voice-commands-xml-validate.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "# Print current working directory\n",
    "# !ls -alh /var/model/Phind-CodeLlama-34B-v2\n",
    "# Change to /var/model/Phind-CodeLlama-34B-v2\n",
    "# os.chdir( \"/var/model/Phind-CodeLlama-34B-v2\" )\n",
    "# Print current working directory\n",
    "# os.getcwd()\n",
    "! ls -alh /var/model/genie-in-the-box/src/ephemera/prompts/data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:35:33.705571Z",
     "start_time": "2024-01-02T18:35:33.583802Z"
    }
   },
   "id": "644a6196802f8630"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import json\n",
    "from xmlschema import XMLSchema\n",
    "\n",
    "os.chdir( \"/var/model/genie-in-the-box/src\" )\n",
    "\n",
    "import lib.utils.util     as du\n",
    "import lib.utils.util_xml as dux\n",
    "import lib.utils.util_pytorch as dupt\n",
    "\n",
    "from ephemera.prompts.xml_fine_tuning_prompt_generator import XmlFineTuningPromptGenerator\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:35:39.593990Z",
     "start_time": "2024-01-02T18:35:38.578840Z"
    }
   },
   "id": "4e1faf28cf522ca7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Validation dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fcd418053c3052b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\"\n",
    "deepily_dataset_validation = du.get_file_as_list( path )\n",
    "deepily_dataset_validation = [ json.loads( line ) for line in deepily_dataset_validation ]\n",
    "len( deepily_dataset_validation )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:35:43.928010Z",
     "start_time": "2024-01-02T18:35:43.911963Z"
    }
   },
   "id": "ae76c88a9a5e791d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Use the Task below and the Input given to write the Response, which is a programmatic instruction that can solve the following Task:\n",
    "def prompt_instruction_format( sample ):\n",
    "    \n",
    "  return f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write a Response that can solve the following Task:\n",
    "\n",
    "    ### Task:\n",
    "    {sample['instruction']}\n",
    "\n",
    "    ### Input:\n",
    "    {sample['input']}\n",
    "\n",
    "    ### Response:\n",
    "    {sample['output']}\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:35:45.709276Z",
     "start_time": "2024-01-02T18:35:45.703422Z"
    }
   },
   "id": "58e3fd8b1b81ce1d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "    Use the Task below and the Input given to write a Response that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
      "\n",
      "        You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\n",
      "\n",
      "        Requirement: You MUST NOT use python code to answer this question.\n",
      "        Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
      "        Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n",
      "\n",
      "    ### Input:\n",
      "    \n",
      "        Below is the raw human voice command transcription formatted using simple XML:\n",
      "        \n",
      "        <human>\n",
      "            <voice-command>Perform Scaling and normalizing data in Pandas search on Google Scholar in a new tab</voice-command>\n",
      "        </human>\n",
      "        \n",
      "        The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
      "        \n",
      "        <response>\n",
      "            <browser-command></browser-command>\n",
      "            <args></args>\n",
      "        </response>\n",
      "\n",
      "        Requirement: The first word of your response MUST be `<response>`\n",
      "\n",
      "    ### Response:\n",
      "    \n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>Scaling and normalizing data in Pandas</args>\n",
      "        </response>\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for line in prompt_instruction_format( deepily_dataset_validation[ 0 ] ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:35:45.813193Z",
     "start_time": "2024-01-02T18:35:45.806313Z"
    }
   },
   "id": "e66898ac4c85e976"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def generate_text( foo_tokenizer, model, question, max_new_tokens=256, debug=False ):\n",
    "    \n",
    "    instruction = f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write a Response that can solve the following Task:\n",
    "\n",
    "    ### Task:\n",
    "    Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
    "\n",
    "    You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\n",
    "\n",
    "    Requirement: You MUST NOT use python code to answer this question.\n",
    "    Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
    "    Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n",
    "\n",
    "    ### Input:\n",
    "    \n",
    "    Below is the raw human voice command transcription formatted using simple XML:\n",
    "    \n",
    "    <human>\n",
    "        <voice-command>{question}</voice-command>\n",
    "    </human>\n",
    "    \n",
    "    The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
    "    \n",
    "    <response>\n",
    "        <browser-command></browser-command>\n",
    "        <args></args>\n",
    "    </response>\n",
    "\n",
    "    Requirement: The first word of your response MUST be `<response>`\n",
    "\n",
    "    ### Response:\"\"\"\n",
    "    \n",
    "    \n",
    "    device = \"cuda:0\"\n",
    "    inputs = foo_tokenizer( instruction, return_tensors=\"pt\" ).to( device )\n",
    "    \n",
    "    stop_token_id = foo_tokenizer.encode( \"</response>\" )[ 0 ]\n",
    "    \n",
    "    generation_output = model.generate(\n",
    "        input_ids=inputs[ \"input_ids\" ],\n",
    "        attention_mask=inputs[ \"attention_mask\" ],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=stop_token_id,\n",
    "        pad_token_id=stop_token_id\n",
    "    )\n",
    "        \n",
    "    if debug: \n",
    "        print( \"generation_output[ 0 ]:\", generation_output[ 0 ], end=\"\\n\\n\" )\n",
    "        print( \"generation_output[ 0 ].shape:\", generation_output[ 0 ].shape, end=\"\\n\\n\" )\n",
    "    \n",
    "    # Skip decoding the prompt part of the output   \n",
    "    input_length = inputs[ \"input_ids\" ].size( 1 )\n",
    "    raw_output = foo_tokenizer.decode( generation_output[ 0 ][ input_length: ] )\n",
    "    \n",
    "    if debug: \n",
    "        print( \"raw_output:\", raw_output, end=\"\\n\\n\" )\n",
    "        print(  \"len( raw_output ):\", len( raw_output ), end=\"\\n\\n\")\n",
    "    \n",
    "    # response   = raw_output.split( \"### Response:\" )[ 1 ]\n",
    "    \n",
    "    response = raw_output.replace( \"</s><s>\", \"\" ).strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "question = \"Ask Google scholar about QLORA and PEFT fine-tuning for XML output, show results in Another tab\"\n",
    "\n",
    "# for line in generate_text( tokenizer, base_model, question ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:35:50.766020Z",
     "start_time": "2024-01-02T18:35:50.760873Z"
    }
   },
   "id": "deee8f0e5ce7efd5"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir( \"/var/model/Phind-CodeLlama-34B-v2\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:35:53.906851Z",
     "start_time": "2024-01-02T18:35:53.866528Z"
    }
   },
   "id": "7ba53819b2920023"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "2477"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drops 16.4/19.0 GB per GPU down to 3.25 GB per GPU!\n",
    "import gc\n",
    "base_model = None \n",
    "adapter_plus_model = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:35:56.228248Z",
     "start_time": "2024-01-02T18:35:56.225295Z"
    }
   },
   "id": "23550e0e9f04149f"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c16c1c25712a4ee7a9231087b958cc28"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "print( bnb_config )\n",
    "tokenizer              = AutoTokenizer.from_pretrained( \".\" )\n",
    "tokenizer.pad_token    = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Identify the second GPU\n",
    "second_gpu = torch.device( 'cuda:1' ) if torch.cuda.is_available() else None\n",
    "\n",
    "# ¡OJO! Why were we turning off the cash here? \n",
    "# We're not! It makes a huge performance difference: 21 vs 14 tokens per second!\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \".\", quantization_config=bnb_config, device_map=\"cuda:1\", low_cpu_mem_usage=True, use_cache=True, use_flash_attention_2=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:36:51.227805Z",
     "start_time": "2024-01-02T18:36:14.751240Z"
    }
   },
   "id": "55afe25a14757322"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight: cuda:1\n",
      "model.layers.0.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.0.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.0.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.0.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.0.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.0.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.0.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.0.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.0.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.0.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.0.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.0.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.0.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.0.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.0.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.0.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.0.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.0.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.0.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.0.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.0.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.0.input_layernorm.weight: cuda:1\n",
      "model.layers.0.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.1.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.1.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.1.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.1.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.1.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.1.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.1.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.1.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.1.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.1.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.1.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.1.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.1.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.1.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.1.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.1.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.1.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.1.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.1.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.1.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.1.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.1.input_layernorm.weight: cuda:1\n",
      "model.layers.1.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.2.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.2.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.2.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.2.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.2.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.2.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.2.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.2.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.2.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.2.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.2.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.2.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.2.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.2.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.2.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.2.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.2.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.2.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.2.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.2.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.2.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.2.input_layernorm.weight: cuda:1\n",
      "model.layers.2.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.3.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.3.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.3.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.3.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.3.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.3.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.3.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.3.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.3.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.3.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.3.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.3.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.3.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.3.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.3.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.3.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.3.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.3.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.3.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.3.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.3.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.3.input_layernorm.weight: cuda:1\n",
      "model.layers.3.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.4.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.4.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.4.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.4.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.4.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.4.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.4.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.4.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.4.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.4.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.4.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.4.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.4.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.4.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.4.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.4.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.4.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.4.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.4.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.4.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.4.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.4.input_layernorm.weight: cuda:1\n",
      "model.layers.4.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.5.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.5.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.5.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.5.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.5.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.5.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.5.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.5.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.5.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.5.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.5.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.5.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.5.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.5.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.5.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.5.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.5.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.5.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.5.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.5.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.5.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.5.input_layernorm.weight: cuda:1\n",
      "model.layers.5.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.6.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.6.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.6.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.6.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.6.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.6.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.6.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.6.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.6.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.6.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.6.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.6.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.6.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.6.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.6.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.6.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.6.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.6.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.6.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.6.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.6.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.6.input_layernorm.weight: cuda:1\n",
      "model.layers.6.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.7.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.7.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.7.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.7.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.7.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.7.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.7.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.7.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.7.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.7.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.7.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.7.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.7.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.7.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.7.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.7.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.7.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.7.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.7.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.7.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.7.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.7.input_layernorm.weight: cuda:1\n",
      "model.layers.7.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.8.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.8.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.8.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.8.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.8.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.8.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.8.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.8.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.8.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.8.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.8.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.8.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.8.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.8.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.8.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.8.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.8.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.8.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.8.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.8.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.8.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.8.input_layernorm.weight: cuda:1\n",
      "model.layers.8.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.9.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.9.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.9.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.9.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.9.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.9.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.9.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.9.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.9.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.9.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.9.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.9.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.9.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.9.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.9.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.9.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.9.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.9.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.9.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.9.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.9.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.9.input_layernorm.weight: cuda:1\n",
      "model.layers.9.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.10.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.10.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.10.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.10.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.10.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.10.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.10.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.10.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.10.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.10.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.10.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.10.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.10.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.10.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.10.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.10.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.10.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.10.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.10.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.10.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.10.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.10.input_layernorm.weight: cuda:1\n",
      "model.layers.10.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.11.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.11.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.11.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.11.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.11.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.11.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.11.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.11.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.11.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.11.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.11.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.11.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.11.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.11.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.11.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.11.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.11.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.11.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.11.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.11.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.11.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.11.input_layernorm.weight: cuda:1\n",
      "model.layers.11.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.12.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.12.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.12.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.12.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.12.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.12.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.12.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.12.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.12.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.12.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.12.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.12.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.12.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.12.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.12.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.12.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.12.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.12.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.12.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.12.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.12.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.12.input_layernorm.weight: cuda:1\n",
      "model.layers.12.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.13.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.13.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.13.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.13.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.13.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.13.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.13.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.13.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.13.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.13.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.13.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.13.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.13.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.13.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.13.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.13.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.13.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.13.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.13.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.13.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.13.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.13.input_layernorm.weight: cuda:1\n",
      "model.layers.13.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.14.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.14.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.14.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.14.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.14.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.14.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.14.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.14.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.14.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.14.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.14.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.14.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.14.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.14.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.14.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.14.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.14.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.14.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.14.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.14.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.14.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.14.input_layernorm.weight: cuda:1\n",
      "model.layers.14.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.15.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.15.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.15.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.15.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.15.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.15.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.15.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.15.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.15.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.15.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.15.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.15.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.15.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.15.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.15.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.15.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.15.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.15.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.15.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.15.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.15.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.15.input_layernorm.weight: cuda:1\n",
      "model.layers.15.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.16.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.16.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.16.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.16.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.16.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.16.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.16.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.16.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.16.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.16.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.16.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.16.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.16.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.16.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.16.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.16.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.16.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.16.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.16.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.16.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.16.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.16.input_layernorm.weight: cuda:1\n",
      "model.layers.16.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.17.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.17.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.17.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.17.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.17.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.17.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.17.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.17.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.17.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.17.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.17.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.17.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.17.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.17.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.17.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.17.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.17.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.17.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.17.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.17.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.17.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.17.input_layernorm.weight: cuda:1\n",
      "model.layers.17.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.18.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.18.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.18.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.18.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.18.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.18.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.18.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.18.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.18.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.18.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.18.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.18.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.18.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.18.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.18.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.18.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.18.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.18.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.18.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.18.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.18.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.18.input_layernorm.weight: cuda:1\n",
      "model.layers.18.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.19.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.19.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.19.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.19.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.19.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.19.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.19.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.19.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.19.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.19.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.19.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.19.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.19.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.19.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.19.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.19.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.19.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.19.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.19.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.19.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.19.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.19.input_layernorm.weight: cuda:1\n",
      "model.layers.19.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.20.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.20.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.20.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.20.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.20.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.20.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.20.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.20.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.20.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.20.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.20.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.20.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.20.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.20.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.20.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.20.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.20.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.20.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.20.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.20.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.20.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.20.input_layernorm.weight: cuda:1\n",
      "model.layers.20.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.21.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.21.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.21.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.21.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.21.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.21.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.21.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.21.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.21.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.21.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.21.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.21.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.21.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.21.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.21.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.21.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.21.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.21.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.21.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.21.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.21.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.21.input_layernorm.weight: cuda:1\n",
      "model.layers.21.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.22.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.22.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.22.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.22.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.22.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.22.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.22.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.22.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.22.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.22.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.22.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.22.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.22.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.22.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.22.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.22.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.22.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.22.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.22.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.22.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.22.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.22.input_layernorm.weight: cuda:1\n",
      "model.layers.22.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.23.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.23.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.23.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.23.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.23.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.23.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.23.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.23.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.23.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.23.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.23.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.23.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.23.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.23.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.23.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.23.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.23.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.23.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.23.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.23.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.23.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.23.input_layernorm.weight: cuda:1\n",
      "model.layers.23.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.24.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.24.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.24.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.24.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.24.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.24.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.24.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.24.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.24.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.24.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.24.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.24.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.24.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.24.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.24.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.24.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.24.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.24.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.24.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.24.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.24.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.24.input_layernorm.weight: cuda:1\n",
      "model.layers.24.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.25.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.25.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.25.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.25.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.25.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.25.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.25.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.25.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.25.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.25.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.25.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.25.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.25.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.25.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.25.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.25.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.25.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.25.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.25.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.25.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.25.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.25.input_layernorm.weight: cuda:1\n",
      "model.layers.25.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.26.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.26.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.26.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.26.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.26.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.26.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.26.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.26.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.26.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.26.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.26.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.26.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.26.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.26.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.26.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.26.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.26.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.26.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.26.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.26.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.26.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.26.input_layernorm.weight: cuda:1\n",
      "model.layers.26.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.27.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.27.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.27.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.27.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.27.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.27.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.27.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.27.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.27.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.27.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.27.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.27.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.27.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.27.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.27.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.27.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.27.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.27.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.27.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.27.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.27.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.27.input_layernorm.weight: cuda:1\n",
      "model.layers.27.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.28.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.28.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.28.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.28.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.28.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.28.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.28.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.28.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.28.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.28.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.28.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.28.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.28.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.28.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.28.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.28.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.28.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.28.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.28.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.28.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.28.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.28.input_layernorm.weight: cuda:1\n",
      "model.layers.28.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.29.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.29.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.29.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.29.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.29.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.29.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.29.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.29.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.29.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.29.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.29.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.29.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.29.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.29.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.29.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.29.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.29.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.29.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.29.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.29.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.29.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.29.input_layernorm.weight: cuda:1\n",
      "model.layers.29.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.30.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.30.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.30.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.30.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.30.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.30.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.30.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.30.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.30.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.30.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.30.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.30.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.30.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.30.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.30.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.30.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.30.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.30.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.30.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.30.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.30.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.30.input_layernorm.weight: cuda:1\n",
      "model.layers.30.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.31.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.31.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.31.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.31.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.31.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.31.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.31.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.31.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.31.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.31.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.31.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.31.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.31.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.31.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.31.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.31.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.31.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.31.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.31.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.31.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.31.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.31.input_layernorm.weight: cuda:1\n",
      "model.layers.31.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.32.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.32.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.32.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.32.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.32.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.32.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.32.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.32.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.32.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.32.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.32.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.32.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.32.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.32.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.32.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.32.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.32.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.32.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.32.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.32.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.32.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.32.input_layernorm.weight: cuda:1\n",
      "model.layers.32.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.33.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.33.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.33.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.33.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.33.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.33.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.33.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.33.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.33.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.33.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.33.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.33.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.33.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.33.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.33.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.33.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.33.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.33.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.33.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.33.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.33.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.33.input_layernorm.weight: cuda:1\n",
      "model.layers.33.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.34.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.34.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.34.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.34.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.34.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.34.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.34.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.34.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.34.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.34.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.34.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.34.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.34.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.34.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.34.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.34.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.34.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.34.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.34.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.34.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.34.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.34.input_layernorm.weight: cuda:1\n",
      "model.layers.34.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.35.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.35.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.35.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.35.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.35.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.35.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.35.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.35.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.35.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.35.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.35.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.35.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.35.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.35.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.35.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.35.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.35.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.35.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.35.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.35.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.35.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.35.input_layernorm.weight: cuda:1\n",
      "model.layers.35.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.36.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.36.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.36.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.36.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.36.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.36.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.36.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.36.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.36.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.36.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.36.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.36.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.36.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.36.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.36.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.36.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.36.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.36.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.36.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.36.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.36.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.36.input_layernorm.weight: cuda:1\n",
      "model.layers.36.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.37.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.37.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.37.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.37.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.37.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.37.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.37.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.37.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.37.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.37.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.37.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.37.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.37.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.37.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.37.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.37.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.37.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.37.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.37.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.37.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.37.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.37.input_layernorm.weight: cuda:1\n",
      "model.layers.37.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.38.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.38.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.38.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.38.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.38.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.38.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.38.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.38.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.38.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.38.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.38.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.38.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.38.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.38.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.38.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.38.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.38.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.38.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.38.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.38.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.38.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.38.input_layernorm.weight: cuda:1\n",
      "model.layers.38.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.39.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.39.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.39.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.39.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.39.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.39.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.39.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.39.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.39.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.39.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.39.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.39.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.39.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.39.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.39.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.39.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.39.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.39.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.39.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.39.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.39.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.39.input_layernorm.weight: cuda:1\n",
      "model.layers.39.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.40.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.40.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.40.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.40.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.40.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.40.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.40.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.40.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.40.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.40.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.40.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.40.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.40.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.40.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.40.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.40.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.40.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.40.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.40.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.40.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.40.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.40.input_layernorm.weight: cuda:1\n",
      "model.layers.40.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.41.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.41.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.41.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.41.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.41.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.41.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.41.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.41.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.41.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.41.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.41.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.41.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.41.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.41.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.41.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.41.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.41.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.41.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.41.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.41.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.41.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.41.input_layernorm.weight: cuda:1\n",
      "model.layers.41.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.42.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.42.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.42.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.42.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.42.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.42.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.42.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.42.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.42.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.42.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.42.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.42.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.42.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.42.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.42.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.42.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.42.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.42.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.42.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.42.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.42.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.42.input_layernorm.weight: cuda:1\n",
      "model.layers.42.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.43.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.43.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.43.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.43.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.43.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.43.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.43.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.43.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.43.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.43.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.43.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.43.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.43.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.43.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.43.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.43.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.43.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.43.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.43.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.43.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.43.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.43.input_layernorm.weight: cuda:1\n",
      "model.layers.43.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.44.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.44.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.44.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.44.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.44.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.44.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.44.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.44.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.44.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.44.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.44.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.44.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.44.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.44.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.44.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.44.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.44.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.44.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.44.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.44.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.44.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.44.input_layernorm.weight: cuda:1\n",
      "model.layers.44.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.45.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.45.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.45.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.45.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.45.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.45.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.45.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.45.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.45.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.45.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.45.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.45.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.45.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.45.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.45.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.45.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.45.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.45.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.45.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.45.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.45.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.45.input_layernorm.weight: cuda:1\n",
      "model.layers.45.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.46.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.46.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.46.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.46.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.46.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.46.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.46.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.46.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.46.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.46.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.46.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.46.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.46.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.46.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.46.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.46.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.46.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.46.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.46.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.46.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.46.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.46.input_layernorm.weight: cuda:1\n",
      "model.layers.46.post_attention_layernorm.weight: cuda:1\n",
      "model.layers.47.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.47.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.47.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "model.layers.47.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.47.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.47.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "model.layers.47.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.47.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.47.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "model.layers.47.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.47.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.47.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "model.layers.47.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.47.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.47.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "model.layers.47.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.47.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.47.mlp.up_proj.base_layer.weight: cuda:1\n",
      "model.layers.47.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "model.layers.47.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "model.layers.47.mlp.down_proj.base_layer.weight: cuda:1\n",
      "model.layers.47.input_layernorm.weight: cuda:1\n",
      "model.layers.47.post_attention_layernorm.weight: cuda:1\n",
      "model.norm.weight: cuda:1\n",
      "lm_head.weight: cuda:1\n"
     ]
    }
   ],
   "source": [
    "dupt.print_device_allocation( base_model )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:37:00.930707Z",
     "start_time": "2024-01-02T18:37:00.922103Z"
    }
   },
   "id": "4b0c2465c3fe3182"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>QLORA and PEFT fine-tuning for XML output</args>\n",
      "        </response>]\n"
     ]
    }
   ],
   "source": [
    "response = generate_text( tokenizer, base_model, question )\n",
    "response = f\"[{response}]\"\n",
    "for line in response.split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:37:23.410215Z",
     "start_time": "2024-01-02T18:37:16.605664Z"
    }
   },
   "id": "2bbddb04a5a998aa"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "adapter_plus_model = PeftModel.from_pretrained( base_model, \"adapters/00-browser-vox-command\", use_flash_attention_2=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:37:44.326481Z",
     "start_time": "2024-01-02T18:37:43.140537Z"
    }
   },
   "id": "34796358a34b99c6"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Move the model to GPU\n",
    "adapter_plus_model = adapter_plus_model.to( \"cuda:1\" ) # where device is your GPU device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:38:50.507025Z",
     "start_time": "2024-01-02T18:38:50.377721Z"
    }
   },
   "id": "e5e1ea7e663a3536"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.0.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.1.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.2.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.3.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.4.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.5.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.6.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.7.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.8.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.9.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.10.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.11.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.12.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.13.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.14.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.15.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.32.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.32.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.32.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.33.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.33.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.33.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.34.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.34.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.34.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.35.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.35.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.35.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.36.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.36.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.36.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.37.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.37.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.37.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.38.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.38.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.38.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.39.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.39.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.39.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.40.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.40.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.40.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.41.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.41.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.41.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.42.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.42.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.42.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.43.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.43.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.43.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.44.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.44.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.44.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.45.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.45.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.45.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.46.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.46.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.46.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.47.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.47.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.47.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.norm.weight: cuda:1\n",
      "base_model.model.lm_head.weight: cuda:1\n"
     ]
    }
   ],
   "source": [
    "dupt.print_device_allocation( adapter_plus_model )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:38:52.760362Z",
     "start_time": "2024-01-02T18:38:52.724866Z"
    }
   },
   "id": "4a4d57dcf172749f"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>QLORA and PEFT fine-tuning for XML output</args>\n",
      "        </response>\n"
     ]
    }
   ],
   "source": [
    "for line in generate_text( tokenizer, adapter_plus_model, question ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:39:13.104389Z",
     "start_time": "2024-01-02T18:39:07.060744Z"
    }
   },
   "id": "d84a5998d65cb738"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:41:28.283275Z",
     "start_time": "2024-01-02T18:41:28.278101Z"
    }
   },
   "id": "f16775997e14a03c"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           instruction  \\\n584  Your job is to discern the intent of a human v...   \n268  Your job is to discern the intent of a human v...   \n164  Your job is to discern the intent of a human v...   \n594  Your job is to discern the intent of a human v...   \n64   Your job is to discern the intent of a human v...   \n\n                                                 input  \\\n584  \\n        Below is the raw human voice command...   \n268  \\n        Below is the raw human voice command...   \n164  \\n        Below is the raw human voice command...   \n594  \\n        Below is the raw human voice command...   \n64   \\n        Below is the raw human voice command...   \n\n                                                output  \\\n584  \\n        <response>\\n            <browser-com...   \n268  \\n        <response>\\n            <browser-com...   \n164  \\n        <response>\\n            <browser-com...   \n594  \\n        <response>\\n            <browser-com...   \n64   \\n        <response>\\n            <browser-com...   \n\n                                                prompt  \n584  ### Instruction:\\n    Use the Task and Input g...  \n268  ### Instruction:\\n    Use the Task and Input g...  \n164  ### Instruction:\\n    Use the Task and Input g...  \n594  ### Instruction:\\n    Use the Task and Input g...  \n64   ### Instruction:\\n    Use the Task and Input g...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>input</th>\n      <th>output</th>\n      <th>prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>584</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\n        Below is the raw human voice command...</td>\n      <td>\\n        &lt;response&gt;\\n            &lt;browser-com...</td>\n      <td>### Instruction:\\n    Use the Task and Input g...</td>\n    </tr>\n    <tr>\n      <th>268</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\n        Below is the raw human voice command...</td>\n      <td>\\n        &lt;response&gt;\\n            &lt;browser-com...</td>\n      <td>### Instruction:\\n    Use the Task and Input g...</td>\n    </tr>\n    <tr>\n      <th>164</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\n        Below is the raw human voice command...</td>\n      <td>\\n        &lt;response&gt;\\n            &lt;browser-com...</td>\n      <td>### Instruction:\\n    Use the Task and Input g...</td>\n    </tr>\n    <tr>\n      <th>594</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\n        Below is the raw human voice command...</td>\n      <td>\\n        &lt;response&gt;\\n            &lt;browser-com...</td>\n      <td>### Instruction:\\n    Use the Task and Input g...</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\n        Below is the raw human voice command...</td>\n      <td>\\n        &lt;response&gt;\\n            &lt;browser-com...</td>\n      <td>### Instruction:\\n    Use the Task and Input g...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df = pd.read_json( \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\", lines=True ).sample( 100)\n",
    "validation_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:43:05.587821Z",
     "start_time": "2024-01-02T18:43:05.547628Z"
    }
   },
   "id": "21de2f127247b122"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(100, 4)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:43:07.924468Z",
     "start_time": "2024-01-02T18:43:07.911919Z"
    }
   },
   "id": "c5263f96c53b2c09"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "'Open a new tab and fetch Google Scholar results for What are common causes and solutions for errors related to incorrect syntax in Python?'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = validation_df[ 'input' ].tolist()\n",
    "for idx, question in enumerate( questions ):\n",
    "    question = dux.get_value_by_xml_tag_name( question, \"voice-command\" )\n",
    "    # print( question )\n",
    "    questions[ idx ] = question\n",
    "    \n",
    "questions[ 0 ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:43:11.152120Z",
     "start_time": "2024-01-02T18:43:11.149183Z"
    }
   },
   "id": "e987324d2b86f79a"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "'<response>\\n            <browser-command>search google scholar new tab</browser-command>\\n            <args>What are common causes and solutions for errors related to incorrect syntax in Python?</args>\\n        </response>'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text( tokenizer, adapter_plus_model, questions[ 0 ] )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:43:25.667309Z",
     "start_time": "2024-01-02T18:43:19.517175Z"
    }
   },
   "id": "251cb152811e288d"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:44:11.053873Z",
     "start_time": "2024-01-02T18:44:11.039905Z"
    }
   },
   "id": "f1030836998a7aa"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] of [100] [1.0%] [Open a new tab and fetch Google Scholar results for What are common causes and solutions for errors related to incorrect syntax in Python?]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>What are common causes and solutions for errors related to incorrect syntax in Python?</args></response>]\n",
      "[2] of [100] [2.0%] [Start a Google search of Efficient file reading with Pandas in this tab]\n",
      "[<response><browser-command>search google current tab</browser-command><args>Efficient file reading with Pandas</args></response>]\n",
      "[3] of [100] [3.0%] [Search and provide results for Scikit-Learn documentation: Where can you find comprehensive documentation for Scikit-Learn? here]\n",
      "[<response><browser-command>search current tab</browser-command><args>Scikit-Learn documentation: Where can you find comprehensive documentation for Scikit-Learn?</args></response>]\n",
      "[4] of [100] [4.0%] [Delve for classic rock music in this window]\n",
      "[<response><browser-command>search current tab</browser-command><args>classic rock music</args></response>]\n",
      "[5] of [100] [5.0%] [Get Google Scholar results for Syntax Warning in a new tab]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>Syntax Warning</args></response>]\n",
      "[6] of [100] [6.0%] [Open a fresh tab, Google search how to plant tomatoes]\n",
      "[<response><browser-command>search google new tab</browser-command><args>how to plant tomatoes</args></response>]\n",
      "[7] of [100] [7.0%] [Let's go, Google Scholar Converting between DataFrame and Series]\n",
      "[<response><browser-command>search google scholar current tab</browser-command><args>Converting between DataFrame and Series</args></response>]\n",
      "[8] of [100] [8.0%] [Run Google Scholar for What are the causes of translation errors with Unicode characters in Python and how can they be fixed?]\n",
      "[<response><browser-command>search google scholar current tab</browser-command><args>What are the causes of translation errors with Unicode characters in Python and how can they be fixed?</args></response>]\n",
      "[9] of [100] [9.0%] [Begin a Google search in a new tab for Convolutional Neural Networks: What are the primary applications of convolutional neural networks in image processing?]\n",
      "[<response><browser-command>search google new tab</browser-command><args>Convolutional Neural Networks: What are the primary applications of convolutional neural networks in image processing?</args></response>]\n",
      "[10] of [100] [10.0%] [Probe and present Voice Recognition technologies: What are the latest advancements in voice recognition technology? in this tab]\n",
      "[<response><browser-command>search current tab</browser-command><args>Voice Recognition technologies: What are the latest advancements in voice recognition technology?</args></response>]\n",
      "[11] of [100] [11.0%] [In a new tab, Google Scholar search for TensorFlow latest updates: What are the latest features added to TensorFlow in its most recent update?]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>TensorFlow latest updates: What are the latest features added to TensorFlow in its most recent update?</args></response>]\n",
      "[12] of [100] [12.0%] [Execute Google Scholar how to brew beer at home]\n",
      "[<response><browser-command>search google scholar current tab</browser-command><args>how to brew beer at home</args></response>]\n",
      "[13] of [100] [13.0%] [Look for Warning: General warning message on Google]\n",
      "[<response><browser-command>search google current tab</browser-command><args>Warning: General warning message</args></response>]\n",
      "[14] of [100] [14.0%] [Bring up how to grow plants indoors in another tab]\n",
      "[<response><browser-command>search new tab</browser-command><args>how to grow plants indoors</args></response>]\n",
      "[15] of [100] [15.0%] [Render Google search results for VMS Error in current tab]\n",
      "[<response><browser-command>search google current tab</browser-command><args>VMS Error</args></response>]\n",
      "[16] of [100] [16.0%] [Do a Google research for top 10 TV shows of all time]\n",
      "[<response><browser-command>search google current tab</browser-command><args>top 10 TV shows of all time</args></response>]\n",
      "[17] of [100] [17.0%] [Search for ChildProcessError, open in new tab]\n",
      "[<response><browser-command>search new tab</browser-command><args>ChildProcessError</args></response>]\n",
      "[18] of [100] [18.0%] [Google Computer Vision applications: What are the groundbreaking applications of computer vision in today's technology? within this tab]\n",
      "[<response><browser-command>search google current tab</browser-command><args>Computer Vision applications: What are the groundbreaking applications of computer vision in today's technology?</args></response>]\n",
      "[19] of [100] [19.0%] [Open new tab with Google Scholar search results for how to compost at home]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>how to compost at home</args></response>]\n",
      "[20] of [100] [20.0%] [Tab new: Google Scholar How can child process-related errors be managed and resolved in Python?]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>How can child process-related errors be managed and resolved in Python?</args></response>]\n",
      "[21] of [100] [21.0%] [Perform Google Scholar search with Bytes Warning: Bytecode issue in the current window]\n",
      "[<response><browser-command>search google scholar current tab</browser-command><args>Bytes Warning: Bytecode issue</args></response>]\n",
      "[22] of [100] [22.0%] [Google Scholar OS Error, open new tab]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>OS Error</args></response>]\n",
      "[23] of [100] [23.0%] [Conduct a Google search on AI in insurance underwriting: How is AI changing the landscape of insurance underwriting?]\n",
      "[<response><browser-command>search google current tab</browser-command><args>AI in insurance underwriting: How is AI changing the landscape of insurance underwriting?</args></response>]\n",
      "[24] of [100] [24.0%] [Carry out a Google lookup for Machine Learning in logistics, results in new tab]\n",
      "[<response><browser-command>search google new tab</browser-command><args>Machine Learning in logistics</args></response>]\n",
      "[25] of [100] [25.0%] [funny jokes for kids, find in new tab]\n",
      "[<response><browser-command>search new tab</browser-command><args>funny jokes for kids</args></response>]\n",
      "[26] of [100] [26.0%] [Open new tab, search for Syntax Error: Invalid syntax]\n",
      "[<response><browser-command>search new tab</browser-command><args>Syntax Error: Invalid syntax</args></response>]\n",
      "[27] of [100] [27.0%] [Bring up Reference Error in another tab]\n",
      "[<response><browser-command>search new tab</browser-command><args>Reference Error</args></response>]\n",
      "[28] of [100] [28.0%] [Start Google search of What steps can you take to prepare for features that will be deprecated in future Python releases? here]\n",
      "[<response><browser-command>search google current tab</browser-command><args>What steps can you take to prepare for features that will be deprecated in future Python releases?</args></response>]\n",
      "[29] of [100] [29.0%] [Go look up classic novels on Google Scholar in a new tab]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>classic novels</args></response>]\n",
      "[30] of [100] [30.0%] [Run a ImportWarning search in the active window]\n",
      "[<response><browser-command>search current tab</browser-command><args>ImportWarning</args></response>]\n",
      "[31] of [100] [31.0%] [Get Google Scholar results for Word Embeddings: How do word embeddings capture semantic relationships in natural language processing? in a new tab]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>Word Embeddings: How do word embeddings capture semantic relationships in natural language processing?</args></response>]\n",
      "[32] of [100] [32.0%] [Start a Google search on Cross-tabulation in Pandas in a new tab]\n",
      "[<response><browser-command>search google new tab</browser-command><args>Cross-tabulation in Pandas</args></response>]\n",
      "[33] of [100] [33.0%] [Now, Google Scholar Quantum AI computing developments: What are the latest developments in quantum AI computing?]\n",
      "[<response><browser-command>search google scholar current tab</browser-command><args>Quantum AI computing developments: What are the latest developments in quantum AI computing?</args></response>]\n",
      "[34] of [100] [34.0%] [Please run a Google search for classic rock music in a new tab]\n",
      "[<response><browser-command>search google new tab</browser-command><args>classic rock music</args></response>]\n",
      "[35] of [100] [35.0%] [Conduct a Google search on healthy snack ideas in a new tab]\n",
      "[<response><browser-command>search google new tab</browser-command><args>healthy snack ideas</args></response>]\n",
      "[36] of [100] [36.0%] [Generate Google Scholar results for best hiking backpacks in a new tab]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>best hiking backpacks</args></response>]\n",
      "[37] of [100] [37.0%] [Proceed to Google Scholar in a new tab and search for top 10 video games of all time]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>top 10 video games of all time</args></response>]\n",
      "[38] of [100] [38.0%] [Search Connection Refused Error in this window]\n",
      "[<response><browser-command>search current tab</browser-command><args>Connection Refused Error</args></response>]\n",
      "[39] of [100] [39.0%] [Unleash Google Scholar search on Scaling and normalizing data in Pandas here]\n",
      "[<response><browser-command>search google scholar current tab</browser-command><args>Scaling and normalizing data in Pandas</args></response>]\n",
      "[40] of [100] [40.0%] [In a new tab, find Type Error on Google Scholar]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>Type Error</args></response>]\n",
      "[41] of [100] [41.0%] [Please Google Scholar Using Pandas for predictive modeling in a new tab]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>Using Pandas for predictive modeling</args></response>]\n",
      "[42] of [100] [42.0%] [This tab: Search for Handling duplicate data in Pandas]\n",
      "[<response><browser-command>search current tab</browser-command><args>Handling duplicate data in Pandas</args></response>]\n",
      "[43] of [100] [43.0%] [Tab search for How do you address deprecation warnings in Python to ensure code compatibility with future versions?]\n",
      "[<response><browser-command>search current tab</browser-command><args>How do you address deprecation warnings in Python to ensure code compatibility with future versions?</args></response>]\n",
      "[44] of [100] [44.0%] [In this tab, Google the terms Import Warning: Module import issue]\n",
      "[<response><browser-command>search google current tab</browser-command><args>Import Warning: Module import issue</args></response>]\n",
      "[45] of [100] [45.0%] [New tab with Google Scholar search for Converting data types in Pandas]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>Converting data types in Pandas</args></response>]\n",
      "[46] of [100] [46.0%] [Look up best burger joints on Google Scholar]\n",
      "[<response><browser-command>search google scholar current tab</browser-command><args>best burger joints</args></response>]\n",
      "[47] of [100] [47.0%] [Perform a Google examination for Using Pandas in web development]\n",
      "[<response><browser-command>search google current tab</browser-command><args>Using Pandas in web development</args></response>]\n",
      "[48] of [100] [48.0%] [Please launch a Google search for why do apples turn brown in a new tab]\n",
      "[<response><browser-command>search google new tab</browser-command><args>why do apples turn brown</args></response>]\n",
      "[49] of [100] [49.0%] [Query why we dream on Google in the current tab]\n",
      "[<response><browser-command>search google current tab</browser-command><args>why we dream</args></response>]\n",
      "[50] of [100] [50.0%] [Google Scholar Advanced DataFrame filtering, new tab]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>Advanced DataFrame filtering</args></response>]\n",
      "[51] of [100] [51.0%] [Make a Google search for Dealing with outliers in Pandas, open in new tab]\n",
      "[<response><browser-command>search google new tab</browser-command><args>Dealing with outliers in Pandas</args></response>]\n",
      "[52] of [100] [52.0%] [Google best vegan restaurants in the current browser tab]\n",
      "[<response><browser-command>search google current tab</browser-command><args>best vegan restaurants</args></response>]\n",
      "[53] of [100] [53.0%] [Google Scholar, let's find Natural Language Understanding]\n",
      "[<response><browser-command>search google scholar current tab</browser-command><args>Natural Language Understanding</args></response>]\n",
      "[54] of [100] [54.0%] [Generate new tab, Recursion Error search]\n",
      "[<response><browser-command>search new tab</browser-command><args>Recursion Error</args></response>]\n",
      "[55] of [100] [55.0%] [Inquire Deep Learning optimization in current window]\n",
      "[<response><browser-command>search current tab</browser-command><args>Deep Learning optimization</args></response>]\n",
      "[56] of [100] [56.0%] [Uncover Machine Learning in logistics in a new tab]\n",
      "[<response><browser-command>search new tab</browser-command><args>Machine Learning in logistics</args></response>]\n",
      "[57] of [100] [57.0%] [Go to Google Scholar and search for What are the common causes of tab-related errors in Python, especially in indentation?, new tab]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>What are the common causes of tab-related errors in Python, especially in indentation?</args></response>]\n",
      "[58] of [100] [58.0%] [Run Google Scholar for Stop Iteration: Iteration stopped]\n",
      "[<response><browser-command>search google scholar current tab</browser-command><args>Stop Iteration: Iteration stopped</args></response>]\n",
      "[59] of [100] [59.0%] [New tab, execute How do you handle situations where a feature or method is not yet implemented in Python? search]\n",
      "[<response><browser-command>search new tab</browser-command><args>How do you handle situations where a feature or method is not yet implemented in Python?</args></response>]\n",
      "[60] of [100] [60.0%] [Please do a AI-driven chatbots search in another tab]\n",
      "[<response><browser-command>search new tab</browser-command><args>AI-driven chatbots</args></response>]\n",
      "[61] of [100] [61.0%] [Google Scholar Import Warning, let's go]\n",
      "[<response><browser-command>search google scholar current tab</browser-command><args>Import Warning</args></response>]\n",
      "[62] of [100] [62.0%] [Show Google Scholar results for how to make a resume in a new tab]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>how to make a resume</args></response>]\n",
      "[63] of [100] [63.0%] [Do Google Scholar search for AI in digital marketing: What role does AI play in digital marketing strategies? in current view]\n",
      "[<response><browser-command>search google scholar current tab</browser-command><args>AI in digital marketing: What role does AI play in digital marketing strategies?</args></response>]\n",
      "[64] of [100] [64.0%] [Google DataFrame column operations, open results in a new tab]\n",
      "[<response><browser-command>search google new tab</browser-command><args>DataFrame column operations</args></response>]\n",
      "[65] of [100] [65.0%] [Display Google Scholar search results for Pandas DataFrame slicing in this window]\n",
      "[<response><browser-command>search google scholar current tab</browser-command><args>Pandas DataFrame slicing</args></response>]\n",
      "[66] of [100] [66.0%] [Tab: Conduct how to play guitar search]\n",
      "[<response><browser-command>search current tab</browser-command><args>Tab: Conduct how to play guitar search</args></response>]\n",
      "[67] of [100] [67.0%] [Execute running shoes review lookup in the current tab]\n",
      "[<response><browser-command>search current tab</browser-command><args>running shoes review</args></response>]\n",
      "[68] of [100] [68.0%] [Show Google Scholar results for Converting data types in Pandas in a new tab]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>Converting data types in Pandas</args></response>]\n",
      "[69] of [100] [69.0%] [Hunt best hiking shoes in another tab]\n",
      "[<response><browser-command>search new tab</browser-command><args>best hiking shoes</args></response>]\n",
      "[70] of [100] [70.0%] [Browse for How do you handle division errors, especially division by zero, in Python? on Google in this window]\n",
      "[<response><browser-command>search google current tab</browser-command><args>How do you handle division errors, especially division by zero, in Python?</args></response>]\n",
      "[71] of [100] [71.0%] [Initiate a Google search in a new tab for Unicode Warning]\n",
      "[<response><browser-command>search google new tab</browser-command><args>Unicode Warning</args></response>]\n",
      "[72] of [100] [72.0%] [Seek best restaurants near me via Google]\n",
      "[<response><browser-command>search google current tab</browser-command><args>best restaurants near me</args></response>]\n",
      "[73] of [100] [73.0%] [Study latest breakthroughs in medicine on Google from this place]\n",
      "[<response><browser-command>search google current tab</browser-command><args>latest breakthroughs in medicine</args></response>]\n",
      "[74] of [100] [74.0%] [Display TensorFlow latest updates results in new tab]\n",
      "[<response><browser-command>search new tab</browser-command><args>TensorFlow latest updates</args></response>]\n",
      "[75] of [100] [75.0%] [New tab, conduct Google search on AI in retail analytics: How is AI transforming retail analytics and customer insights?]\n",
      "[<response><browser-command>search google new tab</browser-command><args>AI in retail analytics: How is AI transforming retail analytics and customer insights?</args></response>]\n",
      "[76] of [100] [76.0%] [Search for Windows Error in the present tab]\n",
      "[<response><browser-command>search current tab</browser-command><args>Windows Error</args></response>]\n",
      "[77] of [100] [77.0%] [Let's see the Google Scholar results for Pandas string operations in a new tab]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>Pandas string operations</args></response>]\n",
      "[78] of [100] [78.0%] [Do a search for Handling time zones in Pandas and show results in another tab]\n",
      "[<response><browser-command>search new tab</browser-command><args>Handling time zones in Pandas</args></response>]\n",
      "[79] of [100] [79.0%] [New tab and Google Scholar BERT architecture please]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>BERT architecture</args></response>]\n",
      "[80] of [100] [80.0%] [Run a search for Feature Engineering best practices: How does feature engineering improve the performance of machine learning models? and show results in another tab]\n",
      "[<response><browser-command>search new tab</browser-command><args>Feature Engineering best practices: How does feature engineering improve the performance of machine learning models?</args></response>]\n",
      "[81] of [100] [81.0%] [Go fetch Google Scholar results for AI in virtual reality in a new tab]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>AI in virtual reality</args></response>]\n",
      "[82] of [100] [82.0%] [Present window: Search Predictive analytics tools: What are some of the most effective tools for predictive analytics?]\n",
      "[<response><browser-command>search current tab</browser-command><args>Predictive analytics tools: What are some of the most effective tools for predictive analytics?</args></response>]\n",
      "[83] of [100] [83.0%] [In a new tab, find OS Error: Operating system error on Google Scholar]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>OS Error: Operating system error</args></response>]\n",
      "[84] of [100] [84.0%] [Now in this tab, Google Scholar popular video games]\n",
      "[<response><browser-command>search google scholar current tab</browser-command><args>popular video games</args></response>]\n",
      "[85] of [100] [85.0%] [Research Outlier Detection methods and present results]\n",
      "[<response><browser-command>search new tab</browser-command><args>Outlier Detection methods</args></response>]\n",
      "[86] of [100] [86.0%] [Run a Future Warning: Future change warning search in the active window]\n",
      "[<response><browser-command>search current tab</browser-command><args>Future Warning: Future change warning</args></response>]\n",
      "[87] of [100] [87.0%] [Launch a Google search for AI in sports analytics: How is AI used in sports analytics to improve performance? in a new tab]\n",
      "[<response><browser-command>search google new tab</browser-command><args>AI in sports analytics: How is AI used in sports analytics to improve performance?</args></response>]\n",
      "[88] of [100] [88.0%] [Scour Data anonymization in Pandas in this window]\n",
      "[<response><browser-command>search current tab</browser-command><args>Data anonymization in Pandas</args></response>]\n",
      "[89] of [100] [89.0%] [Look up FloatingPointError here]\n",
      "[<response><browser-command>search current tab</browser-command><args>FloatingPointError</args></response>]\n",
      "[90] of [100] [90.0%] [Here, Google Scholar Attribute Error: Object has no attribute]\n",
      "[<response><browser-command>search google scholar current tab</browser-command><args>Attribute Error: Object has no attribute</args></response>]\n",
      "[91] of [100] [91.0%] [Run a Google inquiry for SSL Error]\n",
      "[<response><browser-command>search google current tab</browser-command><args>SSL Error</args></response>]\n",
      "[92] of [100] [92.0%] [Execute a Google Scholar search in a new tab for Using Pandas for predictive modeling]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>Using Pandas for predictive modeling</args></response>]\n",
      "[93] of [100] [93.0%] [Current tab, Apache Spark data processing: How does Apache Spark handle large-scale data processing efficiently? lookup]\n",
      "[<response><browser-command>search current tab</browser-command><args>Apache Spark data processing: How does Apache Spark handle large-scale data processing efficiently?</args></response>]\n",
      "[94] of [100] [94.0%] [Google Scholar, search why stars twinkle in a new tab]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>why stars twinkle</args></response>]\n",
      "[95] of [100] [95.0%] [Open new tab, search for how to write a resume]\n",
      "[<response><browser-command>search new tab</browser-command><args>how to write a resume</args></response>]\n",
      "[96] of [100] [96.0%] [Find, fetch, and show Precision-Recall trade-off: In what scenarios is the precision-recall trade-off critical in model evaluation? in this tab]\n",
      "[<response><browser-command>search current tab</browser-command><args>Precision-Recall trade-off: In what scenarios is the precision-recall trade-off critical in model evaluation?</args></response>]\n",
      "[97] of [100] [97.0%] [Delve for what are the benefits of exercise? in this window]\n",
      "[<response><browser-command>search current tab</browser-command><args>what are the benefits of exercise?</args></response>]\n",
      "[98] of [100] [98.0%] [Seek and show UserWarning results here]\n",
      "[<response><browser-command>search current tab</browser-command><args>UserWarning</args></response>]\n",
      "[99] of [100] [99.0%] [Go look up Recursion Error: Maximum recursion depth exceeded on Google Scholar in a new tab]\n",
      "[<response><browser-command>search google scholar new tab</browser-command><args>Recursion Error: Maximum recursion depth exceeded</args></response>]\n",
      "[100] of [100] [100.0%] [Probe AI in human resources: What are the applications of AI in human resources management? on Google from here]\n",
      "[<response><browser-command>search google current tab</browser-command><args>AI in human resources: What are the applications of AI in human resources management?</args></response>]\n"
     ]
    }
   ],
   "source": [
    "responses = [ ]\n",
    "for idx, question in enumerate( questions ):\n",
    "    # Display progress every nth iteration\n",
    "    print( f\"[{idx + 1}] of [{len( questions )}] [{( ( idx + 1 ) / len( questions ) ) * 100:.1f}%] [{question}]\" )\n",
    "    response = generate_text( tokenizer, adapter_plus_model, question )\n",
    "    response = re.sub( r'>\\s+<', '><', response )\n",
    "    print( f\"[{response}]\" )\n",
    "    responses.append( response )\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:53:20.542478Z",
     "start_time": "2024-01-02T18:44:18.322124Z"
    }
   },
   "id": "e641d66989b265ca"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "100"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( responses )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:53:20.563543Z",
     "start_time": "2024-01-02T18:53:20.542062Z"
    }
   },
   "id": "a05b5f377ca0be59"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           instruction  \\\n584  Your job is to discern the intent of a human v...   \n268  Your job is to discern the intent of a human v...   \n164  Your job is to discern the intent of a human v...   \n594  Your job is to discern the intent of a human v...   \n64   Your job is to discern the intent of a human v...   \n\n                                                 input  \\\n584  \\n        Below is the raw human voice command...   \n268  \\n        Below is the raw human voice command...   \n164  \\n        Below is the raw human voice command...   \n594  \\n        Below is the raw human voice command...   \n64   \\n        Below is the raw human voice command...   \n\n                                                output  \\\n584  \\n        <response>\\n            <browser-com...   \n268  \\n        <response>\\n            <browser-com...   \n164  \\n        <response>\\n            <browser-com...   \n594  \\n        <response>\\n            <browser-com...   \n64   \\n        <response>\\n            <browser-com...   \n\n                                                prompt  \\\n584  ### Instruction:\\n    Use the Task and Input g...   \n268  ### Instruction:\\n    Use the Task and Input g...   \n164  ### Instruction:\\n    Use the Task and Input g...   \n594  ### Instruction:\\n    Use the Task and Input g...   \n64   ### Instruction:\\n    Use the Task and Input g...   \n\n                                              response  \n584  <response><browser-command>search google schol...  \n268  <response><browser-command>search google curre...  \n164  <response><browser-command>search current tab<...  \n594  <response><browser-command>search current tab<...  \n64   <response><browser-command>search google schol...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>input</th>\n      <th>output</th>\n      <th>prompt</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>584</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\n        Below is the raw human voice command...</td>\n      <td>\\n        &lt;response&gt;\\n            &lt;browser-com...</td>\n      <td>### Instruction:\\n    Use the Task and Input g...</td>\n      <td>&lt;response&gt;&lt;browser-command&gt;search google schol...</td>\n    </tr>\n    <tr>\n      <th>268</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\n        Below is the raw human voice command...</td>\n      <td>\\n        &lt;response&gt;\\n            &lt;browser-com...</td>\n      <td>### Instruction:\\n    Use the Task and Input g...</td>\n      <td>&lt;response&gt;&lt;browser-command&gt;search google curre...</td>\n    </tr>\n    <tr>\n      <th>164</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\n        Below is the raw human voice command...</td>\n      <td>\\n        &lt;response&gt;\\n            &lt;browser-com...</td>\n      <td>### Instruction:\\n    Use the Task and Input g...</td>\n      <td>&lt;response&gt;&lt;browser-command&gt;search current tab&lt;...</td>\n    </tr>\n    <tr>\n      <th>594</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\n        Below is the raw human voice command...</td>\n      <td>\\n        &lt;response&gt;\\n            &lt;browser-com...</td>\n      <td>### Instruction:\\n    Use the Task and Input g...</td>\n      <td>&lt;response&gt;&lt;browser-command&gt;search current tab&lt;...</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\n        Below is the raw human voice command...</td>\n      <td>\\n        &lt;response&gt;\\n            &lt;browser-com...</td>\n      <td>### Instruction:\\n    Use the Task and Input g...</td>\n      <td>&lt;response&gt;&lt;browser-command&gt;search google schol...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df[ 'response' ] = responses\n",
    "validation_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:53:20.566083Z",
     "start_time": "2024-01-02T18:53:20.561247Z"
    }
   },
   "id": "636a1d8f00d58761"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Install notebook magic that forces reload a source code\n",
    "%load_ext autoreload"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:53:20.625321Z",
     "start_time": "2024-01-02T18:53:20.562120Z"
    }
   },
   "id": "64208c933aab83d5"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "'/var/model/genie-in-the-box/src'"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir( \"/var/model/genie-in-the-box/src\" )\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:53:20.625852Z",
     "start_time": "2024-01-02T18:53:20.588787Z"
    }
   },
   "id": "2933dc0ac5dcd1df"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands file for [search new tab] exists: True\n",
      "Commands file for [search current tab] exists: True\n",
      "Commands file for [search google new tab] exists: True\n",
      "Commands file for [search google current tab] exists: True\n",
      "Commands file for [search google scholar new tab] exists: True\n",
      "Commands file for [search google scholar current tab] exists: True\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation Stats\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "          Contains response 100.0%\n",
      "   Contains browser command 100.0%\n",
      "              Contains args 100.0%\n",
      "          Response is exact 98.0%\n",
      "Response has correct values 98.0%\n",
      " Browser command is correct 99.0%\n",
      "            Args is correct 99.0%\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from ephemera.prompts.xml_fine_tuning_prompt_generator import XmlFineTuningPromptGenerator\n",
    "\n",
    "# # Advice from phind when auto reload fails: https://www.phind.com/search?cache=l7c51x1v3tids6l43mgkkm9j\n",
    "# os.chdir( \"/var/model/genie-in-the-box/src\" )\n",
    "# %run \"ephemera/prompts/xml_fine_tuning_prompt_generator.py\"  \n",
    "\n",
    "xml_ftp_generator = XmlFineTuningPromptGenerator( path_prefix=\"/var/model/genie-in-the-box\" )\n",
    "\n",
    "validation_df = xml_ftp_generator.validate_prompts_and_responses( validation_df )\n",
    "\n",
    "xml_ftp_generator.print_validation_stats( validation_df )\n",
    "\n",
    "#  Results from a 1000 sample run:\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation Stats\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#    Contains browser command 100.0%\n",
    "#               Contains args 100.0%\n",
    "#           Response is exact 99.3%\n",
    "# Response has correct values 99.3%\n",
    "#  Browser command is correct 99.6%\n",
    "#             Args is correct 99.7%"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:53:20.800403Z",
     "start_time": "2024-01-02T18:53:20.594559Z"
    }
   },
   "id": "1b459e918a397994"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b9ec021e401e8645"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
