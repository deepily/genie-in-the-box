{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import json\n",
    "import wandb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:09:48.595468Z",
     "start_time": "2023-11-17T16:09:48.591570Z"
    }
   },
   "id": "c213bdd2418b70f4"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 39\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n\u001B[1;32m     37\u001B[0m product \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCorelogic Smooth Mouse, belonging to category: Optical Mouse\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 39\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m generate_text( tokenizer, base_model, product )\u001B[38;5;241m.\u001B[39msplit( \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m ): \u001B[38;5;28mprint\u001B[39m( line )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_text( foo_tokenizer, model, product, max_new_tokens=128 ):\n",
    "    \n",
    "    instruction = f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
    "\n",
    "    ### Task:\n",
    "    Create a detailed description for the following product\n",
    "\n",
    "    ### Input:\n",
    "    {product}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    device = \"cuda:0\"\n",
    "    inputs = foo_tokenizer( instruction, return_tensors=\"pt\" ).to( device )\n",
    "    \n",
    "    generation_output = model.generate(\n",
    "        input_ids=inputs[ \"input_ids\" ],\n",
    "        attention_mask=inputs[ \"attention_mask\" ],\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "        \n",
    "    print( \"generation_output[ 0 ]:\", generation_output[ 0 ], end=\"\\n\\n\" )\n",
    "    print( \"generation_output[ 0 ].shape:\", generation_output[ 0 ].shape, end=\"\\n\\n\" )\n",
    "    \n",
    "    raw_output = foo_tokenizer.decode( generation_output[ 0 ] )\n",
    "    \n",
    "    print( \"raw_output:\", raw_output, end=\"\\n\\n\" )\n",
    "    print(  \"len( raw_output ):\", len( raw_output ), end=\"\\n\\n\")\n",
    "    \n",
    "    response   = raw_output.split( \"### Response:\" )[ 1 ]\n",
    "    \n",
    "    return response\n",
    "\n",
    "product = \"Corelogic Smooth Mouse, belonging to category: Optical Mouse\"\n",
    "\n",
    "for line in generate_text( tokenizer, base_model, product ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:09:50.173041Z",
     "start_time": "2023-11-17T16:09:50.150487Z"
    }
   },
   "id": "d00502e6da1117f6"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "os.chdir( \"/var/model\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:09:51.420475Z",
     "start_time": "2023-11-17T16:09:51.412762Z"
    }
   },
   "id": "24974f39bed76473"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14G\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Nov 17 15:51 .\r\n",
      "drwxr-xr-x  1 root root 4.0K Nov 17 15:36 ..\r\n",
      "drwxr-xr-x  2 root root 4.0K Nov 11 03:01 .ipynb_checkpoints\r\n",
      "-rw-rw-r--  1 1001 1001 6.9K Nov  5 17:36 LICENSE\r\n",
      "-rw-rw-r--  1 1001 1001 6.1K Nov  5 17:36 README.md\r\n",
      "-rw-rw-r--  1 1001 1001 4.7K Nov  5 17:36 USE_POLICY.md\r\n",
      "-rw-r--r--  1 root root 9.3K Nov 11 02:12 code-llama-instruct-7b-peft.ipynb\r\n",
      "-rw-r--r--  1 1001 1001  97K Nov 11 03:08 code-llama-instruct-7b.ipynb\r\n",
      "-rw-rw-r--  1 1001 1001  646 Nov  5 17:36 config.json\r\n",
      "-rw-rw-r--  1 1001 1001  116 Nov  5 17:36 generation_config.json\r\n",
      "drwxr-xr-x  2 root root 4.0K Nov 17 15:47 merged\r\n",
      "-rw-rw-r--  1 1001 1001 9.3G Nov  5 17:42 model-00001-of-00002.safetensors\r\n",
      "-rw-rw-r--  1 1001 1001 3.3G Nov  5 17:40 model-00002-of-00002.safetensors\r\n",
      "-rw-rw-r--  1 1001 1001  25K Nov  5 17:36 model.safetensors.index.json\r\n",
      "-rw-rw-r--  1 1001 1001  24K Nov  5 17:36 pytorch_model.bin.index.json\r\n",
      "-rw-rw-r--  1 1001 1001  411 Nov  5 17:36 special_tokens_map.json\r\n",
      "-rw-r--r--  1 1001 1001  27K Nov 10 21:15 text-generation.ipynb\r\n",
      "-rw-------  1 1001 1001 1.1G Nov 10 18:44 tmpdknh986h\r\n",
      "-rw-rw-r--  1 1001 1001 1.8M Nov  5 17:36 tokenizer.json\r\n",
      "-rw-rw-r--  1 1001 1001 489K Nov  5 17:36 tokenizer.model\r\n",
      "-rw-rw-r--  1 1001 1001  749 Nov  5 17:36 tokenizer_config.json\r\n",
      "drwxr-xr-x  2 root root 4.0K Nov 17 15:51 training-results\r\n",
      "-rw-r--r--  1 1001 1001    1 Nov 10 18:43 version.txt\r\n",
      "drwxr-xr-x 11 root root 4.0K Nov 17 16:01 wandb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alh"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:09:53.165024Z",
     "start_time": "2023-11-17T16:09:53.038508Z"
    }
   },
   "id": "630b82887ec1f8c7"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:09:55.253165Z",
     "start_time": "2023-11-17T16:09:55.246628Z"
    }
   },
   "id": "d49e208217c2ea36"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=codellama-7b-instruct-hf-peft-fine-tuning\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=codellama-7b-instruct-hf-peft-fine-tuning"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:09:57.446223Z",
     "start_time": "2023-11-17T16:09:57.439020Z"
    }
   },
   "id": "d03f99f12eb04c9e"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "dataset_name = \"iamtarun/python_code_instructions_18k_alpaca\"\n",
    "split = \"train[:10%]\"\n",
    "finetunes_model_name = \"output/codellama-7b-finetuned-int4-python-18k-alpaca\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:09:58.340797Z",
     "start_time": "2023-11-17T16:09:58.338311Z"
    }
   },
   "id": "90fba6c4ff2875f3"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset( dataset_name, split=split )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:09:59.999257Z",
     "start_time": "2023-11-17T16:09:59.037328Z"
    }
   },
   "id": "ccedc3adeb45dcb8"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def prompt_instruction_format( sample ):\n",
    "    \n",
    "  return f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
    "\n",
    "    ### Task:\n",
    "    {sample['instruction']}\n",
    "\n",
    "    ### Input:\n",
    "    {sample['input']}\n",
    "\n",
    "    ### Response:\n",
    "    {sample['output']}\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:10:02.864735Z",
     "start_time": "2023-11-17T16:10:02.859941Z"
    }
   },
   "id": "58e3fd8b1b81ce1d"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e5cf184a3f341b5aa4c6de6c2a45291"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "print( bnb_config )\n",
    "tokenizer              = AutoTokenizer.from_pretrained( \".\" )\n",
    "tokenizer.pad_token    = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# ¡OJO! Why are we turning off the cash here? \n",
    "# We're not! It makes a huge performance difference: 21 vs 14 tokens per second!\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \".\", quantization_config=bnb_config, device_map=\"auto\", low_cpu_mem_usage=True, use_cache=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:10:09.458753Z",
     "start_time": "2023-11-17T16:10:04.672185Z"
    }
   },
   "id": "b6e2cfb0fbdd120"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_output[ 0 ]: tensor([    1,   835,  2799,  4080, 29901,    13,  1678,  4803,   278,  9330,\n",
      "         2400,   322,   278, 10567,  2183,   304,  2436,   278, 13291, 29892,\n",
      "          607,   338,  1824, 29885,  2454, 15278,   393,   508,  4505,   278,\n",
      "         1494,  9330, 29901,    13,    13,  1678,   835,  9330, 29901,    13,\n",
      "         1678,  6204,   263, 13173,  6139,   363,   278,  1494,  3234,    13,\n",
      "           13,  1678,   835, 10567, 29901,    13,  1678,  2994,   295,   468,\n",
      "          293,  4116,  6983, 25992, 29892, 23329,   304,  7663, 29901, 20693,\n",
      "          936, 25992,    13,    13,  1678,   835, 13291, 29901,    13,   268,\n",
      "        29896, 29889, 10969,  4408, 29901,  2994,   295,   468,   293,  4116,\n",
      "         6983, 25992,    13,   268, 29906, 29889, 10969, 12953, 29901,   450,\n",
      "         2994,   295,   468,   293,  4116,  6983, 25992,   338,   263,  1880,\n",
      "        29899, 29567, 27070,  9495,  8688,   304,  3867,   263, 10597,   322,\n",
      "        18378,  7271,   363,  4160, 29889,   739,   338,   263,  1224, 24285,\n",
      "         9495,   393,   508,   367,  1304,   363,  5164,  8324, 29892,  3704,\n",
      "          330, 11500, 29892,  4863, 16278, 29892,   322,  1856,  3347,  2976,\n",
      "        29889,    13,   268, 29941, 29889, 10969, 17943, 29901, 20693,   936,\n",
      "        25992,    13,   268, 29946, 29889, 10969,  5169,  3698, 29901,    13,\n",
      "         4706,   334,  4116,  6983,   322, 18378, 10298,    13,  4706,   334,\n",
      "         5057, 29899, 29567, 27070, 23530,    13,  4706,   334,  2087,  5143,\n",
      "          519,   360,  2227,    13,  4706,   334,  3831,   271,  1821,   411,\n",
      "         3852,   322,  4326,    13,  4706,   334,  7073,   519],\n",
      "       device='cuda:0')\n",
      "\n",
      "generation_output[ 0 ].shape: torch.Size([208])\n",
      "\n",
      "raw_output: <s> ### Instruction:\n",
      "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Create a detailed description for the following product\n",
      "\n",
      "    ### Input:\n",
      "    Corelogic Smooth Mouse, belonging to category: Optical Mouse\n",
      "\n",
      "    ### Response:\n",
      "    1. Product Name: Corelogic Smooth Mouse\n",
      "    2. Product Description: The Corelogic Smooth Mouse is a high-quality optical mouse designed to provide a smooth and precise experience for users. It is a versatile mouse that can be used for various applications, including gaming, video editing, and web browsing.\n",
      "    3. Product Category: Optical Mouse\n",
      "    4. Product Features:\n",
      "        * Smooth and precise movement\n",
      "        * High-quality optical sensor\n",
      "        * Adjustable DPI\n",
      "        * Compatible with Windows and Mac\n",
      "        * Durable\n",
      "\n",
      "len( raw_output ): 867\n",
      "\n",
      "\n",
      "    1. Product Name: Corelogic Smooth Mouse\n",
      "    2. Product Description: The Corelogic Smooth Mouse is a high-quality optical mouse designed to provide a smooth and precise experience for users. It is a versatile mouse that can be used for various applications, including gaming, video editing, and web browsing.\n",
      "    3. Product Category: Optical Mouse\n",
      "    4. Product Features:\n",
      "        * Smooth and precise movement\n",
      "        * High-quality optical sensor\n",
      "        * Adjustable DPI\n",
      "        * Compatible with Windows and Mac\n",
      "        * Durable\n"
     ]
    }
   ],
   "source": [
    "for line in generate_text( tokenizer, base_model, product ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:10:18.112949Z",
     "start_time": "2023-11-17T16:10:11.138166Z"
    }
   },
   "id": "dcaaf8f2e4c38178"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del model\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-16T17:29:37.686678Z"
    }
   },
   "id": "a6e40707f4760dd2"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "{'model.embed_tokens': 0,\n 'model.layers.0': 0,\n 'model.layers.1': 0,\n 'model.layers.2': 0,\n 'model.layers.3': 0,\n 'model.layers.4': 0,\n 'model.layers.5': 0,\n 'model.layers.6': 0,\n 'model.layers.7': 0,\n 'model.layers.8': 0,\n 'model.layers.9': 0,\n 'model.layers.10': 0,\n 'model.layers.11': 0,\n 'model.layers.12': 0,\n 'model.layers.13': 1,\n 'model.layers.14': 1,\n 'model.layers.15': 1,\n 'model.layers.16': 1,\n 'model.layers.17': 1,\n 'model.layers.18': 1,\n 'model.layers.19': 1,\n 'model.layers.20': 1,\n 'model.layers.21': 1,\n 'model.layers.22': 1,\n 'model.layers.23': 1,\n 'model.layers.24': 1,\n 'model.layers.25': 1,\n 'model.layers.26': 1,\n 'model.layers.27': 1,\n 'model.layers.28': 1,\n 'model.layers.29': 1,\n 'model.layers.30': 1,\n 'model.layers.31': 1,\n 'model.norm': 1,\n 'lm_head': 1}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.hf_device_map"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T17:30:05.213834Z",
     "start_time": "2023-11-16T17:30:05.205993Z"
    }
   },
   "id": "fbb05c78a2bde3e4"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# for name, param in base_model.named_parameters():\n",
    "#     print(f\"Parameter {name} is on device {param.device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T20:01:04.820757Z",
     "start_time": "2023-11-15T20:01:04.764619Z"
    }
   },
   "id": "77a87b656bdf19ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up training arguments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "528b5d0d39d9b5df"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_config, PeftModel, PeftConfig, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    # When target_modules was disabled, it was causing detention layers to be assigned to the CPU, throwing this runtime error:\n",
    "    # RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! \n",
    "    # (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n",
    "    target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\" ], \n",
    "    lora_dropout=0.10, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:11:25.809256Z",
     "start_time": "2023-11-17T16:11:25.795325Z"
    }
   },
   "id": "393f4bbf9c6c3ca4"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir=\"./training-results\", # Output directory where the model predictions and checkpoints will be stored\n",
    "    num_train_epochs=3, # Number of training epochs\n",
    "    per_device_train_batch_size=4, # Batch size per GPU for training\n",
    "    gradient_accumulation_steps=2,  # Number of update steps to accumulate the gradients for\n",
    "    gradient_checkpointing=True,# Enable gradient checkpointing\n",
    "    optim=\"paged_adamw_32bit\", # Optimizer to use\n",
    "    #save_steps=save_steps,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    # fp16=True,\n",
    "    bf16=False,\n",
    "    # tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    #max_steps=max_steps,\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    disable_tqdm=True,\n",
    "    report_to=\"wandb\",\n",
    "    seed=42\n",
    ")\n",
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=prompt_instruction_format,\n",
    "    args=trainingArgs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:12:47.103112Z",
     "start_time": "2023-11-17T16:12:46.578329Z"
    }
   },
   "id": "e0ad2d859cefb9c4"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'print_trainable_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mbase_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprint_trainable_parameters\u001B[49m()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1695\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1693\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1694\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1695\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'LlamaForCausalLM' object has no attribute 'print_trainable_parameters'"
     ]
    }
   ],
   "source": [
    "base_model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:12:48.813820Z",
     "start_time": "2023-11-17T16:12:48.718097Z"
    }
   },
   "id": "e5d11100b64024ed"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter model.embed_tokens.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.0.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.0.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.0.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.1.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.1.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.1.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.2.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.2.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.2.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.3.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.3.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.3.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.4.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.4.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.4.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.5.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.5.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.5.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.6.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.6.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.6.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.7.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.7.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.7.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.8.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.8.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.8.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.9.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.9.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.9.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.10.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.10.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.10.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.11.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.11.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.11.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.q_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.q_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.q_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.k_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.k_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.k_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.v_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.v_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.v_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.o_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.o_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.self_attn.o_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.gate_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.gate_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.gate_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.up_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.up_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.up_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.down_proj.lora_A.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.down_proj.lora_B.default.weight is on cuda:0\n",
      "Parameter model.layers.12.mlp.down_proj.base_layer.weight is on cuda:0\n",
      "Parameter model.layers.12.input_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.12.post_attention_layernorm.weight is on cuda:0\n",
      "Parameter model.layers.13.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.13.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.13.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.13.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.13.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.14.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.14.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.14.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.14.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.15.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.15.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.15.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.15.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.16.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.16.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.16.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.16.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.17.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.17.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.17.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.17.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.18.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.18.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.18.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.18.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.19.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.19.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.19.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.19.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.20.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.20.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.20.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.20.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.21.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.21.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.21.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.22.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.22.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.22.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.23.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.23.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.23.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.24.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.24.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.24.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.25.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.25.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.25.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.26.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.26.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.26.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.27.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.27.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.27.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.28.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.28.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.28.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.29.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.29.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.29.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.30.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.30.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.30.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.q_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.q_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.q_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.k_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.k_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.k_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.v_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.v_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.v_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.o_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.o_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.self_attn.o_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.gate_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.gate_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.gate_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.up_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.up_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.up_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.down_proj.lora_A.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.down_proj.lora_B.default.weight is on cuda:1\n",
      "Parameter model.layers.31.mlp.down_proj.base_layer.weight is on cuda:1\n",
      "Parameter model.layers.31.input_layernorm.weight is on cuda:1\n",
      "Parameter model.layers.31.post_attention_layernorm.weight is on cuda:1\n",
      "Parameter model.norm.weight is on cuda:1\n",
      "Parameter lm_head.weight is on cuda:1\n"
     ]
    }
   ],
   "source": [
    "for name, param in base_model.named_parameters():\n",
    "    print(f\"Parameter {name} is on {param.device}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:12:50.464711Z",
     "start_time": "2023-11-17T16:12:50.446123Z"
    }
   },
   "id": "1b1f7733e1c1786d"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.0"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/var/model/wandb/run-20231117_161258-1f92denm</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning/runs/1f92denm' target=\"_blank\">lunar-field-9</a></strong> to <a href='https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning/runs/1f92denm' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning/runs/1f92denm</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CodeLlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7425, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.02}\n",
      "{'loss': 0.6891, 'learning_rate': 9.523809523809524e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6339, 'learning_rate': 0.00014285714285714287, 'epoch': 0.06}\n",
      "{'loss': 0.6216, 'learning_rate': 0.00019047619047619048, 'epoch': 0.09}\n",
      "{'loss': 0.5063, 'learning_rate': 0.00019998282416292055, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5331, 'learning_rate': 0.00019991305743680013, 'epoch': 1.01}\n",
      "{'loss': 0.5162, 'learning_rate': 0.00019978966374934254, 'epoch': 1.03}\n",
      "{'loss': 0.5119, 'learning_rate': 0.00019961270933041477, 'epoch': 1.05}\n",
      "{'loss': 0.4702, 'learning_rate': 0.0001993822891578708, 'epoch': 1.07}\n",
      "{'loss': 0.5098, 'learning_rate': 0.00019909852690657359, 'epoch': 1.09}\n",
      "{'loss': 0.4685, 'learning_rate': 0.00019876157488201424, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.485, 'learning_rate': 0.0001983716139385641, 'epoch': 2.01}\n",
      "{'loss': 0.4565, 'learning_rate': 0.00019792885338240374, 'epoch': 2.03}\n",
      "{'loss': 0.4662, 'learning_rate': 0.0001974335308591806, 'epoch': 2.06}\n",
      "{'loss': 0.4473, 'learning_rate': 0.00019688591222645607, 'epoch': 2.08}\n",
      "{'loss': 0.414, 'learning_rate': 0.00019628629141101012, 'epoch': 2.1}\n",
      "{'loss': 0.4145, 'learning_rate': 0.00019563499025107998, 'epoch': 2.12}\n",
      "{'train_runtime': 1330.6939, 'train_samples_per_second': 4.196, 'train_steps_per_second': 0.525, 'train_loss': 0.5255529887535992, 'epoch': 2.12}\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38ce378f44d943fa82e5c893aa535cb7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▁▄▄▄▅▅▅███████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>▁▃▅██████████████</td></tr><tr><td>train/loss</td><td>█▇▆▅▃▄▃▃▂▃▂▃▂▂▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>2.12</td></tr><tr><td>train/global_step</td><td>85</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>0.4145</td></tr><tr><td>train/total_flos</td><td>5.538112860900557e+16</td></tr><tr><td>train/train_loss</td><td>0.52555</td></tr><tr><td>train/train_runtime</td><td>1330.6939</td></tr><tr><td>train/train_samples_per_second</td><td>4.196</td></tr><tr><td>train/train_steps_per_second</td><td>0.525</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">lunar-field-9</strong> at: <a href='https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning/runs/1f92denm' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/codellama-7b-instruct-hf-peft-fine-tuning/runs/1f92denm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20231117_161258-1f92denm/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "#stop reporting to wandb\n",
    "wandb.finish()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()\n",
    "\n",
    "print(\"Model saved\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:35:15.253197Z",
     "start_time": "2023-11-17T16:12:58.127185Z"
    }
   },
   "id": "597fd4a8fa0f143f"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T17:49:55.354783Z",
     "start_time": "2023-11-16T17:49:55.314638Z"
    }
   },
   "id": "4f1d623ab4bc7b2e"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# # Drops 16.4/19.0 GB per GPU down to 3.25 GB per GPU!\n",
    "# del base_model\n",
    "# torch.cuda.empty_cache() \n",
    "# gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T19:46:54.220938Z",
     "start_time": "2023-11-16T19:46:54.220401Z"
    }
   },
   "id": "6db763ceb8f0bce3"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c2485f524884b64af27772915c297f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the trained model from the output directory\n",
    "trained_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    trainingArgs.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\", \n",
    "    load_in_8bit=True\n",
    "    # quantization_config=bnb_config\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T19:49:25.637174Z",
     "start_time": "2023-11-16T19:49:21.346577Z"
    }
   },
   "id": "95fb4290bb76ac52"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_output[ 0 ]: tensor([    1,   835,  2799,  4080, 29901,    13,  1678,  4803,   278,  9330,\n",
      "         2400,   322,   278, 10567,  2183,   304,  2436,   278, 13291, 29892,\n",
      "          607,   338,  1824, 29885,  2454, 15278,   393,   508,  4505,   278,\n",
      "         1494,  9330, 29901,    13,    13,  1678,   835,  9330, 29901,    13,\n",
      "         1678,  6204,   263, 13173,  6139,   363,   278,  1494,  3234,    13,\n",
      "           13,  1678,   835, 10567, 29901,    13,  1678,  2994,   295,   468,\n",
      "          293,  4116,  6983, 25992, 29892, 23329,   304,  7663, 29901, 20693,\n",
      "          936, 25992,    13,    13,  1678,   835, 13291, 29901,    13,   268,\n",
      "           13,  1678,   450,  2994,   295,   468,   293,  4116,  6983, 25992,\n",
      "          338,   263,  1880, 29899, 29567, 27070,  9495,  8688,   363, 18378,\n",
      "          322, 10597, 10298, 29889,   739,  5680,   263, 12844,  1416,   322,\n",
      "        23682,  4917,   293,  2874, 29892,  3907,   372, 25561,   304,   671,\n",
      "          363, 10410, 23704, 29889,   450,  9495,   756,   263, 29871, 29945,\n",
      "        29899,  3092,  2874, 29892, 14372,   363, 18378,  2761,   975,  5164,\n",
      "         8324, 29889,   450,  9495,   884,  5680,   263,  4240, 29899,   262,\n",
      "         6355, 18875, 29892, 14372,   363,  4780, 11322,  1549,  5164,  1856,\n",
      "         6515, 29889,   450,  2994,   295,   468,   293,  4116,  6983, 25992,\n",
      "          338,   263,  2107,  2984,   363,  5019,  3063,   363,   263, 23279,\n",
      "          322,  8543,  9495,   363,  1009,  6601, 29889,    13,   268,     2],\n",
      "       device='cuda:0')\n",
      "\n",
      "generation_output[ 0 ].shape: torch.Size([190])\n",
      "\n",
      "raw_output: <s> ### Instruction:\n",
      "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Create a detailed description for the following product\n",
      "\n",
      "    ### Input:\n",
      "    Corelogic Smooth Mouse, belonging to category: Optical Mouse\n",
      "\n",
      "    ### Response:\n",
      "    \n",
      "    The Corelogic Smooth Mouse is a high-quality optical mouse designed for precise and smooth movement. It features a sleek and ergonomic design, making it comfortable to use for extended periods. The mouse has a 5-button design, allowing for precise control over various applications. The mouse also features a built-in scroll wheel, allowing for easy navigation through various web pages. The Corelogic Smooth Mouse is a great option for anyone looking for a reliable and efficient mouse for their computer.\n",
      "    </s>\n",
      "\n",
      "len( raw_output ): 855\n",
      "\n",
      "\n",
      "    \n",
      "    The Corelogic Smooth Mouse is a high-quality optical mouse designed for precise and smooth movement. It features a sleek and ergonomic design, making it comfortable to use for extended periods. The mouse has a 5-button design, allowing for precise control over various applications. The mouse also features a built-in scroll wheel, allowing for easy navigation through various web pages. The Corelogic Smooth Mouse is a great option for anyone looking for a reliable and efficient mouse for their computer.\n",
      "    </s>\n"
     ]
    }
   ],
   "source": [
    "product = \"Corelogic Smooth Mouse, belonging to category: Optical Mouse\"\n",
    "\n",
    "for line in generate_text( tokenizer, trained_model, product ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T19:59:57.604983Z",
     "start_time": "2023-11-16T19:59:44.869244Z"
    }
   },
   "id": "609da8dae9893b8b"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:67: UserWarning: Merge lora module to 8-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "('merged/tokenizer_config.json',\n 'merged/special_tokens_map.json',\n 'merged/tokenizer.model',\n 'merged/added_tokens.json',\n 'merged/tokenizer.json')"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge LoRA with the base model and save the merged model\n",
    "lora_merged_model = trained_model.merge_and_unload()\n",
    "lora_merged_model.save_pretrained( \"merged\", safe_serialization=False )\n",
    "tokenizer.save_pretrained( \"merged\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T19:52:39.414484Z",
     "start_time": "2023-11-16T19:52:00.934137Z"
    }
   },
   "id": "ba3dafc8154b872a"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_output[ 0 ]: tensor([    1,   835,  2799,  4080, 29901,    13,  1678,  4803,   278,  9330,\n",
      "         2400,   322,   278, 10567,  2183,   304,  2436,   278, 13291, 29892,\n",
      "          607,   338,  1824, 29885,  2454, 15278,   393,   508,  4505,   278,\n",
      "         1494,  9330, 29901,    13,    13,  1678,   835,  9330, 29901,    13,\n",
      "         1678,  6204,   263, 13173,  6139,   363,   278,  1494,  3234,    13,\n",
      "           13,  1678,   835, 10567, 29901,    13,  1678,  2994,   295,   468,\n",
      "          293,  4116,  6983, 25992, 29892, 23329,   304,  7663, 29901, 20693,\n",
      "          936, 25992,    13,    13,  1678,   835, 13291, 29901,    13,   268,\n",
      "           13,  1678,   450,  2994,   295,   468,   293,  4116,  6983, 25992,\n",
      "          338,   263,  1880, 29899, 29567, 27070,  9495,  8688,   363, 18378,\n",
      "          322, 10597, 10298, 29889,   739,  5680,   263, 12844,  1416,   322,\n",
      "        23682,  4917,   293,  2874, 29892,  3907,   372, 25561,   304,   671,\n",
      "          363, 10410, 23704, 29889,   450,  9495,   756,   263, 29871, 29945,\n",
      "        29899,  3092,  2874, 29892, 14372,   363, 18378,  2761,   975,  5164,\n",
      "         8324, 29889,   450,  9495,   884,  5680,   263,  4240, 29899,   262,\n",
      "         6355, 18875, 29892, 14372,   363,  4780, 11322,  1549,  5164,  1856,\n",
      "         6515, 29889,   450,  2994,   295,   468,   293,  4116,  6983, 25992,\n",
      "          338,   263,  2107,  2984,   363,  5019,  3063,   363,   263, 23279,\n",
      "          322,  8543,  9495,   363,  1009,  6601, 29889,    13,   268,     2],\n",
      "       device='cuda:0')\n",
      "\n",
      "generation_output[ 0 ].shape: torch.Size([190])\n",
      "\n",
      "raw_output: <s> ### Instruction:\n",
      "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Create a detailed description for the following product\n",
      "\n",
      "    ### Input:\n",
      "    Corelogic Smooth Mouse, belonging to category: Optical Mouse\n",
      "\n",
      "    ### Response:\n",
      "    \n",
      "    The Corelogic Smooth Mouse is a high-quality optical mouse designed for precise and smooth movement. It features a sleek and ergonomic design, making it comfortable to use for extended periods. The mouse has a 5-button design, allowing for precise control over various applications. The mouse also features a built-in scroll wheel, allowing for easy navigation through various web pages. The Corelogic Smooth Mouse is a great option for anyone looking for a reliable and efficient mouse for their computer.\n",
      "    </s>\n",
      "\n",
      "len( raw_output ): 855\n",
      "\n",
      "\n",
      "    \n",
      "    The Corelogic Smooth Mouse is a high-quality optical mouse designed for precise and smooth movement. It features a sleek and ergonomic design, making it comfortable to use for extended periods. The mouse has a 5-button design, allowing for precise control over various applications. The mouse also features a built-in scroll wheel, allowing for easy navigation through various web pages. The Corelogic Smooth Mouse is a great option for anyone looking for a reliable and efficient mouse for their computer.\n",
      "    </s>\n"
     ]
    }
   ],
   "source": [
    "product = \"Corelogic Smooth Mouse, belonging to category: Optical Mouse\"\n",
    "\n",
    "for line in generate_text( tokenizer, lora_merged_model, product ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T20:00:39.640417Z",
     "start_time": "2023-11-16T20:00:26.830907Z"
    }
   },
   "id": "2862d9d06a1f90fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the merged model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e72f5b591fc26838"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "472d6c44f61d4ed7a63f4fb090762f16"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "merged_tokenizer              = AutoTokenizer.from_pretrained( \"merged\" )\n",
    "merged_tokenizer.pad_token    = tokenizer.eos_token\n",
    "merged_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# ¡OJO! Why are we turning off the cash here? \n",
    "# We're not! It makes a huge performance difference: 21 vs 14 tokens per second!\n",
    "merged_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"merged\", quantization_config=bnb_config, device_map=\"auto\", low_cpu_mem_usage=True, use_cache=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T20:00:54.664776Z",
     "start_time": "2023-11-16T20:00:49.205311Z"
    }
   },
   "id": "d561783b1db8d3bc"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_output[ 0 ]: tensor([    1,   835,  2799,  4080, 29901,    13,  1678,  4803,   278,  9330,\n",
      "         2400,   322,   278, 10567,  2183,   304,  2436,   278, 13291, 29892,\n",
      "          607,   338,  1824, 29885,  2454, 15278,   393,   508,  4505,   278,\n",
      "         1494,  9330, 29901,    13,    13,  1678,   835,  9330, 29901,    13,\n",
      "         1678,  6204,   263, 13173,  6139,   363,   278,  1494,  3234,    13,\n",
      "           13,  1678,   835, 10567, 29901,    13,  1678,  2994,   295,   468,\n",
      "          293,  4116,  6983, 25992, 29892, 23329,   304,  7663, 29901, 20693,\n",
      "          936, 25992,    13,    13,  1678,   835, 13291, 29901,    13,   268,\n",
      "           13,  1678,   450,  2994,   295,   468,   293,  4116,  6983, 25992,\n",
      "          338,   263,  1880, 29899, 29567, 27070,  9495,  8688,   363, 18378,\n",
      "          322, 10597, 10298, 29889,   739,  5680,   263, 12844,  1416,   322,\n",
      "        23682,  4917,   293,  2874, 29892,  3907,   372, 25561,   304,   671,\n",
      "          363, 10410, 23704, 29889,   450,  9495,   756,   263, 29871, 29896,\n",
      "        29906, 29899, 22466, 27070, 23530,   393,  8128, 16232,   322, 18378,\n",
      "        10298, 29892, 14372,   363, 10597,   322,   409,   314,  2222, 11322,\n",
      "        29889,   450,  9495,   884,  5680,   263, 29871, 29896, 29906, 29899,\n",
      "        22466, 27070, 23530,   393,  8128, 16232,   322, 18378, 10298, 29892,\n",
      "        14372,   363, 10597,   322,   409,   314,  2222, 11322, 29889,   450,\n",
      "         9495,   756,   263, 29871, 29896, 29906, 29899, 22466, 27070, 23530,\n",
      "          393,  8128, 16232,   322, 18378, 10298, 29892, 14372,   363, 10597,\n",
      "          322,   409,   314,  2222, 11322, 29889,   450,  9495],\n",
      "       device='cuda:0')\n",
      "\n",
      "generation_output[ 0 ].shape: torch.Size([208])\n",
      "\n",
      "raw_output: <s> ### Instruction:\n",
      "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Create a detailed description for the following product\n",
      "\n",
      "    ### Input:\n",
      "    Corelogic Smooth Mouse, belonging to category: Optical Mouse\n",
      "\n",
      "    ### Response:\n",
      "    \n",
      "    The Corelogic Smooth Mouse is a high-quality optical mouse designed for precise and smooth movement. It features a sleek and ergonomic design, making it comfortable to use for extended periods. The mouse has a 12-inch optical sensor that provides accurate and precise movement, allowing for smooth and seamless navigation. The mouse also features a 12-inch optical sensor that provides accurate and precise movement, allowing for smooth and seamless navigation. The mouse has a 12-inch optical sensor that provides accurate and precise movement, allowing for smooth and seamless navigation. The mouse\n",
      "\n",
      "len( raw_output ): 940\n",
      "\n",
      "\n",
      "    \n",
      "    The Corelogic Smooth Mouse is a high-quality optical mouse designed for precise and smooth movement. It features a sleek and ergonomic design, making it comfortable to use for extended periods. The mouse has a 12-inch optical sensor that provides accurate and precise movement, allowing for smooth and seamless navigation. The mouse also features a 12-inch optical sensor that provides accurate and precise movement, allowing for smooth and seamless navigation. The mouse has a 12-inch optical sensor that provides accurate and precise movement, allowing for smooth and seamless navigation. The mouse\n"
     ]
    }
   ],
   "source": [
    "product = \"Corelogic Smooth Mouse, belonging to category: Optical Mouse\"\n",
    "\n",
    "for line in generate_text( merged_tokenizer, merged_model, product ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T20:01:14.533429Z",
     "start_time": "2023-11-16T20:00:59.664925Z"
    }
   },
   "id": "935f7cf5c17a9682"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bbb7dfe7343dd36c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "84cdb91d49cdc96b"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,   835,  2799,  4080, 29901,    13,  1678,  4803,   278,  9330,\n",
      "         2400,   322,   278, 10567,  2183,   304,  2436,   278, 13291, 29892,\n",
      "          607,   338,  1824, 29885,  2454, 15278,   393,   508,  4505,   278,\n",
      "         1494,  9330, 29901,    13,    13,  1678,   835,  9330, 29901,    13,\n",
      "         1678, 14350,   263,   740,   297,  3017,   393,  3408,  1078,   278,\n",
      "          447,   874,   457,  5418,  1546,  1023,  3291, 29889, 29871,    13,\n",
      "           13,  1678,   835, 10567, 29901,    13,  1678,  1094,   366,  5706,\n",
      "          278,  3017,   775,  4312,   304,  1234,   445,  2009, 29892,   306,\n",
      "          864,   366,   304, 29901,    13,    13,   268, 29896, 29897,   894,\n",
      "        29901, 26579,  7535,   565,   366,  2274,   278,  1139,   393,   306,\n",
      "          626,  6721,   366, 29889, 29871, 14617,  8570,   304,   278,  4902,\n",
      "        29991,    13,   268, 29906, 29897, 25086, 29901, 10949,   366,   437,\n",
      "         3099, 29892,  1348,   714, 22526,  1048,   825,   306,   626,  6721,\n",
      "          366,   304,   437, 29892,  3704,   825,   526,   278,  6576,   393,\n",
      "          366,   674,   817,   304,  2125,   304,  4505,   445,  1108, 29889,\n",
      "         1522, 12187,   310,   596,  2714,  1889, 29991,    13,   268, 29941,\n",
      "        29897,  5920, 29901,  3251,   403,   263,  9750,   271,   326,  1051,\n",
      "          310,   775,   393,   366,  1304,   304, 18331,   472,   596,  1234,\n",
      "        29892,   697,  1196,   310,   775,   639,  2944,   373,   278,  1051,\n",
      "        29889,   450,   775,  1818,   367,  4866, 29892,   269,  3903,   627,\n",
      "         1711,  1959, 29892,   322, 15390,   310,  2734,   304, 13285, 29889,\n",
      "          450,  1833,  1196,   310,   596,   775,  1818,   367,   278,  2286,\n",
      "          421,  2929,   918,  1673,   607, 11524,   278,  1234, 29889,  8561,\n",
      "         1854,   393,   738, 21166,   366,  2189,  7087,   278,  1139,  4433,\n",
      "          310,   366,   491,   278,  1404, 29991,    13,   268, 29946, 29897,\n",
      "         7106, 29901, 13969,   373,   278,  1203,  1134,   310,   278,  2286,\n",
      "          421,  2929,   918, 29952,   297,   596,  1833,  1196,   310,   775,\n",
      "        29889,  4803,   697,  1734,   304,  2755,   278,  1203,  1134, 29889,\n",
      "           13,   268,    13,  1678, 19191, 29901,   736,   596,  2933,   408,\n",
      "          263,  4663,  1203,   411,   278,  1494,  4235, 29901,    13,  1678,\n",
      "         1139, 29901,   450,  1139, 29892,  9750,   271,   326,   322,  1728,\n",
      "        21733, 29892,    13,  1678, 13133, 29901,  3575, 13133, 29892,    13,\n",
      "         1678,   775, 29901,   319,  1051,   310,  6031, 29892,  1269,  1347,\n",
      "        15783,   263,  1196,   310,   775,   297,   596,  1650, 29889,    13,\n",
      "         1678,  3639, 29901,  4669,  1134,   310,   278,  2286,   421,  2929,\n",
      "          918,  1673,    13,  1678,  1059, 29901,  3575,  6139,   310,   738,\n",
      "         5626,   470,  4436,   393,   366, 18169,  1550, 15661,   304,  6095,\n",
      "         5589,   445,  2009,    13,    13,  1678,   835, 13291, 29901,    13,\n",
      "          268,    13,  1678,   426,    13,   376, 12470,  1115,   376,  6113,\n",
      "          263,   740,   297,  3017,   393,  3408,  1078,   278,   447,   874,\n",
      "          457,  5418,  1546,  1023,  3291, 19602,    13,   376,   386,  1774,\n",
      "        29879,  1115,   376,  1576,   447,   874,   457,  7063,   338,   263,\n",
      "         7063,   363, 25202,   278,  5418,  1546,  1023,  3291,   373,   278,\n",
      "         7101,   310,   263, 20745, 29889,   739,   338, 15574,  1304,   297,\n",
      "        11322,   322,  1737, 12122,  2472,  6757, 29889,   450,  7063,   338,\n",
      "         2183,   491, 29901,    13,    13, 29881,   353,  1274,   359, 29898,\n",
      "         5223, 29898,  5066, 29896, 29897,   334,  4457, 29898,  5066, 29906,\n",
      "        29897,   718,  6776, 29898,  5066, 29896, 29897,   334,  6776, 29898,\n",
      "         5066, 29906, 29897,   334,  6776, 29898, 12957, 29896,   448, 23123,\n",
      "        29906,   876,    13,    13,  3062,   270,   338,   278,  5418,  1546,\n",
      "          278,  1023,  3291, 29892,  3405, 29896,   322,  3405, 29906,   526,\n",
      "          278, 26271,   310,   278,  1023,  3291, 29892, 23123, 29896,   322,\n",
      "        23123, 29906,   526,   278, 28745,   310,   278,  1023,  3291, 29892,\n",
      "          322,   390,   338,   278, 11855,   310,   278, 20745, 29889,    13,\n",
      "           13,  1576,  7063,   338, 10723,   515,   278,   349,  1541,   351,\n",
      "          487,   273,  9185, 29892,   607,   338,  1304,   304,  8147,   278,\n",
      "         5418,  1546,  1023,  3291,   373,   263, 10694, 29889,   450,  7063,\n",
      "          338,   769,  9120,   304,  2125,   964,  3633,   278, 27686,  1535,\n",
      "          310,   278, 11563, 29892,   607,   338,  4464,   839,   408,   263,\n",
      "        20745, 29889,    13,    13,  1576,   447,   874,   457,  7063,   338,\n",
      "         1304,   297,  1784,  8324, 29892,  3704, 11322, 29892,  1737, 12122,\n",
      "         2472,  6757, 29892,   322, 14826, 29821,   579,   292, 29889,   739,\n",
      "          338,   884,  1304,   297,   278, 13944,   310, 24610,  1546,  3291,\n",
      "          373,   278, 11563, 29915, 29879,  7101, 29889,    13,    13,  1576,\n",
      "          447,   874,   457,  7063,   338,  3342,   408, 29901,    13,    13,\n",
      "        29881,   353,  1274,   359, 29898,  5223, 29898,  5066, 29896, 29897,\n",
      "          334,  4457, 29898,  5066, 29906, 29897,   718,  6776, 29898,  5066,\n",
      "        29896, 29897,   334,  6776, 29898,  5066, 29906, 29897,   334,  6776,\n",
      "        29898, 12957, 29896,   448, 23123, 29906,   876,    13,    13,  3062,\n",
      "          270,   338,   278,  5418,  1546,   278,  1023,  3291, 29892,  3405,\n",
      "        29896,   322,  3405, 29906,   526,   278, 26271,   310,   278,  1023,\n",
      "         3291, 29892, 23123, 29896,   322, 23123, 29906,   526,   278, 28745,\n",
      "          310,   278,  1023,  3291, 29892,   322,   390,   338,   278, 11855,\n",
      "          310,   278, 20745, 29889,    13,    13,  1576,   447,   874,   457,\n",
      "         7063,   338,  1304,   297,  1784,  8324, 29892,  3704, 11322, 29892,\n",
      "         1737, 12122,  2472,  6757, 29892,   322, 14826, 29821,   579,   292,\n",
      "        29889,   739,   338,   884,  1304,   297,   278, 13944,   310, 24610,\n",
      "         1546,  3291,   373,   278, 11563, 29915, 29879,  7101, 29889,    13,\n",
      "           13,  1576,   447,   874,   457,  7063,   338,  3342,   408, 29901,\n",
      "           13,    13, 29881,   353,  1274,   359, 29898,  5223, 29898,  5066,\n",
      "        29896, 29897,   334,  4457, 29898,  5066, 29906, 29897,   718,  6776,\n",
      "        29898,  5066, 29896, 29897,   334,  6776, 29898,  5066, 29906, 29897,\n",
      "          334,  6776, 29898, 12957, 29896,   448, 23123, 29906,   876,    13,\n",
      "           13,  3062,   270,   338,   278,  5418,  1546,   278,  1023,  3291,\n",
      "        29892,  3405, 29896,   322,  3405, 29906,   526,   278, 26271,   310,\n",
      "          278,  1023,  3291, 29892, 23123, 29896,   322, 23123, 29906,   526,\n",
      "          278, 28745,   310,   278,  1023,  3291, 29892,   322,   390,   338,\n",
      "          278, 11855,   310,   278, 20745, 29889,    13,    13,  1576,   447,\n",
      "          874,   457,  7063,   338,  1304,   297,  1784,  8324, 29892,  3704,\n",
      "        11322, 29892,  1737], device='cuda:0')\n",
      "\n",
      "torch.Size([893])\n",
      "\n",
      "3593\n",
      "\n",
      "\n",
      "    \n",
      "    {\n",
      " \"question\": \"Write a function in python that calculates the haversine distance between two points.\",\n",
      " \"thoughts\": \"The haversine formula is a formula for calculating the distance between two points on the surface of a sphere. It is commonly used in navigation and geographic information systems. The formula is given by:\n",
      "\n",
      "d = acos(sin(lat1) * sin(lat2) + cos(lat1) * cos(lat2) * cos(lon1 - lon2))\n",
      "\n",
      "where d is the distance between the two points, lat1 and lat2 are the latitude of the two points, lon1 and lon2 are the longitude of the two points, and R is the radius of the sphere.\n",
      "\n",
      "The formula is derived from the Pythagorean theorem, which is used to calculate the distance between two points on a plane. The formula is then modified to take into account the curvature of the Earth, which is modeled as a sphere.\n",
      "\n",
      "The haversine formula is used in many applications, including navigation, geographic information systems, and weather forecasting. It is also used in the calculation of distances between points on the Earth's surface.\n",
      "\n",
      "The haversine formula is defined as:\n",
      "\n",
      "d = acos(sin(lat1) * sin(lat2) + cos(lat1) * cos(lat2) * cos(lon1 - lon2))\n",
      "\n",
      "where d is the distance between the two points, lat1 and lat2 are the latitude of the two points, lon1 and lon2 are the longitude of the two points, and R is the radius of the sphere.\n",
      "\n",
      "The haversine formula is used in many applications, including navigation, geographic information systems, and weather forecasting. It is also used in the calculation of distances between points on the Earth's surface.\n",
      "\n",
      "The haversine formula is defined as:\n",
      "\n",
      "d = acos(sin(lat1) * sin(lat2) + cos(lat1) * cos(lat2) * cos(lon1 - lon2))\n",
      "\n",
      "where d is the distance between the two points, lat1 and lat2 are the latitude of the two points, lon1 and lon2 are the longitude of the two points, and R is the radius of the sphere.\n",
      "\n",
      "The haversine formula is used in many applications, including navigation, ge\n"
     ]
    }
   ],
   "source": [
    "def generate_code( model, max_new_tokens=512 ):\n",
    "    \n",
    "    instruction = f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write the Response, which is programmatic instruction that can solve the following Task:\n",
    "\n",
    "    ### Task:\n",
    "    Write a function in python that calculates the haversine distance between two points. \n",
    "\n",
    "    ### Input:\n",
    "    As you generate the python code needed to answer this request, I want you to:\n",
    "\n",
    "    1) Question: Ask yourself if you understand the question that I am asking you.  Pay attention to the details!\n",
    "    2) Think: Before you do anything, think out loud about what I am asking you to do, including what are the steps that you will need to take to solve this problem. Be critical of your thought process!\n",
    "    3) Code: Generate a verbatim list of code that you used to arrive at your answer, one line of code per item on the list. The code must be complete, syntactically correct, and capable of running to completion. The last line of your code must be the variable `solution`, which represents the answer. Make sure that any filtering you perform matches the question asked of you by the user!\n",
    "    4) Return: Report on the object type of the variable `solution` in your last line of code. Use one word to represent the object type.\n",
    "    \n",
    "    Format: return your response as a JSON object with the following fields:\n",
    "    question: The question, verbatim and without modification,\n",
    "    thoughts: Your thoughts,\n",
    "    code: A list of strings, each string representing a line of code in your solution.\n",
    "    returns: Object type of the variable `solution`,\n",
    "    error: Your description of any issues or errors that you encountered while attempting to fulfill this request\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    device = \"cuda:0\"\n",
    "    inputs = tokenizer( instruction, return_tensors=\"pt\" ).to( device )\n",
    "    \n",
    "    generation_output = model.generate(\n",
    "        input_ids=inputs[ \"input_ids\" ],\n",
    "        attention_mask=inputs[ \"attention_mask\" ],\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "        \n",
    "    print( generation_output[ 0 ], end=\"\\n\\n\" )\n",
    "    print( generation_output[ 0 ].shape, end=\"\\n\\n\" )\n",
    "    \n",
    "    raw_output = tokenizer.decode( generation_output[ 0 ] )\n",
    "    print( len( raw_output ), end=\"\\n\\n\")\n",
    "    \n",
    "    response   = raw_output.split( \"### Response:\" )[ 1 ]\n",
    "    \n",
    "    return response\n",
    "\n",
    "for line in generate_code( base_model ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T19:52:44.250925Z",
     "start_time": "2023-11-15T19:51:23.877911Z"
    }
   },
   "id": "c64a34f4889cc5f8"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "json_prompt = \"\"\"\n",
    "Write a function in python that calculates the haversine distance between two points. \n",
    "\n",
    "As you generate the python code needed to answer this request, I want you to:\n",
    "\n",
    "1) Question: Ask yourself if you understand the question that I am asking you.  Pay attention to the details!\n",
    "2) Think: Before you do anything, think out loud about what I am asking you to do, including what are the steps that you will need to take to solve this problem. Be critical of your thought process!\n",
    "3) Code: Generate a verbatim list of code that you used to arrive at your answer, one line of code per item on the list. The code must be complete, syntactically correct, and capable of running to completion. The last line of your code must be the variable `solution`, which represents the answer. Make sure that any filtering you perform matches the question asked of you by the user!\n",
    "4) Return: Report on the object type of the variable `solution` in your last line of code. Use one word to represent the object type.\n",
    "\n",
    "Format: return your response as a JSON object with the following fields:\n",
    "question: The question, verbatim and without modification,\n",
    "thoughts: Your thoughts,\n",
    "code: A list of strings, each string representing a line of code in your solution.\n",
    "returns: Object type of the variable `solution`,\n",
    "error: Your description of any issues or errors that you encountered while attempting to fulfill this request\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T02:05:57.281457Z",
     "start_time": "2023-11-15T02:05:57.218227Z"
    }
   },
   "id": "1b34345eaf2aae9d"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "'<s> \\nWrite a function in python that calculates the haversine distance between two points. \\n\\nAs you generate the python code needed to answer this request, I want you to:\\n\\n1) Question: Ask yourself if you understand the question that I am asking you.  Pay attention to the details!\\n2) Think: Before you do anything, think out loud about what I am asking you to do, including what are the steps that you will need to take to solve this problem. Be critical of your thought process!\\n3) Code: Generate a verbatim list of code that you used to arrive at your answer, one line of code per item on the list. The code must be complete, syntactically correct, and capable of running to completion. The last line of your code must be the variable `solution`, which represents the answer. Make sure that any filtering you perform matches the question asked of you by the user!\\n4) Return: Report on the object type of the variable `solution` in your last line of code. Use one word to represent the object type.\\n5) Explain: Briefly and succinctly explain your code in plain English.\\n\\nFormat: return your response as a JSON object with the following fields:\\nquestion: The question, verbatim and without modification,\\nthoughts: Your thoughts,\\ncode: [],\\nreturns: Object type of the variable `solution`,\\nexplanation: A brief explanation of your code,\\nerror: Your description of any issues or errors that you encountered while attempting to fulfill this request.\\n\\n    ### Input:\\n    Not applicable\\n\\n    ### Response:\\n    {\\n  \"question\": \"Write a function in python that calculates the haversine distance between two points.\",\\n  \"thoughts\": \"The haversine formula is used to calculate the distance between two points on the Earth\\'s surface. It takes into account the latitude and longitude of the two points, as well as the radius of the Earth. The formula is given by:\\n\\nd = 2 * arcsin(sqrt(sin^2(delta_lat/2) + cos(lat1) * cos(lat2) * sin^2(delta_lon/2)))\\n\\nwhere d is the distance between the two points, lat1 and lat2 are the latitudes of the two points, lon1 and lon2 are the longitudes of the two points, and delta_lat and delta_lon are the differences between the latitudes and longitudes of the two points.\",\\n  \"code\": [\\n    \"import math\"\\n  ],\\n  \"returns\": \"float\",\\n  \"explanation\": \"The haversine formula is used to calculate the distance between two points on the Earth\\'s surface. It takes into account the latitude and longitude of the two points, as well as the radius of the Earth. The formula is given by:\\n\\nd = 2 * arcsin(sqrt(sin^2(delta_lat/2) + cos(lat1) * cos(lat2) * sin^2(delta_lon/2)))\\n\\nwhere d is the distance between the two points, lat1 and lat2 are the latitudes of the two points, lon1 and lon2 are the longitudes of the two points, and delta_lat and delta_lon are the differences between the latitudes and longitudes of the two points.\",\\n  \"error\": \"\"\\n}\\n    </s>'"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text( json_prompt, max_new_tokens=512 )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T02:10:21.074046Z",
     "start_time": "2023-11-15T02:10:04.583190Z"
    }
   },
   "id": "3898bc41a0bd468f"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "226"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_string = \"\"\"\n",
    "{\\n  \"question\": \"Write a function in python that calculates the haversine distance between two points.\",\\n  \"thoughts\": \"The haversine formula is used to calculate the distance between two points on the Earth\\'s surface. It takes into account the latitude and longitude of the two points, as well as the radius of the Earth. The formula is given by:\\n\\nd = 2 * arcsin(sqrt(sin^2(delta_lat/2) + cos(lat1) * cos(lat2) * sin^2(delta_lon/2)))\\n\\nwhere d is the distance between the two points, lat1 and lat2 are the latitudes of the two points, lon1 and lon2 are the longitudes of the two points, and delta_lat and delta_lon are the differences between the latitudes and longitudes of the two points.\",\\n  \"code\": [\\n    \"import math\"\\n  ],\\n  \"returns\": \"float\",\\n  \"explanation\": \"The haversine formula is used to calculate the distance between two points on the Earth\\'s surface. It takes into account the latitude and longitude of the two points, as well as the radius of the Earth. The formula is given by:\\n\\nd = 2 * arcsin(sqrt(sin^2(delta_lat/2) + cos(lat1) * cos(lat2) * sin^2(delta_lon/2)))\\n\\nwhere d is the distance between the two points, lat1 and lat2 are the latitudes of the two points, lon1 and lon2 are the longitudes of the two points, and delta_lat and delta_lon are the differences between the latitudes and longitudes of the two points.\",\\n  \"error\": \"\"\\n}\"\"\"\n",
    "\n",
    "len( response_string.split( \" \" ) )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T02:24:40.418111Z",
     "start_time": "2023-11-15T02:24:40.387123Z"
    }
   },
   "id": "e50aeffca99b0e2"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "24.375"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "inputs = tokenizer( response_string, return_tensors=\"pt\" ).to( device )\n",
    "len( inputs[ \"input_ids\" ][ 0 ] ) / 16"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T02:28:33.801066Z",
     "start_time": "2023-11-15T02:28:33.761595Z"
    }
   },
   "id": "bdb4583343707df"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T02:27:36.545010Z",
     "start_time": "2023-11-15T02:27:36.516144Z"
    }
   },
   "id": "7054aa26e8410298"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid control character at: line 4 column 243 (char 348)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m response_dict \u001B[38;5;241m=\u001B[39m \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_string\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m response_dict\n",
      "File \u001B[0;32m/usr/lib/python3.10/json/__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    341\u001B[0m     s \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mdecode(detect_encoding(s), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msurrogatepass\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[0;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONDecoder\n",
      "File \u001B[0;32m/usr/lib/python3.10/json/decoder.py:337\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[0;34m(self, s, _w)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, s, _w\u001B[38;5;241m=\u001B[39mWHITESPACE\u001B[38;5;241m.\u001B[39mmatch):\n\u001B[1;32m    333\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001B[39;00m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;124;03m    containing a JSON document).\u001B[39;00m\n\u001B[1;32m    335\u001B[0m \n\u001B[1;32m    336\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 337\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_w\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    338\u001B[0m     end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n\u001B[1;32m    339\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m end \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(s):\n",
      "File \u001B[0;32m/usr/lib/python3.10/json/decoder.py:353\u001B[0m, in \u001B[0;36mJSONDecoder.raw_decode\u001B[0;34m(self, s, idx)\u001B[0m\n\u001B[1;32m    344\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    350\u001B[0m \n\u001B[1;32m    351\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 353\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscan_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpecting value\u001B[39m\u001B[38;5;124m\"\u001B[39m, s, err\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m: Invalid control character at: line 4 column 243 (char 348)"
     ]
    }
   ],
   "source": [
    "response_dict = json.loads( response_string )\n",
    "response_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T02:27:37.477773Z",
     "start_time": "2023-11-15T02:27:37.382009Z"
    }
   },
   "id": "9149784118407356"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "{'instruction': 'Create a function to calculate the sum of a sequence of integers.',\n 'input': '[1, 2, 3, 4, 5]',\n 'output': '# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum',\n 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCreate a function to calculate the sum of a sequence of integers.\\n\\n### Input:\\n[1, 2, 3, 4, 5]\\n\\n### Output:\\n# Python code\\ndef sum_sequence(sequence):\\n  sum = 0\\n  for num in sequence:\\n    sum += num\\n  return sum'}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[ 0 ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T16:27:20.344769Z",
     "start_time": "2023-11-15T16:27:20.315034Z"
    }
   },
   "id": "85ce2bbb8b7154c5"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "foo = \"\"\"\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Write a function in python that calculates the haversine distance between two points\n",
    "\n",
    "### Input:\n",
    "As you generate the python code needed to answer this request, I want you to:\n",
    "\n",
    "1) Question: Ask yourself if you understand the question that I am asking you.  Pay attention to the details!\n",
    "2) Think: Before you do anything, think out loud about what I am asking you to do, including what are the steps that you will need to take to solve this problem. Be critical of your thought process!\n",
    "3) Code: Generate the python code that you need to arrive at your answer. The code must be complete, syntactically correct, and capable of running to completion. The last line of your code must be `return solution`. \n",
    "4) Return: Report on the object type of the variable `solution` in your last line of code. Use one word to represent the object type.\n",
    "\n",
    "Format: return your response as a JSON object with the following fields:\n",
    "\n",
    "question: The question, verbatim and without modification,\n",
    "thoughts: Your thoughts about the question,\n",
    "code: You python code solution.\n",
    "returns: Object type of the return variable `solution`,\n",
    "error: Your description of any issues or errors that you encountered while attempting to fulfill this request\n",
    "\n",
    "### Output:\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T16:35:29.289288Z",
     "start_time": "2023-11-15T16:35:29.220551Z"
    }
   },
   "id": "a1b3124815ecab48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "13bbb4cbf08468a2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
