{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# load the notebook magic that forces source code to be reloaded\n",
    "%load_ext autoreload"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T15:41:21.984084Z",
     "start_time": "2023-12-05T15:41:21.960010Z"
    }
   },
   "id": "b5f09708803cc601"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/genie-in-the-box/src/ephemera/notebooks\n",
      "/var/genie-in-the-box/src\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "# get current working directory\n",
    "print( os.getcwd() )\n",
    "\n",
    "# change current directory\n",
    "os.chdir( \"/var/genie-in-the-box/src/\" )\n",
    "\n",
    "print( os.getcwd() )\n",
    "\n",
    "import lib.utils.util as du\n",
    "import lib.utils.util_stopwatch as sw"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T15:41:24.263481Z",
     "start_time": "2023-12-05T15:41:24.056068Z"
    }
   },
   "id": "fdc361c258f43d39"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from lib.agents.agent import Agent\n",
    "from lib.agents.calendaring_agent import CalendaringAgent\n",
    "import re\n",
    "import datetime\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T21:40:21.965923Z",
     "start_time": "2023-12-05T21:40:21.957360Z"
    }
   },
   "id": "56d3cde401f93e59"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from lib.agents.code_agent import CodeAgent\n",
    "import lib.utils.util_xml as dux\n",
    "from lib.memory.solution_snapshot import SolutionSnapshot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T21:40:28.542252Z",
     "start_time": "2023-12-05T21:40:28.507508Z"
    }
   },
   "id": "7a2c8567c2e8cafc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%autoreload\n",
    "class IterativeDebuggingAgent( CodeAgent ):\n",
    "    \n",
    "    def __init__( self, error_message, path_to_code, debug=False, verbose=False ):\n",
    "        \n",
    "        super().__init__( debug=debug, verbose=verbose )\n",
    "        \n",
    "        self.token_count          = 0\n",
    "        self.prompt_components    = None\n",
    "        self.prompt_response_dict = None\n",
    "        self.available_llms       = self._inialize_available_llms()\n",
    "        self.error_message        = error_message\n",
    "        self.path_to_code         = path_to_code\n",
    "        \n",
    "        self.do_not_serialize     = []\n",
    "        \n",
    "    def _inialize_available_llms( self ):\n",
    "        \n",
    "        prompt_run_llms = [\n",
    "            { \"model\": Agent.PHIND_34B_v2, \"short_name\": \"phind34b\", \"temperature\": 1.0, \"max_new_tokens\": 1024 },\n",
    "            { \"model\": Agent.GPT_3_5, \"short_name\": \"gpt3.5\" },\n",
    "            { \"model\": Agent.GPT_4, \"short_name\": \"gpt4\" }\n",
    "        ]\n",
    "        return prompt_run_llms\n",
    "    \n",
    "    def _initialize_prompt_components( self ):\n",
    "        \n",
    "        step_1 = f\"\"\"\n",
    "        You are a cheerful and helpful assistant, with proven expertise using Python to query pandas dataframes.\n",
    "        \n",
    "        Your job is to debug the code that produced the following error message and generate valid Python code that will correct the bug. You will return your response to each question using XML format.\n",
    "        \n",
    "        {self.error_message}\n",
    "        \n",
    "Source code: \n",
    "{self._get_source_code( self.path_to_code )}\n",
    "        \n",
    "        In order to successfully address the error message above, you must follow my instructions step by step. As you complete each step I will recount your progress on the previous steps and provide you with the next step's instructions.\n",
    "        \n",
    "        Step one) Think: think out loud about what you are being asked, including what are the steps that you will need to take to solve this problem. Be critical of your thought process! \n",
    "        \n",
    "        \"\"\"\n",
    "        # Hint: When joining multiple filtering conditions using and/or in pandas, you must use the single bitwise operators `&` and `|` instead of the boolean operators `and` and `or``.\n",
    "        \n",
    "        xml_formatting_instructions_step_1 = \"\"\"\n",
    "        You must respond to the step one directive using the following XML format:\n",
    "        <response>\n",
    "            <thoughts>Your thoughts</thoughts>\n",
    "        </response>\n",
    "        \n",
    "        Begin!\n",
    "        \"\"\"\n",
    "        step_2 = \"\"\"\n",
    "        In response to the instructions that you received for step one you replied:\n",
    "        \n",
    "        {response}\n",
    "        \n",
    "        Step two) Code: Now that you have thought about how you are going to solve the problem, it's time to generate the Python code that fix the buggy code. The code must be complete, syntactically correct, and capable of running to completion. The last line of your function code must be `return solution`. Remember: You must make the least amount of changes that will fix the bug\n",
    "        \"\"\"\n",
    "        xml_formatting_instructions_step_2 = \"\"\"\n",
    "        You must respond to the step 2 directive using the following XML format:\n",
    "        <response>\n",
    "            <code>\n",
    "                <line>def function_name_here( df, arg1, arg2 ):</line>\n",
    "                <line>    ...</line>\n",
    "                <line>    ...</line>\n",
    "                <line>    return solution</line>\n",
    "            </code>\n",
    "        </response>\n",
    "        \n",
    "        Begin!\n",
    "        \"\"\"\n",
    "               \n",
    "        step_3 = \"\"\"\n",
    "        In response to the instructions that you received for step two, you replied:\n",
    "\n",
    "        {response}\n",
    "\n",
    "        Now that you have generated the code that addresses the bug mentioned above, you will need to perform the following three steps:\n",
    "\n",
    "        Step three) Return: Report on the object type of the variable `solution` returned in your last line of code. Use one word to represent the object type.\n",
    "\n",
    "        Step four) Example: Create a one line example of how to call your code.\n",
    "\n",
    "        Step five) Explain: Explain how your code works, including any assumptions that you have made.\n",
    "        \"\"\"\n",
    "        xml_formatting_instructions_step_3 = \"\"\"\n",
    "        You must respond to the directives in steps three, four and five using the following XML format:\n",
    "\n",
    "        <response>\n",
    "            <returns>Object type of the variable `solution`</returns>\n",
    "            <example>One-line example of how to call your code: solution = function_name_here( arguments )</example>\n",
    "            <explanation>Explanation of how the code works</explanation>\n",
    "        </response>\n",
    "\n",
    "        Begin!\n",
    "        \"\"\"\n",
    "        \n",
    "        step_4 = \"\"\"\n",
    "        In response to the instructions that you received for step three, you replied:\n",
    "\n",
    "        {response}\n",
    "\n",
    "        Congratulations! We're finished ðŸ˜€\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        steps = [ step_1, step_2, step_3, step_4 ]\n",
    "        self.step_len = len( steps )\n",
    "        prompt_components = {\n",
    "            \"steps\"                      : steps,\n",
    "            \"responses\"                  : [ ],\n",
    "            \"response_tag_names\"         : [ [ \"thoughts\" ], [ \"code\" ], [ \"returns\", \"example\", \"explanation\" ] ],\n",
    "            \"running_history\"            : \"\",\n",
    "            \"xml_formatting_instructions\": [\n",
    "                xml_formatting_instructions_step_1, xml_formatting_instructions_step_2, xml_formatting_instructions_step_3\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return prompt_components\n",
    "    \n",
    "    def run_prompts( self ):\n",
    "        \n",
    "        idx = 1\n",
    "        ran_to_completion = False\n",
    "        \n",
    "        for llm in self.available_llms:\n",
    "            \n",
    "            run_descriptor = f\"Run {idx} of {len( self.available_llms )}\"\n",
    "            \n",
    "            if ran_to_completion:\n",
    "                print( f\"Ran to completion? Â¡Yes! Exiting LLM loop...\" )\n",
    "                break\n",
    "                \n",
    "            model_name     = llm[ \"model\" ]\n",
    "            short_name     = llm[ \"short_name\" ]\n",
    "            temperature    = llm[ \"temperature\"    ] if \"temperature\"    in llm else 0.5\n",
    "            max_new_tokens = llm[ \"max_new_tokens\" ] if \"max_new_tokens\" in llm else 1024\n",
    "            \n",
    "            du.print_banner( f\"{run_descriptor}: Executing prompt using model [{model_name}] and short name [{short_name}]...\" )\n",
    "            \n",
    "            prompt_response_dict = self.run_prompt( run_descriptor=run_descriptor, model=model_name, short_name=short_name, temperature=temperature, max_new_tokens=max_new_tokens )\n",
    "            if self.is_code_runnable(): \n",
    "                \n",
    "                code_response_dict = self.run_code()\n",
    "                ran_to_completion  = self.ran_to_completion()\n",
    "                if not ran_to_completion: print( f\"Ran to completion? Â¡FAIL! Moving on to the next LLM...\" )\n",
    "            else:\n",
    "                print( \"Skipping code execution step because the prompt did not produce any code to run.\" )\n",
    "                \n",
    "            idx += 1\n",
    "    \n",
    "    def run_prompt( self, run_descriptor=\"Run 1 of 1\", model=Agent.PHIND_34B_v2, short_name=\"phind34b\", max_new_tokens=1024, temperature=0.5 ):\n",
    "        \n",
    "        self.prompt_components      = self._initialize_prompt_components()\n",
    "        \n",
    "        steps                       = self.prompt_components[ \"steps\" ]\n",
    "        xml_formatting_instructions = self.prompt_components[ \"xml_formatting_instructions\" ]\n",
    "        response_tag_names          = self.prompt_components[ \"response_tag_names\" ]\n",
    "        responses                   = self.prompt_components[ \"responses\" ]\n",
    "        running_history             = self.prompt_components[ \"running_history\" ]\n",
    "        timer                       = sw.Stopwatch( msg=f\"{run_descriptor}: Executing iterative prompt(s) with {len( steps )} steps...\" )\n",
    "        \n",
    "        self.token_count            = 0\n",
    "        prompt_response_dict        = { }\n",
    "        \n",
    "        # Get the current time so that we can track all the steps in this iterative prompt using the same timestamp\n",
    "        now = du.get_current_datetime_raw()\n",
    "        \n",
    "        for step in range( len( steps ) ):\n",
    "            \n",
    "            print( f\"Step [{step + 1}] of [{len( steps )}]\" )\n",
    "            if step == 0:\n",
    "                # the first step doesn't have any previous responses to incorporate into it\n",
    "                running_history = steps[ step ]\n",
    "            else:\n",
    "                # incorporate the previous response into the current step, then append it to the running history\n",
    "                running_history = running_history + steps[ step ].format( response=responses[ step - 1 ] )\n",
    "            \n",
    "            # we're not going to execute the last step, it's been added just to keep the running history current\n",
    "            if step != len( steps ) - 1:\n",
    "                \n",
    "                # response = self._query_llm_phind( running_history + xml_formatting_instructions[ step ], model=model, debug=True )\n",
    "                response = self._query_llm( \n",
    "                    running_history, xml_formatting_instructions[ step ], model=model, max_new_tokens=max_new_tokens, temperature=temperature, debug=True \n",
    "                )\n",
    "                responses.append( response )\n",
    "                \n",
    "                # Incrementally update the contents of the response dictionary according to the results of the XML-esque parsing\n",
    "                prompt_response_dict = self._update_response_dictionary(\n",
    "                    step, response, prompt_response_dict, response_tag_names, debug=False\n",
    "                )\n",
    "            else:\n",
    "                print( \"LAST STEP: Skipping execution. Response from the previous step:\" )\n",
    "                print( responses[ step - 1 ] )\n",
    "            \n",
    "            # Update the prompt component's state before serializing a copy of it\n",
    "            self.prompt_components[ \"running_history\" ] = running_history\n",
    "            self.prompt_response_dict = prompt_response_dict\n",
    "            \n",
    "            self.serialize_to_json( \"code-debugging\", step, self.step_len, now, run_descriptor=run_descriptor, short_name=short_name )\n",
    "            \n",
    "        timer.print( \"Done!\", use_millis=True, prepend_nl=False )\n",
    "        tokens_per_second = self.token_count / (timer.get_delta_ms() / 1000.0 )\n",
    "        print( f\"Tokens per second [{round( tokens_per_second, 1 )}]\" )\n",
    "        \n",
    "        return self.prompt_response_dict\n",
    "    \n",
    "    def _update_response_dictionary( self, step, response, prompt_response_dict, tag_names, debug=True ):\n",
    "        \n",
    "        if debug: print( f\"update_response_dictionary called with step [{step}]...\" )\n",
    "\n",
    "        # Parse response and update response dictionary\n",
    "        xml_tags_for_step_n = tag_names[ step ]\n",
    "\n",
    "        for xml_tag in xml_tags_for_step_n:\n",
    "\n",
    "            if debug: print( f\"Looking for xml_tag [{xml_tag}]\" )\n",
    "\n",
    "            if xml_tag == \"code\":\n",
    "                # the get_code method expects enclosing tags\n",
    "                xml_string = \"<code>\" + dux.get_value_by_xml_tag_name( response, xml_tag ) + \"</code>\"\n",
    "                prompt_response_dict[ xml_tag ] = dux.get_code_list( xml_string, debug=debug )\n",
    "            else:\n",
    "                prompt_response_dict[ xml_tag ] = dux.get_value_by_xml_tag_name( response, xml_tag ).strip()\n",
    "\n",
    "        return prompt_response_dict\n",
    "    \n",
    "    def serialize_to_json( self, topic, current_step, total_steps, now, run_descriptor=\"Run 1 of 1\", short_name=\"phind34b\" ):\n",
    "\n",
    "        # Convert object's state to a dictionary\n",
    "        state_dict = self.__dict__\n",
    "\n",
    "        # Convert object's state to a dictionary, omitting specified fields\n",
    "        state_dict = { key: value for key, value in self.__dict__.items() if key not in self.do_not_serialize }\n",
    "\n",
    "        # Constructing the filename, format: \"topic-run-on-llm-at-year-month-day-hour-minute-step-N-of-M.json\"\n",
    "        run_descriptor = run_descriptor.replace( \" \", \"-\" ).lower()\n",
    "        short_name     = short_name.replace( \" \", \"-\" ).lower()\n",
    "        filename       = f\"{du.get_project_root()}/io/log/{topic}-{run_descriptor}-on-{short_name}-at-{now.year}-{now.month}-{now.day}-{now.hour}-{now.minute}-step-{( current_step + 1 )}-of-{total_steps}.json\"\n",
    "\n",
    "        # Serialize and save to file\n",
    "        with open( filename, 'w' ) as file:\n",
    "            json.dump( state_dict, file, indent=4 )\n",
    "\n",
    "        print( f\"Serialized to {filename}\" )\n",
    "        \n",
    "    def _get_system_message( self ):\n",
    "        \n",
    "        print( \" _get_system_message NOT implemented\" )\n",
    "        \n",
    "    def _get_user_message( self ):\n",
    "        \n",
    "        print( \" _get_user_message NOT implemented\" )\n",
    "        \n",
    "    def format_output( self ):\n",
    "        \n",
    "        print( \" format_output NOT implemented\" )\n",
    "        \n",
    "    def is_code_runnable( self ):\n",
    "        \n",
    "        if self.prompt_response_dict is not None and len( self.prompt_response_dict[ \"code\" ] ) > 0:\n",
    "            return True\n",
    "        else:\n",
    "            print( \"No code to run: self.response_dict[ 'code' ] = [ ]\" )\n",
    "            return False\n",
    "        \n",
    "    def is_prompt_executable( self ):\n",
    "        \n",
    "        return self.prompt_components is not None\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    error_message = \"\"\"\n",
    "ERROR executing code: \n",
    "\n",
    "File \"/Users/rruiz/Projects/projects-sshfs/genie-in-the-box/io/code.py\", line 14\n",
    "    mask = (df['start_date'] <= today) && (df['end_date'] >= today)\n",
    "                                        ^\n",
    "SyntaxError: invalid syntax\"\"\"\n",
    "    \n",
    "    debugging_agent = IterativeDebuggingAgent( error_message, du.get_project_root() + \"/io/code.py\", debug=True, verbose=False )\n",
    "    \n",
    "    debugging_agent.run_prompts()\n",
    "    # print( f\"Is promptable? {debugging_agent.is_prompt_executable()}, is runnable? {debugging_agent.is_code_runnable()}\" )\n",
    "    # prompt_response = debugging_agent.run_prompt()\n",
    "    # print( f\"Is promptable? {debugging_agent.is_prompt_executable()}, is runnable? {debugging_agent.is_code_runnable()}\" )\n",
    "    # \n",
    "    # code_response     = debugging_agent.run_code()\n",
    "    # ran_to_completion = debugging_agent.ran_to_completion()\n",
    "    # \n",
    "    # du.print_banner( f\"Ran to completion? Â¡{ran_to_completion}!\", prepend_nl=False )\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea60fb72058e244d"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "{'return_code': 0, 'output': 'No results returned'}"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T22:01:23.717229Z",
     "start_time": "2023-12-05T22:01:23.713215Z"
    }
   },
   "id": "5dd71baad4690590"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove duplicate lines!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc06e020d25c3f2f"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "['import datetime',\n 'import pytz',\n 'import pandas as pd',\n 'import lib.utils.util as du',\n 'import lib.utils.util_pandas as dup',\n '',\n 'debug = True',\n '',\n \"df = pd.read_csv( du.get_project_root() + '/src/conf/long-term-memory/events.csv' )\",\n 'df = dup.cast_to_datetime( df, debug=debug )',\n '',\n 'import datetime',\n 'import pytz',\n 'import pandas as pd',\n 'import lib.utils.util as du',\n 'import lib.utils.util_pandas as dup',\n 'debug = True',\n \"df = pd.read_csv( du.get_project_root() + '/src/conf/long-term-memory/events.csv' )\",\n 'df = dup.cast_to_datetime( df, debug=debug )',\n 'def get_events_for_today( df ):',\n '    today = pd.Timestamp.now().normalize()',\n \"    mask = (df['start_date'] <= today) & (df['end_date'] >= today)\",\n '    solution = df.loc[mask]',\n '    return solution',\n 'solution = get_events_for_today(df)',\n \"print( solution.to_json( orient='records', lines=True ) )\",\n '',\n 'solution = get_events_for_today(df)',\n \"print( solution.to_json( orient='records', lines=True ) )\"]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_lines = du.get_file_as_list( du.get_project_root() + \"/io/code-duplicate-lines.py\", clean=False )\n",
    "code_lines = [ line.replace( \"\\n\", \"\" ) for line in code_lines ]\n",
    "code_lines"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T22:17:58.090254Z",
     "start_time": "2023-12-05T22:17:58.048136Z"
    }
   },
   "id": "bb1a708763d16de7"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "({'import datetime': 2,\n  'import pytz': 2,\n  'import pandas as pd': 2,\n  'import lib.utils.util as du': 2,\n  'import lib.utils.util_pandas as dup': 2,\n  '': 4,\n  'debug = True': 2,\n  \"df = pd.read_csv( du.get_project_root() + '/src/conf/long-term-memory/events.csv' )\": 2,\n  'df = dup.cast_to_datetime( df, debug=debug )': 2,\n  'solution = get_events_for_today(df)': 2,\n  \"print( solution.to_json( orient='records', lines=True ) )\": 2},\n ['import datetime',\n  'import pytz',\n  'import pandas as pd',\n  'import lib.utils.util as du',\n  'import lib.utils.util_pandas as dup'])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def remove_duplicate_lines( code_lines ):\n",
    "    \n",
    "    # Count the occurrences of each line\n",
    "    line_count = Counter( code_lines )\n",
    "    \n",
    "    # Report the duplicate lines and their frequencies\n",
    "    duplicates_report = { line: count for line, count in line_count.items() if count > 1 }\n",
    "    \n",
    "    # Remove duplicate lines, keeping only the first occurrence\n",
    "    unique_lines = [ ]\n",
    "    seen_lines = set()\n",
    "    for line in code_lines:\n",
    "        if line not in seen_lines:\n",
    "            unique_lines.append( line )\n",
    "            seen_lines.add( line )\n",
    "    \n",
    "    return duplicates_report, unique_lines\n",
    "\n",
    "\n",
    "# Apply the method to the sample code\n",
    "duplicates_report, unique_code_lines = remove_duplicate_lines( code_lines )\n",
    "\n",
    "duplicates_report, unique_code_lines[ :5 ]  # Show first 5 lines of the unique code for brevity\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T22:18:16.145234Z",
     "start_time": "2023-12-05T22:18:16.140113Z"
    }
   },
   "id": "9e4d250deb1e933e"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "['import datetime',\n 'import pytz',\n 'import pandas as pd',\n 'import lib.utils.util as du',\n 'import lib.utils.util_pandas as dup',\n '',\n 'debug = True',\n \"df = pd.read_csv( du.get_project_root() + '/src/conf/long-term-memory/events.csv' )\",\n 'df = dup.cast_to_datetime( df, debug=debug )',\n 'def get_events_for_today( df ):',\n '    today = pd.Timestamp.now().normalize()',\n \"    mask = (df['start_date'] <= today) & (df['end_date'] >= today)\",\n '    solution = df.loc[mask]',\n '    return solution',\n 'solution = get_events_for_today(df)',\n \"print( solution.to_json( orient='records', lines=True ) )\"]"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_code_lines"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T22:18:33.293812Z",
     "start_time": "2023-12-05T22:18:33.290561Z"
    }
   },
   "id": "c97a55fda516e014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e6f00936bd2d6bc0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
