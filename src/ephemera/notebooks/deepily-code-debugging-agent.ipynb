{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# load the notebook magic that forces source code to be reloaded\n",
    "%load_ext autoreload\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T16:16:28.580270Z",
     "start_time": "2023-12-07T16:16:28.535934Z"
    }
   },
   "id": "b5f09708803cc601"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/genie-in-the-box/src/ephemera/notebooks\n",
      "/var/genie-in-the-box/src\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "# get current working directory\n",
    "print( os.getcwd() )\n",
    "\n",
    "# change current directory\n",
    "os.chdir( \"/var/genie-in-the-box/src/\" )\n",
    "\n",
    "print( os.getcwd() )\n",
    "\n",
    "import lib.utils.util as du\n",
    "import lib.utils.util_stopwatch as sw"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T16:16:31.534132Z",
     "start_time": "2023-12-07T16:16:31.302706Z"
    }
   },
   "id": "fdc361c258f43d39"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from lib.agents.agent import XXX_Agent\n",
    "from lib.agents.calendaring_agent import XXX_DataQueryingAgent\n",
    "import re\n",
    "import datetime\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T17:28:33.679948Z",
     "start_time": "2023-12-06T17:28:33.073838Z"
    }
   },
   "id": "56d3cde401f93e59"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from lib.agents.code_agent import CodeAgent\n",
    "import lib.utils.util_xml as dux\n",
    "from lib.memory.solution_snapshot import SolutionSnapshot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T17:28:35.989124Z",
     "start_time": "2023-12-06T17:28:35.985027Z"
    }
   },
   "id": "7a2c8567c2e8cafc"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# %autoreload\n",
    "# class IterativeDebuggingAgent( CodeAgent ):\n",
    "#     \n",
    "#     def __init__( self, error_message, path_to_code, debug=False, verbose=False ):\n",
    "#         \n",
    "#         super().__init__( debug=debug, verbose=verbose )\n",
    "#         \n",
    "#         self.token_count          = 0\n",
    "#         self.prompt_components    = None\n",
    "#         self.prompt_response_dict = None\n",
    "#         self.available_llms       = self._inialize_available_llms()\n",
    "#         self.error_message        = error_message\n",
    "#         self.path_to_code         = path_to_code\n",
    "#         \n",
    "#         self.do_not_serialize     = []\n",
    "#         \n",
    "#     def _inialize_available_llms( self ):\n",
    "#         \n",
    "#         prompt_run_llms = [\n",
    "#             { \"model\": XXX_Agent.PHIND_34B_v2, \"short_name\": \"phind34b\", \"temperature\": 1.0, \"max_new_tokens\": 1024 },\n",
    "#             { \"model\": XXX_Agent.GPT_3_5, \"short_name\": \"gpt3.5\" },\n",
    "#             { \"model\": XXX_Agent.GPT_4, \"short_name\": \"gpt4\" }\n",
    "#         ]\n",
    "#         return prompt_run_llms\n",
    "#     \n",
    "#     def _initialize_prompt_components( self ):\n",
    "#         \n",
    "#         step_1 = f\"\"\"\n",
    "#         You are a cheerful and helpful assistant, with proven expertise using Python to query pandas dataframes.\n",
    "#         \n",
    "#         Your job is to debug the code that produced the following error message and generate valid Python code that will correct the bug. You will return your response to each question using XML format.\n",
    "#         \n",
    "#         {self.error_message}\n",
    "#         \n",
    "# Source code: \n",
    "# {self._get_source_code( self.path_to_code )}\n",
    "#         \n",
    "#         In order to successfully address the error message above, you must follow my instructions step by step. As you complete each step I will recount your progress on the previous steps and provide you with the next step's instructions.\n",
    "#         \n",
    "#         Step one) Think: think out loud about what you are being asked, including what are the steps that you will need to take to solve this problem. Be critical of your thought process! \n",
    "#         \n",
    "#         \"\"\"\n",
    "#         # Hint: When joining multiple filtering conditions using and/or in pandas, you must use the single bitwise operators `&` and `|` instead of the boolean operators `and` and `or``.\n",
    "#         \n",
    "#         xml_formatting_instructions_step_1 = \"\"\"\n",
    "#         You must respond to the step one directive using the following XML format:\n",
    "#         <response>\n",
    "#             <thoughts>Your thoughts</thoughts>\n",
    "#         </response>\n",
    "#         \n",
    "#         Begin!\n",
    "#         \"\"\"\n",
    "#         step_2 = \"\"\"\n",
    "#         In response to the instructions that you received for step one you replied:\n",
    "#         \n",
    "#         {response}\n",
    "#         \n",
    "#         Step two) Code: Now that you have thought about how you are going to solve the problem, it's time to generate the Python code that fix the buggy code. The code must be complete, syntactically correct, and capable of running to completion. The last line of your function code must be `return solution`. Remember: You must make the least amount of changes that will fix the bug\n",
    "#         \"\"\"\n",
    "#         xml_formatting_instructions_step_2 = \"\"\"\n",
    "#         You must respond to the step 2 directive using the following XML format:\n",
    "#         <response>\n",
    "#             <code>\n",
    "#                 <line>def function_name_here( df, arg1, arg2 ):</line>\n",
    "#                 <line>    ...</line>\n",
    "#                 <line>    ...</line>\n",
    "#                 <line>    return solution</line>\n",
    "#             </code>\n",
    "#         </response>\n",
    "#         \n",
    "#         Begin!\n",
    "#         \"\"\"\n",
    "#                \n",
    "#         step_3 = \"\"\"\n",
    "#         In response to the instructions that you received for step two, you replied:\n",
    "# \n",
    "#         {response}\n",
    "# \n",
    "#         Now that you have generated the code that addresses the bug mentioned above, you will need to perform the following three steps:\n",
    "# \n",
    "#         Step three) Return: Report on the object type of the variable `solution` returned in your last line of code. Use one word to represent the object type.\n",
    "# \n",
    "#         Step four) Example: Create a one line example of how to call your code.\n",
    "# \n",
    "#         Step five) Explain: Explain how your code works, including any assumptions that you have made.\n",
    "#         \"\"\"\n",
    "#         xml_formatting_instructions_step_3 = \"\"\"\n",
    "#         You must respond to the directives in steps three, four and five using the following XML format:\n",
    "# \n",
    "#         <response>\n",
    "#             <returns>Object type of the variable `solution`</returns>\n",
    "#             <example>solution = function_name_here( df, arg1, etc )</example>\n",
    "#             <explanation>Explanation of how the code works</explanation>\n",
    "#         </response>\n",
    "# \n",
    "#         Begin!\n",
    "#         \"\"\"\n",
    "#         \n",
    "#         step_4 = \"\"\"\n",
    "#         In response to the instructions that you received for step three, you replied:\n",
    "# \n",
    "#         {response}\n",
    "# \n",
    "#         Congratulations! We're finished ðŸ˜€\n",
    "# \n",
    "#         \"\"\"\n",
    "#         \n",
    "#         steps = [ step_1, step_2, step_3, step_4 ]\n",
    "#         self.step_len = len( steps )\n",
    "#         prompt_components = {\n",
    "#             \"steps\"                      : steps,\n",
    "#             \"responses\"                  : [ ],\n",
    "#             \"response_tag_names\"         : [ [ \"thoughts\" ], [ \"code\" ], [ \"returns\", \"example\", \"explanation\" ] ],\n",
    "#             \"running_history\"            : \"\",\n",
    "#             \"xml_formatting_instructions\": [\n",
    "#                 xml_formatting_instructions_step_1, xml_formatting_instructions_step_2, xml_formatting_instructions_step_3\n",
    "#             ]\n",
    "#         }\n",
    "#         \n",
    "#         return prompt_components\n",
    "#     \n",
    "#     def run_prompts( self ):\n",
    "#         \n",
    "#         idx = 1\n",
    "#         ran_to_completion = False\n",
    "#         \n",
    "#         for llm in self.available_llms:\n",
    "#             \n",
    "#             run_descriptor = f\"Run {idx} of {len( self.available_llms )}\"\n",
    "#             \n",
    "#             if ran_to_completion:\n",
    "#                 print( f\"Ran to completion? Â¡Yes! Exiting LLM loop...\" )\n",
    "#                 break\n",
    "#                 \n",
    "#             model_name     = llm[ \"model\" ]\n",
    "#             short_name     = llm[ \"short_name\" ]\n",
    "#             temperature    = llm[ \"temperature\"    ] if \"temperature\"    in llm else 0.5\n",
    "#             max_new_tokens = llm[ \"max_new_tokens\" ] if \"max_new_tokens\" in llm else 1024\n",
    "#             \n",
    "#             du.print_banner( f\"{run_descriptor}: Executing prompt using model [{model_name}] and short name [{short_name}]...\" )\n",
    "#             \n",
    "#             prompt_response_dict = self.run_prompt( run_descriptor=run_descriptor, model=model_name, short_name=short_name, temperature=temperature, max_new_tokens=max_new_tokens )\n",
    "#             if self.is_code_runnable(): \n",
    "#                 \n",
    "#                 code_response_dict = self.run_code()\n",
    "#                 ran_to_completion  = self.ran_to_completion()\n",
    "#                 if not ran_to_completion: print( f\"Ran to completion? Â¡FAIL! Moving on to the next LLM...\" )\n",
    "#             else:\n",
    "#                 print( \"Skipping code execution step because the prompt did not produce any code to run.\" )\n",
    "#                 \n",
    "#             idx += 1\n",
    "#     \n",
    "#     def run_prompt( self, run_descriptor=\"Run 1 of 1\", model=XXX_Agent.PHIND_34B_v2, short_name=\"phind34b\", max_new_tokens=1024, temperature=0.5 ):\n",
    "#         \n",
    "#         self.prompt_components      = self._initialize_prompt_components()\n",
    "#         \n",
    "#         steps                       = self.prompt_components[ \"steps\" ]\n",
    "#         xml_formatting_instructions = self.prompt_components[ \"xml_formatting_instructions\" ]\n",
    "#         response_tag_names          = self.prompt_components[ \"response_tag_names\" ]\n",
    "#         responses                   = self.prompt_components[ \"responses\" ]\n",
    "#         running_history             = self.prompt_components[ \"running_history\" ]\n",
    "#         timer                       = sw.Stopwatch( msg=f\"{run_descriptor}: Executing iterative prompt(s) with {len( steps )} steps...\" )\n",
    "#         \n",
    "#         self.token_count            = 0\n",
    "#         prompt_response_dict        = { }\n",
    "#         \n",
    "#         # Get the current time so that we can track all the steps in this iterative prompt using the same timestamp\n",
    "#         now = du.get_current_datetime_raw()\n",
    "#         \n",
    "#         for step in range( len( steps ) ):\n",
    "#             \n",
    "#             print( f\"Step [{step + 1}] of [{len( steps )}]\" )\n",
    "#             if step == 0:\n",
    "#                 # the first step doesn't have any previous responses to incorporate into it\n",
    "#                 running_history = steps[ step ]\n",
    "#             else:\n",
    "#                 # incorporate the previous response into the current step, then append it to the running history\n",
    "#                 running_history = running_history + steps[ step ].format( response=responses[ step - 1 ] )\n",
    "#             \n",
    "#             # we're not going to execute the last step, it's been added just to keep the running history current\n",
    "#             if step != len( steps ) - 1:\n",
    "#                 \n",
    "#                 # response = self._query_llm_phind( running_history + xml_formatting_instructions[ step ], model=model, debug=True )\n",
    "#                 response = self._query_llm( \n",
    "#                     running_history, xml_formatting_instructions[ step ], model=model, max_new_tokens=max_new_tokens, temperature=temperature, debug=True \n",
    "#                 )\n",
    "#                 responses.append( response )\n",
    "#                 \n",
    "#                 # Incrementally update the contents of the response dictionary according to the results of the XML-esque parsing\n",
    "#                 prompt_response_dict = self._update_response_dictionary(\n",
    "#                     step, response, prompt_response_dict, response_tag_names, debug=False\n",
    "#                 )\n",
    "#             else:\n",
    "#                 print( \"LAST STEP: Skipping execution. Response from the previous step:\" )\n",
    "#                 print( responses[ step - 1 ] )\n",
    "#             \n",
    "#             # Update the prompt component's state before serializing a copy of it\n",
    "#             self.prompt_components[ \"running_history\" ] = running_history\n",
    "#             self.prompt_response_dict = prompt_response_dict\n",
    "#             \n",
    "#             self.serialize_to_json( \"code-debugging\", step, self.step_len, now, run_descriptor=run_descriptor, short_name=short_name )\n",
    "#             \n",
    "#         timer.print( \"Done!\", use_millis=True, prepend_nl=False )\n",
    "#         tokens_per_second = self.token_count / (timer.get_delta_ms() / 1000.0 )\n",
    "#         print( f\"Tokens per second [{round( tokens_per_second, 1 )}]\" )\n",
    "#         \n",
    "#         return self.prompt_response_dict\n",
    "#     \n",
    "#     def _update_response_dictionary( self, step, response, prompt_response_dict, tag_names, debug=True ):\n",
    "#         \n",
    "#         if debug: print( f\"update_response_dictionary called with step [{step}]...\" )\n",
    "# \n",
    "#         # Parse response and update response dictionary\n",
    "#         xml_tags_for_step_n = tag_names[ step ]\n",
    "# \n",
    "#         for xml_tag in xml_tags_for_step_n:\n",
    "# \n",
    "#             if debug: print( f\"Looking for xml_tag [{xml_tag}]\" )\n",
    "# \n",
    "#             if xml_tag == \"code\":\n",
    "#                 \n",
    "#                 xml_string = dux.get_value_by_xml_tag_name( response, xml_tag, default_value=\"\" )\n",
    "#                 if xml_string == \"\":\n",
    "#                     print( f\"WARNING: No <code> tags found, falling back to the default tick tick tick syntax...\" )\n",
    "#                     xml_string = dux.rescue_code_using_tick_tick_tick_syntax( response )\n",
    "#                     \n",
    "#                 # the get_code method expects enclosing tags\n",
    "#                 xml_string = \"<code>\" + xml_string + \"</code>\"\n",
    "#                 # xml_string = \"<code>\" + dux.get_value_by_xml_tag_name( response, xml_tag ) + \"</code>\"\n",
    "#                 prompt_response_dict[ xml_tag ] = dux.get_code_list( xml_string, debug=debug )\n",
    "#             else:\n",
    "#                 prompt_response_dict[ xml_tag ] = dux.get_value_by_xml_tag_name( response, xml_tag ).strip()\n",
    "# \n",
    "#         return prompt_response_dict\n",
    "#     \n",
    "#     def serialize_to_json( self, topic, current_step, total_steps, now, run_descriptor=\"Run 1 of 1\", short_name=\"phind34b\" ):\n",
    "# \n",
    "#         # Convert object's state to a dictionary\n",
    "#         state_dict = self.__dict__\n",
    "# \n",
    "#         # Convert object's state to a dictionary, omitting specified fields\n",
    "#         state_dict = { key: value for key, value in self.__dict__.items() if key not in self.do_not_serialize }\n",
    "# \n",
    "#         # Constructing the filename, format: \"topic-run-on-llm-at-year-month-day-hour-minute-step-N-of-M.json\"\n",
    "#         run_descriptor = run_descriptor.replace( \" \", \"-\" ).lower()\n",
    "#         short_name     = short_name.replace( \" \", \"-\" ).lower()\n",
    "#         filename       = f\"{du.get_project_root()}/io/log/{topic}-on-{now.year}-{now.month}-{now.day}-at-{now.hour}-{now.minute}-{run_descriptor}-using-llm-{short_name}-step-{( current_step + 1 )}-of-{total_steps}.json\"\n",
    "# \n",
    "#         # Serialize and save to file\n",
    "#         with open( filename, 'w' ) as file:\n",
    "#             json.dump( state_dict, file, indent=4 )\n",
    "# \n",
    "#         print( f\"Serialized to {filename}\" )\n",
    "#         \n",
    "#     def _get_system_message( self ):\n",
    "#         \n",
    "#         print( \" _get_system_message NOT implemented\" )\n",
    "#         \n",
    "#     def _get_user_message( self ):\n",
    "#         \n",
    "#         print( \" _get_user_message NOT implemented\" )\n",
    "#         \n",
    "#     def format_output( self ):\n",
    "#         \n",
    "#         print( \" format_output NOT implemented\" )\n",
    "#         \n",
    "#     def is_code_runnable( self ):\n",
    "#         \n",
    "#         if self.prompt_response_dict is not None and len( self.prompt_response_dict[ \"code\" ] ) > 0:\n",
    "#             return True\n",
    "#         else:\n",
    "#             print( \"No code to run: self.response_dict[ 'code' ] = [ ]\" )\n",
    "#             return False\n",
    "#         \n",
    "#     def is_prompt_executable( self ):\n",
    "#         \n",
    "#         return self.prompt_components is not None\n",
    "#         \n",
    "# if __name__ == \"__main__\":\n",
    "#     \n",
    "#     error_message = \"\"\"\n",
    "# ERROR executing code: \n",
    "# \n",
    "# File \"/Users/rruiz/Projects/projects-sshfs/genie-in-the-box/io/code.py\", line 14\n",
    "#     mask = (df['start_date'] <= today) && (df['end_date'] >= today)\n",
    "#                                         ^\n",
    "# SyntaxError: invalid syntax\"\"\"\n",
    "#     \n",
    "#     debugging_agent = IterativeDebuggingAgent( error_message, du.get_project_root() + \"/io/code.py\", debug=True, verbose=False )\n",
    "#     \n",
    "#     debugging_agent.run_prompts()\n",
    "#     # print( f\"Is promptable? {debugging_agent.is_prompt_executable()}, is runnable? {debugging_agent.is_code_runnable()}\" )\n",
    "#     # prompt_response = debugging_agent.run_prompt()\n",
    "#     # print( f\"Is promptable? {debugging_agent.is_prompt_executable()}, is runnable? {debugging_agent.is_code_runnable()}\" )\n",
    "#     # \n",
    "#     # code_response     = debugging_agent.run_code()\n",
    "#     # ran_to_completion = debugging_agent.ran_to_completion()\n",
    "#     # \n",
    "#     # du.print_banner( f\"Ran to completion? Â¡{ran_to_completion}!\", prepend_nl=False )\n",
    "#     "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T20:30:16.365011Z",
     "start_time": "2023-12-06T20:30:16.355354Z"
    }
   },
   "id": "ea60fb72058e244d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "from lib.agents.iterative_debugging_agent import IterativeDebuggingAgent\n",
    "error_message = \"\"\"\n",
    "ERROR executing code:\n",
    "\n",
    "File \"/Users/rruiz/Projects/projects-sshfs/genie-in-the-box/io/code.py\", line 14\n",
    "    mask = (df['start_date'] <= today) && (df['end_date'] >= today)\n",
    "                                        ^\n",
    "SyntaxError: invalid syntax\"\"\"\n",
    "    \n",
    "debugging_agent = IterativeDebuggingAgent( error_message, du.get_project_root() + \"/io/code.py\", debug=True, verbose=False )\n",
    "\n",
    "debugging_agent.run_prompts()\n",
    "# print( f\"Is promptable? {debugging_agent.is_prompt_executable()}, is runnable? {debugging_agent.is_code_runnable()}\" )\n",
    "# prompt_response = debugging_agent.run_prompt()\n",
    "# print( f\"Is promptable? {debugging_agent.is_prompt_executable()}, is runnable? {debugging_agent.is_code_runnable()}\" )\n",
    "#\n",
    "# code_response     = debugging_agent.run_code()\n",
    "# ran_to_completion = debugging_agent.ran_to_completion()\n",
    "#\n",
    "# du.print_banner( f\"Ran to completion? Â¡{ran_to_completion}!\", prepend_nl=False )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0a7595e2077444c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: [\n",
      "        Please introduce one, and only one, Python or Pandas syntax error into the provided source code snippet below that will cause execution to fail at runtime.\n",
      "        \n",
      "        Source code:\n",
      "001 import datetime\n",
      "002 import pytz\n",
      "003 import datetime\n",
      "004 import pytz\n",
      "005 def get_time():\n",
      "006 def get_time():\n",
      "007     import datetime\n",
      "008     now = datetime.datetime.now()\n",
      "009     tz_name = 'America/New_York'\n",
      "010     tz = pytz.timezone( tz_name )\n",
      "011     tz_date = now.astimezone( tz )\n",
      "012     return tz_date.strftime( '%I:%M %p %Z' )\n",
      "        \n",
      "        The error should prevent the program from running.\n",
      "        \n",
      "        You must only return only one line of source code that you have modified to fail at runtime\n",
      "        ]\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]...\n",
      "\n",
      "        Please introduce one, and only one, Python or Pandas syntax error into the provided source code snippet below that will cause execution to fail at runtime.\n",
      "        \n",
      "        Source code:\n",
      "001 import datetime\n",
      "002 import pytz\n",
      "003 import datetime\n",
      "004 import pytz\n",
      "005 def get_time():\n",
      "006 def get_time():\n",
      "007     import datetime\n",
      "008     now = datetime.datetime.now()\n",
      "009     tz_name = 'America/New_York'\n",
      "010     tz = pytz.timezone( tz_name )\n",
      "011     tz_date = now.astimezone( tz )\n",
      "012     return tz_date.strftime( '%I:%M %p %Z' )\n",
      "        \n",
      "        The error should prevent the program from running.\n",
      "        \n",
      "        You must only return only one line of source code that you have modified to fail at runtime\n",
      "        \n",
      "\n",
      "        Format the response as follows, replacing the placeholders with the actual details:\n",
      "        <response>\n",
      "            <line-number>[line number where bug is introduced]</line-number>\n",
      "            <bug>[one line of modified source code with bug in it]</bug>\n",
      "        </response>\n",
      "        \n",
      "\n",
      "        <response>\n",
      "            <line-number>5</line-number>\n",
      "            <bug>def get_time():</bug>\n",
      "        </response>\n",
      "Asking LLM [TGI/Phind-CodeLlama-34B-v2]... Done! in 1,256 ms\n",
      "\n",
      "Token list length [36]\n",
      "line_number: [4], bug: [def get_time():]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'code': ['001 import datetime',\n  '002 import pytz',\n  '003 import datetime',\n  '004 import pytz',\n  'def get_time():',\n  '006 def get_time():',\n  '007     import datetime',\n  '008     now = datetime.datetime.now()',\n  \"009     tz_name = 'America/New_York'\",\n  '010     tz = pytz.timezone( tz_name )',\n  '011     tz_date = now.astimezone( tz )',\n  \"012     return tz_date.strftime( '%I:%M %p %Z' )\"]}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "import lib.utils.util as du\n",
    "\n",
    "from lib.agents.bug_injector import BugInjector\n",
    "\n",
    "code = [\n",
    "        \"import datetime\",\n",
    "        \"import pytz\",\n",
    "        \"import datetime\",\n",
    "        \"import pytz\",\n",
    "        \"def get_time():\",\n",
    "        \"def get_time():\",\n",
    "        \"    import datetime\",\n",
    "        \"    now = datetime.datetime.now()\",\n",
    "        \"    tz_name = 'America/New_York'\",\n",
    "        \"    tz = pytz.timezone( tz_name )\",\n",
    "        \"    tz_date = now.astimezone( tz )\",\n",
    "        \"    return tz_date.strftime( '%I:%M %p %Z' )\"\n",
    "    ]\n",
    "bug_injector = BugInjector( code, debug=True, verbose=False )\n",
    "bug_injector.run_prompt()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T16:17:03.592342Z",
     "start_time": "2023-12-07T16:17:02.026561Z"
    }
   },
   "id": "e6f00936bd2d6bc0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4ef62df1fc64f854"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
