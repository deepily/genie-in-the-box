{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T17:07:28.423166Z",
     "start_time": "2024-02-08T17:07:28.401730Z"
    }
   },
   "id": "9c05dc51c1556825"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/genie-in-the-box/src\n",
      "/var/model/genie-in-the-box/src\n"
     ]
    }
   ],
   "source": [
    "print( os.getcwd() )\n",
    "# change working directory\n",
    "os.chdir( \"/var/model/genie-in-the-box/src\" )\n",
    "print( os.getcwd() )\n",
    "import lib.utils.util as du\n",
    "import lib.utils.util_xml as dux"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T17:07:47.071504Z",
     "start_time": "2024-02-08T17:07:47.040200Z"
    }
   },
   "id": "6ad17d259f359e1b"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from lib.utils.util_stopwatch import Stopwatch\n",
    "from ephemera.prompts.xml_fine_tuning_prompt_generator import XmlFineTuningPromptGenerator\n",
    "\n",
    "path_prefix = \"/var/model/genie-in-the-box\"\n",
    "xml_ftp_generator = XmlFineTuningPromptGenerator( path_prefix=path_prefix, tgi_url=\"http://172.17.0.4:3000\", debug=False, silent=True, init_prompt_templates=False )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T17:10:17.174879Z",
     "start_time": "2024-02-08T17:10:17.069518Z"
    }
   },
   "id": "984563e3206b751d"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "sql_prompt_template    = du.get_file_as_string( path_prefix + \"/src/conf/prompts/sql-proofreading-template.txt\" )\n",
    "python_prompt_template = du.get_file_as_string( path_prefix + \"/src/conf/prompts/python-proofreading-template.txt\" )\n",
    "# for line in sql_prompt_template.split( \"\\n\" ): print( line ) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T19:30:19.565477Z",
     "start_time": "2024-02-08T19:30:19.548462Z"
    }
   },
   "id": "10d679ba69d718b9"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "\n",
      "Use the Task and Input given below to write a Response that can solve the following Task.\n",
      "\n",
      "### Task:\n",
      "\n",
      "Your job is to discern the intent of two human voice command transcriptions and translate them into one line of syntactically valid python code.\n",
      "The human voice command transcriptions you receive will contain odd formatting, wording, and punctuation mistakes. You must be able to understand the intent of the command and translate it into one line of syntactically valid python code.\n",
      "\n",
      "The first sentence will be a description in plain English of what the human is attempting to accomplish.\n",
      "Second sentence will be an approximation of the python code desired to accomplish the task.\n",
      "You will use the first sentence to help disambiguate any syntactic or spelling errors you encounter in the second sentence.\n",
      "\n",
      "Requirement: You MUST use your linguistic knowledge and intuition to discern the speaker's intent.\n",
      "Requirement: You MUST also use your knowledge of Python to translate the second sentence into well-formed Python code.\n",
      "Requirement: You MUST use the first sentence to help disambiguate any syntactic or spelling errors you encounter in the second sentence.\n",
      "Requirement: You MUST only return one line of python code, and no more.\n",
      "Requirement: You MUST N0T attempt to implement methods or generate any code beyond the one-line asked of you\n",
      "Requirement: The first word of your response MUST be `<response>`\n",
      "\n",
      "### Input:\n",
      "\n",
      "Below is the raw human voice transcription formatted using simple XML:\n",
      "\n",
      "<human>\n",
      "    <voice-command>I want to create a method signature. def increment by N ( self,foo,n )</voice-command>\n",
      "</human>\n",
      "\n",
      "The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
      "\n",
      "<response>\n",
      "    <python>one line only!</python>\n",
      "</response>\n",
      "\n",
      "Take a deep breath and think out loud before you begin to translate the voice transcriptions into Python.\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "context     = \"def increment_by_n( self, foo, n ):\"\n",
    "# description = \"Add n to foo. temp value = input plus n.\"\n",
    "# description = \"I want to add n to foo. temp value = foo plus n.\"\n",
    "description = \"I want to create a method signature. def increment by N ( self,foo,n )\"\n",
    "python_prompt  = python_prompt_template.format( voice_command=description )\n",
    "for line in python_prompt.split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T19:32:38.083606Z",
     "start_time": "2024-02-08T19:32:37.939300Z"
    }
   },
   "id": "178ebee86ecaf459"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking LLM [Phind-CodeLlama-34B-v2]...\n",
      ".........................\n",
      "Asking LLM [Phind-CodeLlama-34B-v2]... Done! in 1,018 ms\n",
      "Tokens per second [24.6]\n",
      "<response><python>def increment(self, foo, n):</python></response>\n"
     ]
    },
    {
     "data": {
      "text/plain": "'def increment(self, foo, n):'"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload\n",
    "response = xml_ftp_generator.query_llm_tgi( python_prompt, model_name=\"Phind-CodeLlama-34B-v2\", max_new_tokens=1024, temperature=0.25, top_k=10, top_p=0.9, silent=False )\n",
    "# print( response )\n",
    "response = dux.strip_all_white_space( response )\n",
    "print( response )\n",
    "python = dux.get_value_by_xml_tag_name( response, \"python\" )\n",
    "python"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T19:32:41.834799Z",
     "start_time": "2024-02-08T19:32:40.775780Z"
    }
   },
   "id": "5a377453bf467f4d"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "\n",
      "Use the Task and Input given below to write a Response that can solve the following Task.\n",
      "\n",
      "### Task:\n",
      "\n",
      "Your job is to discern the intent of two human voice command transcriptions and translate them into a standardized SQL command.\n",
      "The human voice command transcriptions you receive will contain odd formatting, wording, and punctuation mistakes. You must be able to understand the intent of the command and translate it into a standardized SQL command.\n",
      "\n",
      "The first sentence will be a description in plain English of what the human is attempting to accomplish.\n",
      "Second sentence will be an approximation of the SQL commands necessary to accomplish this.\n",
      "You will use the first sentence to help disambiguate any syntactic or spelling errors you encounter in the second sentence.\n",
      "\n",
      "Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
      "Requirement: You MUST use your SQL knowledge to translate the second sentence into a standardized SQL command.\n",
      "Requirement: You MUST use the first sentence to help disambiguate any syntactic or spelling errors you encounter in the second sentence.\n",
      "Requirement: All column and table names MUST be lowercase\n",
      "Requirement: The first word of your response MUST be `<response>`\n",
      "\n",
      "### Input:\n",
      "\n",
      "Below is the raw human voice transcription formatted using simple XML:\n",
      "\n",
      "<human>\n",
      "    <voice-command>I want to select all the countries whose names contain the letter 'x'. select name from country where name like 'percent X percent '</voice-command>\n",
      "</human>\n",
      "\n",
      "The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
      "\n",
      "<response>\n",
      "    <thoughts></thoughts>\n",
      "    <sql></sql>\n",
      "</response>\n",
      "\n",
      "Take a deep breath and think out loud before you begin to translate the voice transcriptions into SQL.\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "description = \"I want to select all the countries whose names contain the letter 'x'. select name from country where name like 'percent X percent '\"\n",
    "sql_prompt  = sql_prompt_template.format( voice_command=description )\n",
    "for line in sql_prompt.split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T17:50:08.758397Z",
     "start_time": "2024-02-08T17:50:08.731787Z"
    }
   },
   "id": "bb0026ff32308291"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking LLM [Phind-CodeLlama-34B-v2]...\n",
      "......................................................................................\n",
      "Asking LLM [Phind-CodeLlama-34B-v2]... Done! in 2,753 ms\n",
      "Tokens per second [31.2]\n",
      "<response>\n",
      "    <thoughts>The user wants to select all countries whose names contain the letter 'x'. The provided SQL command is incorrect as it is using the wrong syntax for the LIKE keyword. The correct syntax should be 'percent x percent' without the single quotes.</thoughts>\n",
      "    <sql>SELECT name FROM country WHERE name LIKE '%x%';</sql>\n",
      "</response>\n",
      "<response><thoughts>The user wants to select all countries whose names contain the letter 'x'. The provided SQL command is incorrect as it is using the wrong syntax for the LIKE keyword. The correct syntax should be 'percent x percent' without the single quotes.</thoughts><sql>SELECT name FROM country WHERE name LIKE '%x%';</sql></response>\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"SELECT name FROM country WHERE name LIKE '%x%';\""
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload\n",
    "response = xml_ftp_generator.query_llm_tgi( sql_prompt, model_name=\"Phind-CodeLlama-34B-v2\", max_new_tokens=1024, temperature=0.25, top_k=10, top_p=0.9, silent=False )\n",
    "print( response )\n",
    "response = dux.strip_all_white_space( response )\n",
    "print( response )\n",
    "sql = dux.get_value_by_xml_tag_name( response, \"sql\" )\n",
    "sql"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T17:50:13.626069Z",
     "start_time": "2024-02-08T17:50:10.844551Z"
    }
   },
   "id": "8d8d04460e2a6091"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "foo = \"\"\"\n",
    "The corrected Python code would be:\n",
    "\n",
    "```python\n",
    "tail_seeker = linked_list.head_node\n",
    "```\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T19:21:18.427345Z",
     "start_time": "2024-02-07T19:21:18.413660Z"
    }
   },
   "id": "850b013a10bdac5e"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting string from ```sql ...CODE...```, returning raw_string\n"
     ]
    },
    {
     "data": {
      "text/plain": "'asdfasdfasdfasdf'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _extract_string_from_backticked_llm_output( raw_string, tag_name=\"python\" ):\n",
    "    \n",
    "    try:\n",
    "        return raw_string.split( \"```\" + tag_name )[ 1 ].split( \"```\" )[ 0 ].strip()\n",
    "    except:\n",
    "        print( f\"Error extracting string from ```{tag_name} ...CODE...```, returning raw_string\" )\n",
    "        return raw_string\n",
    "    \n",
    "# _extract_string_from_backticked_llm_output( foo, tag_name=\"python\" )\n",
    "_extract_string_from_backticked_llm_output( \"asdfasdfasdfasdf\", tag_name=\"sql\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T19:31:37.613743Z",
     "start_time": "2024-02-07T19:31:37.601107Z"
    }
   },
   "id": "2222aa2c84a2f714"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 314M\r\n",
      "drwxr--r-- 4 1001 1001 4.0K Feb  2 21:04 .\r\n",
      "drwxr--r-- 4 1001 1001 4.0K Feb  5 17:42 ..\r\n",
      "-rwxr--r-- 1 1001 1001 6.1K Jun 20  2023 .DS_Store\r\n",
      "-rwxr--r-- 1 1001 1001 4.0K Jun 20  2023 ._.DS_Store\r\n",
      "-rw-rw-r-- 1 1001 1001    0 Oct 13 18:48 __init__.py\r\n",
      "drwxrwxr-x 2 1001 1001 4.0K Oct 13 18:48 fine-tuning-results\r\n",
      "drwxrwxr-x 2 1001 1001 4.0K Oct 13 18:48 jsonl\r\n",
      "-rw-rw-r-- 1 1001 1001 3.2K Feb  1 02:27 munger.py\r\n",
      "-rw-r--r-- 1 1001 1001 2.6K Jan 31 23:15 placeholders-calendaring-dates-and-times.txt\r\n",
      "-rw-r--r-- 1 1001 1001  414 Feb  1 02:43 placeholders-calendaring-events.txt\r\n",
      "-rw-r--r-- 1 1001 1001 5.4K Feb  1 00:39 placeholders-calendaring-locations.txt\r\n",
      "-rw-r--r-- 1 1001 1001 1.4K Jan 31 23:04 placeholders-calendaring-people.txt\r\n",
      "-rw-r--r-- 1 1001 1001 3.6K Jan 30 04:40 placeholders-cities-and-countries.txt\r\n",
      "-rw-r--r-- 1 1001 1001  982 Feb  2 17:17 placeholders-receptionist-titles.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  36K Jan 24 18:35 placeholders-search-terms.txt\r\n",
      "-rw-r--r-- 1 1001 1001  18K Jan 31 22:55 synthetic-data-agent-routing-calendaring.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 5.6K Jan 29 20:06 synthetic-data-agent-routing-date-and-time.txt\r\n",
      "-rw-r--r-- 1 1001 1001  13K Feb  2 17:17 synthetic-data-agent-routing-receptionist.txt\r\n",
      "-rw-r--r-- 1 1001 1001  25K Jan 30 04:05 synthetic-data-agent-routing-todo-lists.txt\r\n",
      "-rw-r--r-- 1 1001 1001  11K Jan 29 19:36 synthetic-data-agent-routing-weather.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 7.0K Jan 24 18:35 synthetic-data-load-url-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 7.3K Oct 13 18:48 synthetic-data-load-url-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  11K Feb  2 16:29 synthetic-data-none-of-the-above.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24 18:35 synthetic-data-search-clipboard-GENERIC-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24 18:35 synthetic-data-search-clipboard-GENERIC-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  11K Jan 24 18:35 synthetic-data-search-clipboard-google-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  11K Jan 24 18:35 synthetic-data-search-clipboard-google-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24 18:35 synthetic-data-search-clipboard-google-scholar-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24 18:35 synthetic-data-search-clipboard-google-scholar-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  14K Jan 24 18:35 synthetic-data-search-clipboard-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  13K Jan 24 18:35 synthetic-data-search-clipboard-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24 18:35 synthetic-data-search-clipboard-perplexity-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  12K Jan 24 18:35 synthetic-data-search-clipboard-perplexity-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  11K Feb  2 15:48 synthetic-data-search-clipboard-phind-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  11K Feb  2 15:48 synthetic-data-search-clipboard-phind-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 8.5K Jan 24 18:35 synthetic-data-search-google-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  11K Oct 13 18:48 synthetic-data-search-google-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 8.5K Oct 13 18:48 synthetic-data-search-google-scholar-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  11K Oct 13 18:48 synthetic-data-search-google-scholar-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 7.6K Oct 13 18:48 synthetic-data-search-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 8.9K Jan 24 18:35 synthetic-data-search-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 4.6K Jan 24 18:35 synthetic-data-search-perplexity-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 5.5K Jan 24 18:35 synthetic-data-search-perplexity-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 4.2K Feb  2 15:52 synthetic-data-search-phind-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 5.2K Feb  2 15:54 synthetic-data-search-phind-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  723 Oct 13 18:48 training-commands.map\r\n",
      "-rw-rw-rw- 1 1001 1001 6.7M Feb  2 19:26 voice-commands-xml-test-gpt.jsonl\r\n",
      "-rw-rw-rw- 1 1001 1001  24M Feb  2 19:26 voice-commands-xml-test.jsonl\r\n",
      "-rw-rw-rw- 1 1001 1001  54M Feb  2 19:26 voice-commands-xml-train-gpt.jsonl\r\n",
      "-rw-rw-rw- 1 1001 1001 185M Feb  2 19:26 voice-commands-xml-train.jsonl\r\n",
      "-rw-rw-rw- 1 1001 1001  24M Feb  2 19:26 voice-commands-xml-validate-gpt.jsonl\r\n",
      "-rw-rw-rw- 1 1001 1001  24M Feb  2 19:26 voice-commands-xml-validate.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /var/model/genie-in-the-box/src/ephemera/prompts/data/"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T18:24:06.884747Z",
     "start_time": "2024-02-05T18:24:06.756050Z"
    }
   },
   "id": "7bbb131aadef752e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 412K\r\n",
      "drwxr--r-- 35 1001 1001 4.0K Jan 30 19:19 .\r\n",
      "drwxr-xr-x  1 root root 4.0K Feb  1 15:40 ..\r\n",
      "drwxr-xr-x  3 root root 4.0K Jan 18 15:14 .locks\r\n",
      "drwxr--r-- 16 1001 1001 4.0K Aug 15 19:52 Auto-GPT\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Nov 19 02:04 CodeLlama-13b-Instruct-hf\r\n",
      "drwxrwxr-x  7 1001 1001 4.0K Dec 19 14:10 CodeLlama-7b-Instruct-hf\r\n",
      "-rwxr--r--  1 1001 1001 3.3K Feb 28  2023 Dockerfile\r\n",
      "drwxrwxr-x 15 1001 1001 4.0K Nov  8 18:06 OpenLLM\r\n",
      "drwxrwxr-x  9 1001 1001 4.0K Jan 12 20:09 Phind-CodeLlama-34B-v2\r\n",
      "drwxrwxr-x 13 1001 1001 4.0K Dec 13 00:08 TTS\r\n",
      "drwxr--r--  6 1001 1001 4.0K Feb 28  2023 ampe\r\n",
      "drwxr--r--  5 1001 1001 4.0K Jul  5  2023 cache\r\n",
      "-rwxr--r--  1 1001 1001 254K Aug  1  2023 chat_history.txt\r\n",
      "drwxr--r--  5 1001 1001 4.0K Apr 28  2023 coursework\r\n",
      "drwxrwxr-x  8 1001 1001 4.0K Nov  3 15:33 cursor-flask-chatbot\r\n",
      "drwxr--r--  6 1001 1001 4.0K Sep  3 23:49 cursor-flask-js-websocket\r\n",
      "drwxr-xr-x  3 1001 1001 4.0K Oct 16 18:49 cursor-gib\r\n",
      "drwxr--r--  3 1001 1001 4.0K Sep  6 19:51 cursor-langchain-experiments\r\n",
      "drwxrwxr-x 11 1001 1001 4.0K Jan  5 22:06 flash-attention-v2\r\n",
      "drwxr--r-- 12 1001 1001 4.0K Jul 31  2023 foo\r\n",
      "drwxr--r-- 11 1001 1001 4.0K Jan 29 18:58 genie-in-the-box\r\n",
      "drwxr--r--  9 1001 1001 4.0K Jan 27 20:43 genie-plugin-firefox\r\n",
      "drwxr--r--  9 1001 1001 4.0K Aug 21 17:49 genie-plugin-intellij\r\n",
      "drwxr--r--  2 1001 1001 4.0K Mar 24  2023 io\r\n",
      "drwxr-xr-x  2 1001 1001 4.0K Nov 16 23:00 kaitshup.substack.com\r\n",
      "drwxr--r--  9 1001 1001 4.0K Aug 22 14:30 langchain\r\n",
      "drwxrwxr-x  9 1001 1001 4.0K Jan 19 15:59 llm-awq\r\n",
      "drwxr--r--  5 1001 1001 4.0K Dec 13 19:15 mimape\r\n",
      "drwxrwxr-x  7 1001 1001 4.0K Jan 24 16:05 models\r\n",
      "-rw-------  1 root root 1.8K Feb  1 15:38 nohup.out\r\n",
      "drwxrwxr-x 11 1001 1001 4.0K Jan 11 20:34 peft\r\n",
      "drwxr--r--  5 1001 1001 4.0K Dec 13 16:25 pyxtermjs\r\n",
      "drwxr--r--  6 1001 1001 4.0K Mar 28  2023 scripts\r\n",
      "drwxrwxr-x 16 1001 1001 4.0K Jan 10 19:24 text-generation-inference\r\n",
      "drwxrwxr-x 16 1001 1001 4.0K Dec 26 19:52 transformers\r\n",
      "-rw-r--r--  1 root root    1 Nov 10 19:04 version.txt\r\n",
      "drwxrwxr-x 12 1001 1001 4.0K Jan  4 18:29 vllm\r\n",
      "drwxr--r--  6 1001 1001 4.0K Sep  8 14:36 wandb\r\n",
      "drwxr--r-- 64 1001 1001 4.0K Apr 26  2023 webextensions-examples\r\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /var/model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:40:33.253864Z",
     "start_time": "2024-02-01T15:40:33.140365Z"
    }
   },
   "id": "9feb9097838c568d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Re-purpose google search queries for use with PHIND"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab581df1cd9e7d1a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "os.chdir( \"/var/model/genie-in-the-box/src/ephemera/prompts/data\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T21:12:34.872001Z",
     "start_time": "2024-01-23T21:12:34.863464Z"
    }
   },
   "id": "45bedec696c7bf50"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipboard, p find search this tab.\n",
      "Run p find with clipboard data here.\n",
      "find, search from clipboard in current window.\n",
      "Clipboard text to phind search, this tab.\n",
      "p find, find clipboard content here.\n",
      "Use pea find with clipboard, in this window.\n",
      "Clipboard data, search here using pea find.\n",
      "pea find lookup with clipboard, in this tab.\n",
      "Clipboard content search in tab, via find.\n",
      "p find, conduct search from clipboard here.\n",
      "Clipboard info to pea find, search this window.\n",
      "find, use clipboard contents in current tab.\n",
      "Can phind search clipboard data in this window?\n",
      "p find, execute search with clipboard here.\n",
      "Clipboard search in this tab, through p find.\n",
      "p find, look up using clipboard in tab.\n",
      "Clipboard contents, find, search in window.\n",
      "phind, process clipboard search here.\n",
      "Clipboard to p find lookup, current window.\n",
      "pea find, analyze clipboard in this tab.\n",
      "Clipboard data, p find lookup in this window.\n",
      "find, clipboard content search here.\n",
      "Clipboard, pea find web search in tab.\n",
      "p find, clipboard search in current window.\n",
      "Clipboard to find, search in this tab.\n",
      "p find, apply clipboard in current search.\n",
      "phind, scan clipboard contents here.\n",
      "Clipboard data, use phind in this tab.\n",
      "pea find, clipboard lookup in current window.\n",
      "Clipboard, pea find tab search.\n",
      "pea find, clipboard content find in window.\n",
      "phind, deploy clipboard search in tab.\n",
      "Clipboard data, find search here.\n",
      "find, clipboard tab search.\n",
      "Clipboard, p find find in this window.\n",
      "find, investigate clipboard in tab.\n",
      "p find, apply clipboard data in current search.\n",
      "Clipboard contents to find, this window.\n",
      "find, conduct clipboard analysis in tab.\n",
      "Clipboard data, find search in window.\n",
      "find, utilize clipboard in this tab.\n",
      "Clipboard, p find window search.\n",
      "pea find, clipboard data use in tab.\n",
      "pea find, current tab clipboard lookup.\n",
      "Clipboard to p find, current window search.\n",
      "find, perform clipboard search here.\n",
      "Clipboard, use p find in this window.\n",
      "pea find, clipboard search in this tab now.\n",
      "Clipboard data, find quick search in window.\n",
      "phind, process clipboard data in tab.\n",
      "Clipboard, find analysis in current window.\n",
      "pea find, execute clipboard search in tab.\n",
      "Clipboard to p find, find in this window.\n",
      "pea find, clipboard content analysis in tab.\n",
      "Clipboard, phind search in this window.\n",
      "pea find, use clipboard for search in current tab.\n",
      "Clipboard data, pea find, search in tab.\n",
      "pea find, current window clipboard search.\n",
      "Clipboard, p find lookup here.\n",
      "p find, find using clipboard in this tab.\n",
      "Clipboard to p find, search now in window.\n",
      "pea find, clipboard search in tab.\n",
      "Clipboard, phind quick lookup in window.\n",
      "pea find, use clipboard data in this search.\n",
      "Clipboard data, p find tab search.\n",
      "p find, perform clipboard analysis in window.\n",
      "Clipboard, find search in current tab.\n",
      "pea find, tab search with clipboard.\n",
      "Clipboard to find, find in tab.\n",
      "phind, clipboard window lookup.\n",
      "Clipboard, find, search this window.\n",
      "pea find, current tab clipboard find.\n",
      "Clipboard data, phind, find in window.\n",
      "pea find, use clipboard in tab search.\n",
      "Clipboard, pea find, window search.\n",
      "pea find, search with clipboard in tab.\n",
      "Clipboard to find, current tab search.\n",
      "pea find clipboard search, this tab.\n",
      "Please use clipboard contents for a p find search here.\n",
      "Can you search using the clipboard in this window with p find?\n",
      "Search this tab with clipboard data on pea find.\n",
      "Conduct a search with what's on my clipboard with pea find, please.\n",
      "Clipboard to find search, in tab.\n",
      "Lookup using clipboard contents with p find, in this window.\n",
      "Perform a web search with my clipboard text here using pea find.\n",
      "Use the text from the clipboard for a p find search in this tab.\n",
      "Clipboard content, find now with find, in this tab.\n",
      "Search here with clipboard data on p find, please.\n",
      "Clipboard data, search in this window with pea find.\n",
      "Can you use the clipboard to search in the current tab with find?\n",
      "Take my clipboard contents and search here with p find.\n",
      "I'd like a search using the clipboard in this tab with pea find.\n",
      "Clipboard info, search in this window with p find, please.\n",
      "Use clipboard, find results in this tab with find.\n",
      "Search using clipboard, display in current tab with find.\n",
      "Clipboard content to p find web search, here.\n",
      "Please look up what's on my clipboard in this tab with p find.\n",
      "Clipboard to pea find search, present results here.\n",
      "Can you perform a search from the clipboard in this tab with p find?\n",
      "Search with clipboard text in this window using phind.\n",
      "Find using clipboard, show results here with p find.\n",
      "Clipboard text, use for pea find search in this tab.\n",
      "Execute a search with clipboard contents here using find.\n",
      "Clipboard data, lookup in this window with find.\n",
      "Can you do a search with my clipboard contents in this tab on p find?\n",
      "Search here using clipboard text with phind.\n",
      "Clipboard, search in current window with p find, please.\n",
      "Perform a clipboard-based search in this tab with p find.\n",
      "Clipboard contents to find, in this tab with p find.\n",
      "Use clipboard for search in this window with phind.\n",
      "Search using clipboard in current tab with pea find.\n",
      "Clipboard, conduct a search here with find.\n",
      "Clipboard data, quick search with find, this window.\n",
      "Can you find using clipboard in this tab with p find?\n",
      "Search this window with clipboard using find.\n",
      "Clipboard text, lookup in current tab with phind.\n",
      "Clipboard contents, use for search here with find.\n",
      "Execute search with clipboard in this tab using pea find.\n",
      "Clipboard, find in this window with p find.\n",
      "Search with what's on clipboard in tab using find.\n",
      "Clipboard data to phind web lookup, in this window.\n",
      "Can you search this tab with clipboard contents on phind?\n",
      "Clipboard to find, in current tab with find.\n",
      "Use clipboard, search in this window with pea find.\n",
      "Clipboard contents, quick find here with pea find.\n",
      "Search using clipboard, show in this tab with pea find.\n",
      "Clipboard, use for search in this window with p find.\n",
      "Perform a search with my clipboard in this tab using p find.\n",
      "Clipboard data, lookup here with pea find, please.\n",
      "Can you search with clipboard in current window on pea find?\n",
      "Search here, clipboard contents with p find.\n",
      "Clipboard to find web search, in this tab.\n",
      "Clipboard contents, use for quick search here with p find.\n",
      "Do a search with clipboard in this window using find.\n",
      "Clipboard, search in this tab with find.\n",
      "Can you look up the clipboard contents in this window with p find?\n",
      "Use clipboard for quick search in this tab with find.\n",
      "Clipboard text, search here with find.\n",
      "Perform a search using clipboard in current window with pea find.\n",
      "Search with clipboard data in this tab on pea find.\n",
      "Clipboard, lookup in this window with p find.\n",
      "Can you use clipboard for search in this tab with phind?\n",
      "Search this window, clipboard contents with pea find.\n",
      "Clipboard to p find search, show results in tab.\n",
      "Use clipboard, search now in this window with find.\n",
      "Clipboard contents, find in this tab with p find.\n",
      "Do a quick search with clipboard here using pea find.\n",
      "Clipboard data, search in this tab with pea find.\n",
      "Can you find using clipboard in current window with p find?\n",
      "Search here, clipboard with pea find.\n",
      "Clipboard to p find lookup, in this window.\n",
      "Use clipboard for search, show in tab with pea find.\n",
      "Clipboard, do a search in this window with p find.\n",
      "Can you search using clipboard in this tab with pea find?\n",
      "Search with clipboard, display here with pea find.\n",
      "Clipboard contents, lookup in this tab with pea find.\n",
      "Perform a web search with clipboard here using p find.\n",
      "Clipboard data, find now in this window with pea find.\n",
      "Can you look up with clipboard in this tab using p find?\n",
      "Search here using clipboard with find.\n",
      "Clipboard to phind web lookup, in tab.\n",
      "Use clipboard, find in this window with pea find.\n",
      "Clipboard, search in this tab, please, with find.\n",
      "Can you search this window with clipboard on pea find?\n",
      "Search with clipboard text here using p find.\n",
      "Clipboard contents to p find search, in this tab.\n",
      "Do a search with clipboard in this window using phind.\n",
      "Clipboard, lookup here with phind.\n",
      "Can you use clipboard contents for search in tab with pea find?\n",
      "Search this tab, clipboard data with pea find.\n",
      "Clipboard to find, show in window with pea find.\n",
      "Use clipboard, search in tab with pea find.\n",
      "Clipboard, do a quick search here with p find.\n",
      "Can you find with clipboard in this window using find?\n",
      "Search here, clipboard contents with pea find.\n",
      "Clipboard to p find web search, show in this tab.\n",
      "Use clipboard for quick lookup in this window with pea find.\n",
      "Can you take the text from my clipboard and use it to perform a search in this tab with pea find?\n",
      "Please execute a web search using the information I've just copied with pea find, displaying the results in this tab.\n",
      "Use the data I've copied to the clipboard for a phind search, and show the results in the current window.\n",
      "Could you search the internet with the contents of my clipboard using pea find and present the findings in this same tab?\n",
      "Take the clipboard contents and conduct a search with p find, making sure to display the results here in this browser tab.\n",
      "Please perform a search using the clipboard text with p find and display the results in the current tab.\n",
      "Can you use the contents I've just copied for a pea find web search and show the findings in this window?\n",
      "Execute a search with the data from my clipboard using p find and present it right here in this tab.\n",
      "I'd like you to search the web using what's on my clipboard with phind and keep the results in this browser window.\n",
      "Use what I've copied to the clipboard for a p find search and display the results in this same tab, please.\n",
      "Could you perform an internet search with the clipboard contents on p find and show the results in the current window?\n",
      "Please use the text I've copied for a search with p find and display the findings here in this tab.\n",
      "Conduct a web search using the clipboard data with p find and ensure the results appear in this window.\n",
      "Search the internet with my clipboard contents on pea find and present the findings in this tab, please.\n",
      "I need a search done with what's on the clipboard with find, showing the results in this browser tab.\n",
      "Can you do a web lookup with the clipboard text on phind and display the results in this current window?\n",
      "Please perform an online search with the contents of my clipboard using phind and keep the results in this tab.\n",
      "Take the data I've copied and use it for a find search, displaying the findings in the same tab.\n",
      "Execute a search using the clipboard contents on pea find and show the results here, in this browser window.\n",
      "Use the clipboard data for a phind web search and display the results in this tab, if you could.\n",
      "Could you look up the information on my clipboard using p find and present the search results in the current window?\n",
      "Please conduct a search with what I've copied to the clipboard on find and display the findings in this tab.\n",
      "I'd like a web search with the clipboard contents on p find and keep the results in this browser window.\n",
      "Object `you` not found.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "file = \"/var/model/genie-in-the-box/src/ephemera/prompts/data/synthetic-data-search-clipboard-phind-in-current-tab.txt\"\n",
    "lines = du.get_file_as_list( file, clean=True )#[ :10 ]\n",
    "\n",
    "# 1/4 of the time replace PHIND with find, 1/4 of the time with p find and 1/4 of the time do nothing\n",
    "for idx, line in enumerate( lines ):\n",
    "    \n",
    "    if line.find( \"phind\" ) > -1:\n",
    "        if random.random() < 0.25:\n",
    "            line = line.replace( \"phind\", \"find\" )\n",
    "        elif random.random() < 0.50:\n",
    "            line = line.replace( \"phind\", \"p find\" )\n",
    "        elif random.random() < 0.75:\n",
    "            line = line.replace( \"phind\", \"pea find\" )\n",
    "        else:\n",
    "            line = line.replace( \"phind\", \"phind\" )\n",
    "            \n",
    "    lines[ idx ] = line\n",
    "    \n",
    "for line in lines: print( line )\n",
    "\n",
    "du.write_lines_to_file( file, lines, strip_blank_lines=True, world_read_write=True )\n",
    "what does open ai mean to you?\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T21:20:02.529140Z",
     "start_time": "2024-01-23T21:20:02.448387Z"
    }
   },
   "id": "9c8b07fa466fd3fa"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "%autoreload\n",
    "file = \"/var/model/genie-in-the-box/src/ephemera/prompts/data/synthetic-data-search-phind-in-current-tab.txt\"\n",
    "lines = du.get_file_as_list( file, clean=True )#[ :10 ]\n",
    "du.write_lines_to_file( file, lines, strip_blank_lines=True, world_read_write=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T21:11:21.842925Z",
     "start_time": "2024-01-22T21:11:21.756428Z"
    }
   },
   "id": "5a2d6fffb020a05e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# associate commands with files containing examples\n",
    "browser_commands = {\n",
    "    \"search new tab\": \"/src/ephemera/prompts/data/synthetic-data-search-in-new-tab.txt\",\n",
    "    \"search current tab\": \"/src/ephemera/prompts/data/synthetic-data-search-in-current-tab.txt\",\n",
    "    \"search google new tab\": \"/src/ephemera/prompts/data/synthetic-data-search-google-in-new-tab.txt\",\n",
    "    \"search google current tab\": \"/src/ephemera/prompts/data/synthetic-data-search-google-in-current-tab.txt\",\n",
    "    \"search google scholar new tab\": \"/src/ephemera/prompts/data/synthetic-data-search-google-scholar-in-new-tab.txt\",\n",
    "    \"search google scholar current tab\": \"/src/ephemera/prompts/data/synthetic-data-search-google-scholar-in-current-tab.txt\",\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T15:30:08.969522Z",
     "start_time": "2024-01-17T15:30:08.962515Z"
    }
   },
   "id": "f2a3c680fe730fc4"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "prefix = \"/var/model/genie-in-the-box\"\n",
    "# prefix = du.get_project_root()\n",
    "\n",
    "for command in browser_commands.keys():\n",
    "    print( os.path.exists( prefix + browser_commands[ command ] ) )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T15:30:11.389747Z",
     "start_time": "2024-01-17T15:30:11.382421Z"
    }
   },
   "id": "cc3f7b6e767d780e"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-17T15:32:51.484042Z",
     "start_time": "2024-01-17T15:32:51.424592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['SEARCH_TERMS new tab search',\n 'New tab, search SEARCH_TERMS',\n 'Open new tab, search for SEARCH_TERMS',\n 'Pull up SEARCH_TERMS in another tab',\n 'Find SEARCH_TERMS in a new tab',\n 'Hunt for SEARCH_TERMS, new tab',\n 'Display SEARCH_TERMS results in new tab',\n 'Do a search for SEARCH_TERMS and show results in another tab',\n 'Execute SEARCH_TERMS search in a new tab',\n 'Seek SEARCH_TERMS in a fresh tab']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_lines = du.get_file_as_list( prefix + \"/src/ephemera/prompts/data/synthetic-data-search-in-new-tab.txt\", clean=True, randomize=False )[ 0:100 ]\n",
    "raw_lines[ 0:10 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_search_terms( requested_length ):\n",
    "    \n",
    "    # Load search terms file\n",
    "    search_terms = du.get_file_as_list( prefix + \"/src/ephemera/prompts/data/search-terms.txt\", lower_case=False, clean=True, randomize=True )\n",
    "    \n",
    "    # If we don't have enough search terms, append copies of the search term list until we do\n",
    "    while requested_length > len( search_terms ):\n",
    "        search_terms += search_terms\n",
    "        \n",
    "    # Truncate the search terms list to equal the requested len\n",
    "    search_terms = search_terms[ :requested_length ]\n",
    "    \n",
    "    return search_terms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T15:33:29.786637Z",
     "start_time": "2024-01-17T15:33:29.771980Z"
    }
   },
   "id": "39a8e819c4e89638"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['Indentation Error',\n 'Using Pandas in web development',\n 'Exporting data from Pandas to Excel',\n 'ResourceWarning',\n 'what is the meaning of success?',\n 'AI in space exploration',\n 'Data compression in Pandas',\n 'Using apply and map in Pandas',\n 'popular movie genres',\n 'Pandas DataFrame reshaping techniques']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_terms = get_search_terms( len( raw_lines ) )\n",
    "search_terms[ 0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T15:33:31.591200Z",
     "start_time": "2024-01-17T15:33:31.580223Z"
    }
   },
   "id": "f16cbbcf81d784f"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "\"'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'\""
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command_choices = \"'\" + \"', '\".join( browser_commands.keys() ) + \"' and 'none'\"\n",
    "command_choices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T15:33:39.298286Z",
     "start_time": "2024-01-17T15:33:39.287017Z"
    }
   },
   "id": "b538b5f3645f851f"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "gpt_instruction = f\"\"\"INSTRUCTIONS:\n",
    "Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
    "\n",
    "You will be given a human voice command as INPUT as well as a list of possible standardized commands. You must choose the correct standardized command from the following list: `{command_choices}`.\n",
    "\n",
    "RESPONSE FORMAT: MUST be returned wrapped in simple, well-formed XML\n",
    "<response>\n",
    "    <browser-command></browser-command>\n",
    "    <args></args>\n",
    "</response>\n",
    "\"\"\"\n",
    "\n",
    "# gpt_instruction = f\"\"\"\n",
    "# INSTRUCTIONS:\n",
    "# Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
    "# \n",
    "# You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `{command_choices}`.\n",
    "# \n",
    "# Requirement: You MUST NOT use python code to answer this question.\n",
    "# Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
    "# Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n",
    "# \n",
    "# INPUT FORMAT: You will receive a raw human voice command transcription tagged using simple XML\n",
    "# <human>\n",
    "#     <voice-command></voice-command>\n",
    "# </human>\n",
    "# \n",
    "# RESPONSE FORMAT: MUST be returned wrapped in simple, well-formed XML\n",
    "# <response>\n",
    "#     <browser-command></browser-command>\n",
    "#     <args></args>\n",
    "# </response>\n",
    "# \"\"\"\n",
    "\n",
    "# gpt_input_template = \"\"\"\n",
    "# INPUT:\n",
    "# <human>\n",
    "#     <voice-command>{voice_command}</voice-command>\n",
    "# </human>\n",
    "# \n",
    "# YOUR RESPONSE:\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "output_template = \"\"\"\n",
    "<response>\n",
    "    <browser-command>{browser_command}</browser-command>\n",
    "    <args>{args}</args>\n",
    "</response>\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:58:27.115076Z",
     "start_time": "2024-01-16T17:58:27.041273Z"
    }
   },
   "id": "d01beabe0654fcf0"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTIONS:\n",
      "Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
      "\n",
      "You will be given a human voice command as INPUT as well as a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\n",
      "\n",
      "RESPONSE FORMAT: MUST be returned wrapped in simple, well-formed XML\n",
      "<response>\n",
      "    <browser-command></browser-command>\n",
      "    <args></args>\n",
      "</response>\n"
     ]
    }
   ],
   "source": [
    "for line in gpt_instruction.split( \"\\n\" ):\n",
    "    print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:58:28.598717Z",
     "start_time": "2024-01-16T17:58:28.588766Z"
    }
   },
   "id": "2ea7d8cffc581ac9"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "\n",
    "instruction_template = \"\"\"Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
    "\n",
    "You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `{command_choices}`.\n",
    "\n",
    "Requirement: You MUST NOT use python code to answer this question.\n",
    "Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
    "Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\"\"\"\n",
    "\n",
    "input_template = \"\"\"\n",
    "Below is the raw human voice command transcription formatted using simple XML: \n",
    "{human_says}\n",
    "\n",
    "The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
    "{response_format}\"\"\"\n",
    "\n",
    "human_says_template = \"\"\"\n",
    "<human>\n",
    "    <voice-command>{voice_command}</voice-command>\n",
    "</human>\"\"\"\n",
    "\n",
    "response_format = \"\"\"\n",
    "<response>\n",
    "    <browser-command></browser-command>\n",
    "    <args></args>\n",
    "</response>\n",
    "\n",
    "Requirement: The first word of your response MUST be `<response>`\"\"\"\n",
    "\n",
    "output_template = \"\"\"\n",
    "<response>\n",
    "    <browser-command>{browser_command}</browser-command>\n",
    "    <args>{args}</args>\n",
    "</response>\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:58:33.998508Z",
     "start_time": "2024-01-16T17:58:33.987573Z"
    }
   },
   "id": "c3537ada45cdf79b"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
      "\n",
      "You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `{command_choices}`.\n",
      "\n",
      "Requirement: You MUST NOT use python code to answer this question.\n",
      "Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
      "Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n"
     ]
    }
   ],
   "source": [
    "for line in instruction_template.split( \"\\n\" ):\n",
    "    print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T16:23:24.980628Z",
     "start_time": "2024-01-16T16:23:24.971579Z"
    }
   },
   "id": "60274be0aad8737a"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\\n\\nYou will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\\n\\nRequirement: You MUST NOT use python code to answer this question.\\nRequirement: You MUST use your linguistic knowledge and intuition to answer this question.\\nHint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\""
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = instruction_template.format( command_choices=command_choices )\n",
    "instruction"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:38:09.116184Z",
     "start_time": "2024-01-16T15:38:09.108137Z"
    }
   },
   "id": "e7996237fec316c5"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:38:12.418421Z",
     "start_time": "2024-01-16T15:38:12.408858Z"
    }
   },
   "id": "2ea54f4c983515b6"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def prompt_instruction_format( instruction, input ):\n",
    "\n",
    "    return f\"\"\"### Instruction:\n",
    "Use the Task and Input given below to write a Response that can solve the following Task.\n",
    "\n",
    "### Task:\n",
    "{instruction}\n",
    "\n",
    "### Input:{input}\n",
    "### Response:\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:38:22.252054Z",
     "start_time": "2024-01-16T15:38:22.245528Z"
    }
   },
   "id": "50e11006cf38e624"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "messages = [\n",
    "    { \"role\": \"system\", \"content\": gpt_instruction },\n",
    "    { \"role\": \"user\", \"content\": voice_command },\n",
    "    # { \"role\": \"user\", \"content\": gpt_input_template.format( voice_command=voice_command ) },\n",
    "    { \"role\": \"assistant\", \"content\": output_template.format( browser_command=browser_command, args=terms ) }\n",
    "]\n",
    "messages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f7dc74ad42f15da"
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- search new tab\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- search current tab\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- search google new tab\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- search google current tab\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- search google scholar new tab\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- search google scholar current tab\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "instructions = []\n",
    "inputs       = []\n",
    "outputs      = []\n",
    "prompts      = []\n",
    "# gpt_prompts  = []\n",
    "# gpt_inputs   = []\n",
    "gpt_messages = []\n",
    "\n",
    "for browser_command in browser_commands.keys():\n",
    "    \n",
    "    du.print_banner( browser_command, prepend_nl=True, end=\"\\n\" )\n",
    "    \n",
    "    raw_lines = du.get_file_as_list( prefix + browser_commands[ browser_command ], clean=True )\n",
    "    for line in raw_lines[ 0:100 ]: #[ 0:2 ]:#\n",
    "        \n",
    "        # get newly randomized search terms on every iteration\n",
    "        for terms in get_search_terms( len( raw_lines ) ):#[ 0:10 ]: #[ 0:2]:#:\n",
    "            \n",
    "            voice_command = line.replace( \"SEARCH_TERMS\", terms )\n",
    "            instruction   = instruction_template.format( command_choices=command_choices )\n",
    "            human_says    = human_says_template.format( voice_command=voice_command )\n",
    "            input         = input_template.format( human_says=human_says, response_format=response_format )\n",
    "            # gpt_input     = gpt_input_template.format( voice_command=voice_command )\n",
    "            output        = output_template.format( browser_command=browser_command, args=terms )\n",
    "            sql_prompt        = prompt_instruction_format( instruction, input )\n",
    "    \n",
    "            instructions.append( instruction )\n",
    "            inputs.append( input )\n",
    "            outputs.append( output )\n",
    "            prompts.append( sql_prompt )\n",
    "            # gpt_inputs.append( gpt_input )\n",
    "            # gpt_prompts.append( gpt_instruction )\n",
    "            gpt_messages.append( { \n",
    "                \"messages\": [\n",
    "                    { \"role\": \"system\", \"content\": gpt_instruction },\n",
    "                    { \"role\": \"user\", \"content\": voice_command },\n",
    "                    { \"role\": \"assistant\", \"content\": output_template.format( browser_command=browser_command, args=terms ) }\n",
    "                ] \n",
    "            } )\n",
    "    \n",
    "            print( \".\", end=\"\" )\n",
    "        \n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:37:52.064488Z",
     "start_time": "2024-01-16T18:37:50.799082Z"
    }
   },
   "id": "df63613a4a89fd76"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'role': 'system',\n  'content': \"INSTRUCTIONS:\\nYour job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\\n\\nYou will be given a human voice command as INPUT as well as a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\\n\\nRESPONSE FORMAT: MUST be returned wrapped in simple, well-formed XML\\n<response>\\n    <browser-command></browser-command>\\n    <args></args>\\n</response>\\n\"},\n {'role': 'user', 'content': 'what is the stock market? new tab search'},\n {'role': 'assistant',\n  'content': '\\n<response>\\n    <browser-command>search new tab</browser-command>\\n    <args>what is the stock market?</args>\\n</response>'}]"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_messages[ 0 ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:58:51.328404Z",
     "start_time": "2024-01-16T17:58:51.315879Z"
    }
   },
   "id": "4e8325c691d7b7b0"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Use the Task and Input given below to write a Response that can solve the following Task.\n",
      "\n",
      "### Task:\n",
      "Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
      "\n",
      "You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\n",
      "\n",
      "Requirement: You MUST NOT use python code to answer this question.\n",
      "Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
      "Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n",
      "\n",
      "### Input:\n",
      "Below is the raw human voice command transcription formatted using simple XML: \n",
      "\n",
      "<human>\n",
      "    <voice-command>what is the stock market? new tab search</voice-command>\n",
      "</human>\n",
      "\n",
      "The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
      "\n",
      "<response>\n",
      "    <browser-command></browser-command>\n",
      "    <args></args>\n",
      "</response>\n",
      "\n",
      "Requirement: The first word of your response MUST be `<response>`\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "for line in prompts[ 0 ].split( \"\\n\" ):\n",
    "    print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:59:05.631629Z",
     "start_time": "2024-01-16T17:59:05.622910Z"
    }
   },
   "id": "f816307c20687d3c"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
      "\n",
      "You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\n",
      "\n",
      "Requirement: You MUST NOT use python code to answer this question.\n",
      "Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
      "Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n",
      "\n",
      "Below is the raw human voice command transcription formatted using simple XML: \n",
      "\n",
      "<human>\n",
      "    <voice-command>best hiking trails near me new tab search</voice-command>\n",
      "</human>\n",
      "\n",
      "The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
      "\n",
      "<response>\n",
      "    <browser-command></browser-command>\n",
      "    <args></args>\n",
      "</response>\n",
      "\n",
      "Requirement: The first word of your response MUST be `<response>`\n",
      "\n",
      "<response>\n",
      "    <browser-command>search new tab</browser-command>\n",
      "    <args>best hiking trails near me</args>\n",
      "</response>\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "for line in instructions[ idx ].split( \"\\n\" ):\n",
    "    print( line )\n",
    "    \n",
    "for line in inputs[ idx ].split( \"\\n\" ):\n",
    "    print( line )\n",
    "    \n",
    "for line in outputs[ idx ].split( \"\\n\" ):\n",
    "    print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T16:33:08.736632Z",
     "start_time": "2024-01-16T16:33:08.724512Z"
    }
   },
   "id": "ff379435dddb3a10"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def inject_space( line ):\n",
    "    if random.random() < 0.5:\n",
    "        index = random.randint( 0, len( line ) - 1 )\n",
    "        return line[ :index ] + ' ' + line[ index: ]\n",
    "    else:\n",
    "        return line"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T16:33:51.885598Z",
     "start_time": "2024-01-16T16:33:51.870086Z"
    }
   },
   "id": "10936b21cad13826"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "0    [{'role': 'system', 'content': 'INSTRUCTIONS:\n...\n1    [{'role': 'system', 'content': 'INSTRUCTIONS:\n...\n2    [{'role': 'system', 'content': 'INSTRUCTIONS:\n...\n3    [{'role': 'system', 'content': 'INSTRUCTIONS:\n...\n4    [{'role': 'system', 'content': 'INSTRUCTIONS:\n...\nName: gpt_messages, dtype: object"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df = pd.DataFrame( { \"instruction\": instructions, \"input\": inputs, \"output\": outputs, \"prompt\": prompts, \"gpt_message\": gpt_messages } )\n",
    "\n",
    "# Perturb the answer to create a faked response  \n",
    "qna_df[ \"response\" ] = qna_df[ \"output\" ].apply( lambda cell: inject_space( cell ) )\n",
    "qna_df.gpt_messages.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:59:15.166474Z",
     "start_time": "2024-01-16T17:59:15.018776Z"
    }
   },
   "id": "69cd9131a026aa59"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validate the structure of the xml response"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c6c0eeb4ab6429c"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmlschema in /usr/local/lib/python3.10/dist-packages (3.0.1)\r\n",
      "Requirement already satisfied: elementpath<5.0.0,>=4.1.5 in /usr/local/lib/python3.10/dist-packages (from xmlschema) (4.1.5)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install xmlschema"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:18:22.451767Z",
     "start_time": "2024-01-16T17:18:20.571529Z"
    }
   },
   "id": "9cf0e4b55266f65b"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from xmlschema import XMLSchema\n",
    "\n",
    "xml_str_1 = \"\"\"\n",
    "<response>\n",
    "  <browser-command></browser-command>\n",
    "  <args></args>\n",
    "</response>\n",
    "\"\"\"\n",
    "xml_str_2 = \"\"\"\n",
    "<response>\n",
    "  <browser_command></browser_command>\n",
    "  <args></args>\n",
    "</response>\n",
    "\"\"\"\n",
    "xml_str_3 = \"\"\"\n",
    "Sure here's your command!\n",
    "<response>\n",
    "  <browser-command></browser-command>\n",
    "  <args></args>\n",
    "</response>\n",
    "\"\"\"\n",
    "\n",
    "xml_str_4 = \"\"\"\n",
    "```xml\n",
    "<response>\n",
    "  <browser-command></browser-command>\n",
    "  <args></args>\n",
    "</response>\n",
    "\"\"\"\n",
    "\n",
    "xsd_string = \"\"\"\n",
    "<xs:schema xmlns:xs=\"http://www.w3.org/2001/XMLSchema\">\n",
    "  <xs:element name=\"response\">\n",
    "    <xs:complexType>\n",
    "      <xs:sequence>\n",
    "        <xs:element name=\"browser-command\" type=\"xs:string\"/>\n",
    "        <xs:element name=\"args\" type=\"xs:string\"/>\n",
    "      </xs:sequence>\n",
    "    </xs:complexType>\n",
    "  </xs:element>\n",
    "</xs:schema>\n",
    "\"\"\"\n",
    "\n",
    "def is_valid_xml( xml_str, schema ):\n",
    "    \n",
    "    try:\n",
    "        schema.is_valid( xml_str )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # print( \"Invalid XML: \" + str( e ) )\n",
    "        return False\n",
    "\n",
    "schema   = XMLSchema( xsd_string )\n",
    "\n",
    "print( is_valid_xml( xml_str_1, schema ) )  # Output: ???\n",
    "print( is_valid_xml( xml_str_2, schema ) )  # Output: ???\n",
    "print( is_valid_xml( xml_str_3, schema ) )  # Output: False\n",
    "print( is_valid_xml( xml_str_4, schema ) )  # Output: False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:18:23.722823Z",
     "start_time": "2024-01-16T17:18:23.716700Z"
    }
   },
   "id": "ebae05bcfc626b0c"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "                                         instruction  \\\n0  Your job is to discern the intent of a human v...   \n1  Your job is to discern the intent of a human v...   \n2  Your job is to discern the intent of a human v...   \n3  Your job is to discern the intent of a human v...   \n4  Your job is to discern the intent of a human v...   \n\n                                               input  \\\n0  \\nBelow is the raw human voice command transcr...   \n1  \\nBelow is the raw human voice command transcr...   \n2  \\nBelow is the raw human voice command transcr...   \n3  \\nBelow is the raw human voice command transcr...   \n4  \\nBelow is the raw human voice command transcr...   \n\n                                              output  \\\n0  \\n<response>\\n    <browser-command>search new ...   \n1  \\n<response>\\n    <browser-command>search new ...   \n2  \\n<response>\\n    <browser-command>search new ...   \n3  \\n<response>\\n    <browser-command>search new ...   \n4  \\n<response>\\n    <browser-command>search new ...   \n\n                                              prompt  \\\n0  ### Instruction:\\nUse the Task and Input given...   \n1  ### Instruction:\\nUse the Task and Input given...   \n2  ### Instruction:\\nUse the Task and Input given...   \n3  ### Instruction:\\nUse the Task and Input given...   \n4  ### Instruction:\\nUse the Task and Input given...   \n\n                                        gpt_messages  \\\n0  [{'role': 'system', 'content': '\nINSTRUCTIONS:...   \n1  [{'role': 'system', 'content': '\nINSTRUCTIONS:...   \n2  [{'role': 'system', 'content': '\nINSTRUCTIONS:...   \n3  [{'role': 'system', 'content': '\nINSTRUCTIONS:...   \n4  [{'role': 'system', 'content': '\nINSTRUCTIONS:...   \n\n                                            response  response_xml_is_valid  \n0  \\n<response>\\n    <browser-command>search new ...                   True  \n1  \\n<response>\\n    <browser-command>search new ...                   True  \n2  \\n<response>\\n    <browser-command>search new ...                  False  \n3  \\n<response>\\n    <browser-command>search new ...                   True  \n4  \\n<response>\\n     <browser-command>search new...                   True  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>input</th>\n      <th>output</th>\n      <th>prompt</th>\n      <th>gpt_messages</th>\n      <th>response</th>\n      <th>response_xml_is_valid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\nBelow is the raw human voice command transcr...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>### Instruction:\\nUse the Task and Input given...</td>\n      <td>[{'role': 'system', 'content': '\nINSTRUCTIONS:...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\nBelow is the raw human voice command transcr...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>### Instruction:\\nUse the Task and Input given...</td>\n      <td>[{'role': 'system', 'content': '\nINSTRUCTIONS:...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\nBelow is the raw human voice command transcr...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>### Instruction:\\nUse the Task and Input given...</td>\n      <td>[{'role': 'system', 'content': '\nINSTRUCTIONS:...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\nBelow is the raw human voice command transcr...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>### Instruction:\\nUse the Task and Input given...</td>\n      <td>[{'role': 'system', 'content': '\nINSTRUCTIONS:...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\nBelow is the raw human voice command transcr...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>### Instruction:\\nUse the Task and Input given...</td>\n      <td>[{'role': 'system', 'content': '\nINSTRUCTIONS:...</td>\n      <td>\\n&lt;response&gt;\\n     &lt;browser-command&gt;search new...</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df[ \"response_xml_is_valid\" ] = qna_df[ \"response\" ].apply( lambda cell: is_valid_xml( cell, schema ) )\n",
    "qna_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:18:47.496521Z",
     "start_time": "2024-01-16T17:18:25.287001Z"
    }
   },
   "id": "476168c0862285f8"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7940583333333333"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df.response_xml_is_valid.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:18:53.756596Z",
     "start_time": "2024-01-16T17:18:53.738684Z"
    }
   },
   "id": "1de4a9107307a2fa"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def contains_valid_xml_tag( xml_str, tag_name ):\n",
    "    \n",
    "    try:\n",
    "        return \"<\" + tag_name + \">\" in xml_str and \"</\" + tag_name + \">\" in xml_str\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    \n",
    "contains_valid_xml_tag( \"```xml<response><browser-command></browser-command><args></args></response>\", \"browser-command\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:18:56.156285Z",
     "start_time": "2024-01-16T17:18:56.135615Z"
    }
   },
   "id": "cd990344958fe1d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validate the structure of the xml response"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d7b5958266e9c1a"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "qna_df[ \"contains_response\" ]        = qna_df[ \"response\" ].apply( lambda cell: contains_valid_xml_tag( cell, \"response\" ) )\n",
    "qna_df[ \"contains_browser_command\" ] = qna_df[ \"response\" ].apply( lambda cell: contains_valid_xml_tag( cell, \"browser-command\" ) )\n",
    "qna_df[ \"contains_args\" ]            = qna_df[ \"response\" ].apply( lambda cell: contains_valid_xml_tag( cell, \"args\" ) )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:01.200010Z",
     "start_time": "2024-01-16T17:19:01.019929Z"
    }
   },
   "id": "a93536b26f1ab89"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9306083333333334\n",
      "0.8822166666666666\n",
      "0.9600916666666667\n"
     ]
    }
   ],
   "source": [
    "print( qna_df[ \"contains_response\" ].mean() )\n",
    "print( qna_df[ \"contains_browser_command\" ].mean() )\n",
    "print( qna_df[ \"contains_args\" ].mean() )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:02.639731Z",
     "start_time": "2024-01-16T17:19:02.610123Z"
    }
   },
   "id": "4dfecb7ae4d37c63"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validate the response by removing all white space and comparing to the original answer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1789a964c4b89ea"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "xml_str = \"\"\"\n",
    "<response>\n",
    "  <browser-command>Whos your favorite browser?</browser-command>\n",
    "  <args>Bar browser</args>\n",
    "</response>\n",
    "\"\"\"\n",
    "\n",
    "# Remove white space outside XML tags\n",
    "xml_str = re.sub(r'>\\s+<', '><', xml_str.strip())\n",
    "\n",
    "print(xml_str)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:07.167036Z",
     "start_time": "2024-01-16T17:19:07.123514Z"
    }
   },
   "id": "17461163fc843dde"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_response_exact( response, answer ):\n",
    "    \n",
    "    # Remove white space outside XML tags\n",
    "    response = re.sub( r'>\\s+<', '><', response.strip() )\n",
    "    \n",
    "    # Remove white space outside XML tags\n",
    "    answer = re.sub( r'>\\s+<', '><', answer.strip() )\n",
    "    \n",
    "    return response == answer\n",
    "\n",
    "is_response_exact( \"<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\", \"<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:08.851437Z",
     "start_time": "2024-01-16T17:19:08.845905Z"
    }
   },
   "id": "dcf24366c637b66b"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dux' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[58], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      9\u001B[0m answer   \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28mprint\u001B[39m( \u001B[43mtag_values_are_equal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtag_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbrowser-command\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m )\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m( tag_values_are_equal( response, answer, tag_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margs\u001B[39m\u001B[38;5;124m\"\u001B[39m ) )\n",
      "Cell \u001B[0;32mIn[58], line 3\u001B[0m, in \u001B[0;36mtag_values_are_equal\u001B[0;34m(response, answer, tag_name)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtag_values_are_equal\u001B[39m( response, answer, tag_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbrowser-command\u001B[39m\u001B[38;5;124m\"\u001B[39m ):\n\u001B[0;32m----> 3\u001B[0m     command_response \u001B[38;5;241m=\u001B[39m \u001B[43mdux\u001B[49m\u001B[38;5;241m.\u001B[39mget_value_by_xml_tag_name( response, tag_name, default_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbroken\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n\u001B[1;32m      4\u001B[0m     command_answer   \u001B[38;5;241m=\u001B[39m dux\u001B[38;5;241m.\u001B[39mget_value_by_xml_tag_name(   answer, tag_name, default_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbroken\u001B[39m\u001B[38;5;124m\"\u001B[39m )\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m command_response \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbroken\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m command_answer \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbroken\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m command_response \u001B[38;5;241m==\u001B[39m command_answer\n",
      "\u001B[0;31mNameError\u001B[0m: name 'dux' is not defined"
     ]
    }
   ],
   "source": [
    "def tag_values_are_equal( response, answer, tag_name=\"browser-command\" ):\n",
    "    \n",
    "    command_response = dux.get_value_by_xml_tag_name( response, tag_name, default_value=\"broken\" )\n",
    "    command_answer   = dux.get_value_by_xml_tag_name(   answer, tag_name, default_value=\"broken\" )\n",
    "    \n",
    "    return command_response != \"broken\" and command_answer != \"broken\" and command_response == command_answer\n",
    "\n",
    "response = \"<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\"\n",
    "answer   = \"<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\"\n",
    "\n",
    "print( tag_values_are_equal( response, answer, tag_name=\"browser-command\" ) )\n",
    "print( tag_values_are_equal( response, answer, tag_name=\"args\" ) )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:10.278652Z",
     "start_time": "2024-01-16T17:19:10.239047Z"
    }
   },
   "id": "5d7316ca3f07b221"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def contains_correct_response_values( response, answer ):\n",
    "    \n",
    "    \"\"\"Check to see if the most common formatting error (```xml) is hiding a correct response\"\"\"\n",
    "    response = \"<response>\" + dux.get_value_by_xml_tag_name( response, \"response\", default_value=\"broken\" ) + \"</response>\"\n",
    "    response = dux.get_xml_tag_and_value_by_name( response, \"response\", default_value=\"broken\" )\n",
    "    if response == \"broken\":\n",
    "        return False\n",
    "    \n",
    "    # Remove white space outside XML tags\n",
    "    response = re.sub( r'>\\s+<', '><', response.strip() )\n",
    "    \n",
    "    # Remove white space outside XML tags\n",
    "    answer = re.sub( r'>\\s+<', '><', answer.strip() )\n",
    "    \n",
    "    return response == answer\n",
    "\n",
    "contains_correct_response_values( \"```xml<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\", \"<response><browser-command>Whos your favorite browser?</browser-command><args>Bar browser</args></response>\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:12.220650Z",
     "start_time": "2024-01-16T17:19:12.208324Z"
    }
   },
   "id": "5648ef7387c840f7"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "qna_df[ \"response_is_correct\" ] = qna_df.apply( lambda row: is_response_exact( row[ \"response\" ], row[ \"output\" ] ), axis=1 )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:15.651795Z",
     "start_time": "2024-01-16T17:19:14.536369Z"
    }
   },
   "id": "d1f5537569851fb6"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5582"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df.response_is_correct.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:16.867638Z",
     "start_time": "2024-01-16T17:19:16.851340Z"
    }
   },
   "id": "6776bf949b6f1538"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          instruction  \\\n10  Your job is to discern the intent of a human v...   \n17  Your job is to discern the intent of a human v...   \n22  Your job is to discern the intent of a human v...   \n26  Your job is to discern the intent of a human v...   \n28  Your job is to discern the intent of a human v...   \n31  Your job is to discern the intent of a human v...   \n32  Your job is to discern the intent of a human v...   \n34  Your job is to discern the intent of a human v...   \n36  Your job is to discern the intent of a human v...   \n44  Your job is to discern the intent of a human v...   \n\n                                                input  \\\n10  \\nBelow is the raw human voice command transcr...   \n17  \\nBelow is the raw human voice command transcr...   \n22  \\nBelow is the raw human voice command transcr...   \n26  \\nBelow is the raw human voice command transcr...   \n28  \\nBelow is the raw human voice command transcr...   \n31  \\nBelow is the raw human voice command transcr...   \n32  \\nBelow is the raw human voice command transcr...   \n34  \\nBelow is the raw human voice command transcr...   \n36  \\nBelow is the raw human voice command transcr...   \n44  \\nBelow is the raw human voice command transcr...   \n\n                                               output  \\\n10  \\n<response>\\n    <browser-command>search new ...   \n17  \\n<response>\\n    <browser-command>search new ...   \n22  \\n<response>\\n    <browser-command>search new ...   \n26  \\n<response>\\n    <browser-command>search new ...   \n28  \\n<response>\\n    <browser-command>search new ...   \n31  \\n<response>\\n    <browser-command>search new ...   \n32  \\n<response>\\n    <browser-command>search new ...   \n34  \\n<response>\\n    <browser-command>search new ...   \n36  \\n<response>\\n    <browser-command>search new ...   \n44  \\n<response>\\n    <browser-command>search new ...   \n\n                                               prompt  \\\n10  ### Instruction:\\nUse the Task and Input given...   \n17  ### Instruction:\\nUse the Task and Input given...   \n22  ### Instruction:\\nUse the Task and Input given...   \n26  ### Instruction:\\nUse the Task and Input given...   \n28  ### Instruction:\\nUse the Task and Input given...   \n31  ### Instruction:\\nUse the Task and Input given...   \n32  ### Instruction:\\nUse the Task and Input given...   \n34  ### Instruction:\\nUse the Task and Input given...   \n36  ### Instruction:\\nUse the Task and Input given...   \n44  ### Instruction:\\nUse the Task and Input given...   \n\n                                         gpt_messages  \\\n10  [{'role': 'system', 'content': '\nINSTRUCTIONS:...   \n17  [{'role': 'system', 'content': '\nINSTRUCTIONS:...   \n22  [{'role': 'system', 'content': '\nINSTRUCTIONS:...   \n26  [{'role': 'system', 'content': '\nINSTRUCTIONS:...   \n28  [{'role': 'system', 'content': '\nINSTRUCTIONS:...   \n31  [{'role': 'system', 'content': '\nINSTRUCTIONS:...   \n32  [{'role': 'system', 'content': '\nINSTRUCTIONS:...   \n34  [{'role': 'system', 'content': '\nINSTRUCTIONS:...   \n36  [{'role': 'system', 'content': '\nINSTRUCTIONS:...   \n44  [{'role': 'system', 'content': '\nINSTRUCTIONS:...   \n\n                                             response  response_xml_is_valid  \\\n10  \\n<response>\\n    <browser-command>search new ...                   True   \n17  \\n<response>\\n    <browser-command>search new ...                   True   \n22  \\n<response>\\n    <browser-command>search new ...                   True   \n26  \\n<response>\\n    <browser-command>search new ...                   True   \n28  \\n<response>\\n    <browser-command>search new ...                   True   \n31  \\n<response>\\n    <browser-command>search new ...                   True   \n32  \\n<response>\\n    <browser-command>search new ...                   True   \n34  \\n<response>\\n    <browser-command>search  new...                   True   \n36  \\n<response >\\n    <browser-command>search new...                   True   \n44  \\n<response>\\n    <browser-command>search new ...                   True   \n\n    contains_response  contains_browser_command  contains_args  \\\n10               True                      True           True   \n17               True                      True           True   \n22               True                      True           True   \n26               True                      True           True   \n28               True                      True           True   \n31               True                      True           True   \n32               True                      True           True   \n34               True                      True           True   \n36              False                      True           True   \n44               True                      True           True   \n\n    response_is_correct  \n10                False  \n17                False  \n22                False  \n26                False  \n28                False  \n31                False  \n32                False  \n34                False  \n36                False  \n44                False  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>input</th>\n      <th>output</th>\n      <th>prompt</th>\n      <th>gpt_messages</th>\n      <th>response</th>\n      <th>response_xml_is_valid</th>\n      <th>contains_response</th>\n      <th>contains_browser_command</th>\n      <th>contains_args</th>\n      <th>response_is_correct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\nBelow is the raw human voice command transcr...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>### Instruction:\\nUse the Task and Input given...</td>\n      <td>[{'role': 'system', 'content': '\nINSTRUCTIONS:...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\nBelow is the raw human voice command transcr...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>### Instruction:\\nUse the Task and Input given...</td>\n      <td>[{'role': 'system', 'content': '\nINSTRUCTIONS:...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\nBelow is the raw human voice command transcr...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>### Instruction:\\nUse the Task and Input given...</td>\n      <td>[{'role': 'system', 'content': '\nINSTRUCTIONS:...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\nBelow is the raw human voice command transcr...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>### Instruction:\\nUse the Task and Input given...</td>\n      <td>[{'role': 'system', 'content': '\nINSTRUCTIONS:...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\nBelow is the raw human voice command transcr...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>### Instruction:\\nUse the Task and Input given...</td>\n      <td>[{'role': 'system', 'content': '\nINSTRUCTIONS:...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\nBelow is the raw human voice command transcr...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>### Instruction:\\nUse the Task and Input given...</td>\n      <td>[{'role': 'system', 'content': '\nINSTRUCTIONS:...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\nBelow is the raw human voice command transcr...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>### Instruction:\\nUse the Task and Input given...</td>\n      <td>[{'role': 'system', 'content': '\nINSTRUCTIONS:...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\nBelow is the raw human voice command transcr...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>### Instruction:\\nUse the Task and Input given...</td>\n      <td>[{'role': 'system', 'content': '\nINSTRUCTIONS:...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search  new...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\nBelow is the raw human voice command transcr...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>### Instruction:\\nUse the Task and Input given...</td>\n      <td>[{'role': 'system', 'content': '\nINSTRUCTIONS:...</td>\n      <td>\\n&lt;response &gt;\\n    &lt;browser-command&gt;search new...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>Your job is to discern the intent of a human v...</td>\n      <td>\\nBelow is the raw human voice command transcr...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>### Instruction:\\nUse the Task and Input given...</td>\n      <td>[{'role': 'system', 'content': '\nINSTRUCTIONS:...</td>\n      <td>\\n&lt;response&gt;\\n    &lt;browser-command&gt;search new ...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df[ ( qna_df.response_xml_is_valid == True ) & ( qna_df.response_is_correct == False ) ].head( 10 )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:19:19.848536Z",
     "start_time": "2024-01-16T17:19:19.798993Z"
    }
   },
   "id": "26f8b8cef970a601"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What's the baseline behavior of the Vanilla model?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9471509827a815a8"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install tiktoken"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T15:49:56.709073Z",
     "start_time": "2024-01-17T15:49:56.656996Z"
    }
   },
   "id": "81af6460ca025f6c"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from huggingface_hub import InferenceClient\n",
    "from lib.utils.util_stopwatch import Stopwatch\n",
    "from lib.agents.agent import Agent\n",
    "\n",
    "def query_llm_phind( prompt, model=Agent.PHIND_34B_v2, max_new_tokens=1024, temperature=0.25, top_k=10, top_p=0.9, debug=False, verbose=False, silent=True ):\n",
    "    \n",
    "    timer = Stopwatch( msg=f\"Asking LLM [{model}]...\".format( model ), silent=silent )\n",
    "    \n",
    "    # Get the TGI server URL for this context\n",
    "    # default_url    = self.config_mgr.get( \"tgi_server_codegen_url\", default=None )\n",
    "    # tgi_server_url = du.get_tgi_server_url_for_this_context( default_url=default_url )\n",
    "    \n",
    "    client         = InferenceClient( \"http://172.17.0.5:3000\" )\n",
    "    token_list     = [ ]\n",
    "    ellipsis_count = 0\n",
    "    \n",
    "    if debug:\n",
    "        for line in prompt.split( \"\\n\" ):\n",
    "            print( line )\n",
    "    \n",
    "    for token in client.text_generation(\n",
    "        prompt, max_new_tokens=max_new_tokens, stream=True, temperature=temperature, top_k=top_k, top_p=top_p, stop_sequences=[ \"</response>\" ]\n",
    "    ):\n",
    "        if debug:\n",
    "            print( token, end=\"\" )\n",
    "        else:\n",
    "            if not silent: print( \".\", end=\"\" )\n",
    "            ellipsis_count += 1\n",
    "            if ellipsis_count == 120:\n",
    "                ellipsis_count = 0\n",
    "                print()\n",
    "            \n",
    "        token_list.append( token )\n",
    "        \n",
    "    response = \"\".join( token_list ).strip()\n",
    "    \n",
    "    timer.print( msg=\"Done!\", use_millis=True, prepend_nl=True, end=\"\\n\" )\n",
    "    tokens_per_second = len( token_list ) / ( timer.get_delta_ms() / 1000.0 )\n",
    "    print( f\"Tokens per second [{round( tokens_per_second, 1 )}]\" )\n",
    "            \n",
    "    if debug:\n",
    "        print( f\"Token list length [{len( token_list )}]\" )\n",
    "        if verbose:\n",
    "            for line in response.split( \"\\n\" ):\n",
    "                print( line )\n",
    "    \n",
    "    \n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T18:27:25.005491Z",
     "start_time": "2023-12-19T18:27:24.975954Z"
    }
   },
   "id": "c34ef51ad77d4f42"
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "import openai\n",
    "from lib.utils.util_stopwatch import Stopwatch\n",
    "\n",
    "def query_llm_openai( messages, model=\"gpt-3.5-turbo-1106\", debug=False, verbose=False ):\n",
    "    \n",
    "    openai.api_key = du.get_api_key( \"openai\", project_root=\"/var/model/genie-in-the-box\" )\n",
    "    \n",
    "    timer = Stopwatch( msg=f\"Asking LLM [{model}]...\".format( model ) )\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=256,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0\n",
    "    )\n",
    "    \n",
    "    timer.print( \"Done!\", use_millis=True )\n",
    "    if debug and verbose:\n",
    "        # print( json.dumps( response.to_dict(), indent=4 ) )\n",
    "        print( response )\n",
    "    \n",
    "    return response.choices[ 0 ].message.content.strip()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T01:31:22.756566Z",
     "start_time": "2024-01-17T01:31:22.745276Z"
    }
   },
   "id": "e405fb4639f53174"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\r\n",
      "Version: 1.5.0\r\n",
      "Summary: The official Python library for the openai API\r\n",
      "Home-page: \r\n",
      "Author: \r\n",
      "Author-email: OpenAI <support@openai.com>\r\n",
      "License: \r\n",
      "Location: /usr/local/lib/python3.10/dist-packages\r\n",
      "Requires: anyio, distro, httpx, pydantic, sniffio, tqdm, typing-extensions\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show openai"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T18:21:27.518248Z",
     "start_time": "2023-12-19T18:21:25.804363Z"
    }
   },
   "id": "51a242bf736ad53b"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "call_counter = 0\n",
    "# rows         = df.shape[ 0 ]\n",
    "\n",
    "def get_response_to_question( prompt, rows ):\n",
    "    \n",
    "    global call_counter\n",
    "    \n",
    "    call_counter += 1\n",
    "    print( f\"On call [{call_counter:03d}] out of [{rows}] = [{round( call_counter / rows * 100.0, 1 )}%]... \", end=\"\" )\n",
    "    \n",
    "    return query_llm_phind( prompt )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:28:09.012038Z",
     "start_time": "2024-01-16T17:28:08.956798Z"
    }
   },
   "id": "c2b55c7f68001d05"
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "call_counter = 0\n",
    "\n",
    "def get_gpt_response_to_question( cell, rows, model=\"gpt-3.5-turbo-1106\" ):\n",
    "    \n",
    "    global call_counter    \n",
    "    call_counter += 1\n",
    "    \n",
    "    print( f\"On call [{call_counter:03d}] out of [{rows}] = [{round( call_counter / rows * 100.0, 1 )}%]... \", end=\"\" )\n",
    "    \n",
    "    return query_llm_openai( cell[ \"messages\" ], model=model )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T01:33:09.420395Z",
     "start_time": "2024-01-17T01:33:09.349179Z"
    }
   },
   "id": "d398e1e54dcd06e6"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "(120000, 11)"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:28:56.527250Z",
     "start_time": "2024-01-16T17:28:56.515672Z"
    }
   },
   "id": "d6a4435850065f6c"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "def print_stats( df ):\n",
    "\n",
    "    du.print_banner( \"Stats\" )\n",
    "    print( f\"               Is valid xml {df.response_xml_is_valid.mean() * 100:.1f}%\" )\n",
    "    print( f\"          Contains response {df.contains_response.mean() * 100:.1f}%\" )\n",
    "    print( f\"   Contains browser command {df.contains_browser_command.mean() * 100:.1f}%\" )\n",
    "    print( f\"              Contains args {df.contains_args.mean() * 100:.1f}%\" )\n",
    "    print( f\"          Response is exact {df.response_is_exact.mean() * 100:.1f}%\" )\n",
    "    print( f\"Response has correct values {df.response_has_correct_values.mean() * 100:.1f}%\" )\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:53:23.931169Z",
     "start_time": "2024-01-16T17:53:23.920391Z"
    }
   },
   "id": "dc726f722b6f687d"
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'role': 'system',\n  'content': \"INSTRUCTIONS:\\nYour job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\\n\\nYou will be given a human voice command as INPUT as well as a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\\n\\nRESPONSE FORMAT: MUST be returned wrapped in simple, well-formed XML\\n<response>\\n    <browser-command></browser-command>\\n    <args></args>\\n</response>\\n\"},\n {'role': 'user',\n  'content': 'Big Data analysis techniques: What techniques are essential for effective big data analysis? new tab search'},\n {'role': 'assistant',\n  'content': '\\n<response>\\n    <browser-command>search new tab</browser-command>\\n    <args>Big Data analysis techniques: What techniques are essential for effective big data analysis?</args>\\n</response>'}]"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_messages[ 0 ][ \"messages\" ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T01:32:50.839912Z",
     "start_time": "2024-01-17T01:32:50.787104Z"
    }
   },
   "id": "9719cdf3539ecb28"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120000 entries, 0 to 119999\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   instruction   120000 non-null  object\n",
      " 1   input         120000 non-null  object\n",
      " 2   output        120000 non-null  object\n",
      " 3   prompt        120000 non-null  object\n",
      " 4   gpt_messages  120000 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "qna_df = pd.DataFrame( { \"instruction\": instructions, \"input\": inputs, \"output\": outputs, \"prompt\": prompts, \"gpt_message\": gpt_messages } )\n",
    "qna_df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:39:43.473848Z",
     "start_time": "2024-01-16T18:39:43.371521Z"
    }
   },
   "id": "56733d5c866f4894"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sampled_df = qna_df[ [ \"instruction\", \"input\", \"output\", \"prompt\", \"gpt_message\" ] ].sample( 1000, random_state=42 ).copy()\n",
    "\n",
    "# Split the dataframe into train and (test+validate)\n",
    "train_df, test_validate_df = train_test_split( sampled_df, test_size=0.2, random_state=42 )\n",
    "\n",
    "# Then split (test+validate) into test and validate\n",
    "test_df, validate_df = train_test_split( test_validate_df, test_size=0.5, random_state=42 )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:39:46.112672Z",
     "start_time": "2024-01-16T18:39:46.083958Z"
    }
   },
   "id": "c1efe69ae319c998"
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 5)\n",
      "(100, 5)\n",
      "(100, 5)\n"
     ]
    }
   ],
   "source": [
    "print( train_df.shape )\n",
    "print( test_df.shape )\n",
    "print( validate_df.shape )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:39:48.819579Z",
     "start_time": "2024-01-16T18:39:48.804756Z"
    }
   },
   "id": "868f24f396c7505a"
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On call [001] out of [100] = [1.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 743 ms\n",
      "\n",
      "On call [002] out of [100] = [2.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 830 ms\n",
      "\n",
      "On call [003] out of [100] = [3.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 569 ms\n",
      "\n",
      "On call [004] out of [100] = [4.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 640 ms\n",
      "\n",
      "On call [005] out of [100] = [5.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 852 ms\n",
      "\n",
      "On call [006] out of [100] = [6.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 669 ms\n",
      "\n",
      "On call [007] out of [100] = [7.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 832 ms\n",
      "\n",
      "On call [008] out of [100] = [8.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 599 ms\n",
      "\n",
      "On call [009] out of [100] = [9.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 790 ms\n",
      "\n",
      "On call [010] out of [100] = [10.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 688 ms\n",
      "\n",
      "On call [011] out of [100] = [11.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 825 ms\n",
      "\n",
      "On call [012] out of [100] = [12.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 777 ms\n",
      "\n",
      "On call [013] out of [100] = [13.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 787 ms\n",
      "\n",
      "On call [014] out of [100] = [14.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 526 ms\n",
      "\n",
      "On call [015] out of [100] = [15.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 442 ms\n",
      "\n",
      "On call [016] out of [100] = [16.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 622 ms\n",
      "\n",
      "On call [017] out of [100] = [17.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 396 ms\n",
      "\n",
      "On call [018] out of [100] = [18.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 576 ms\n",
      "\n",
      "On call [019] out of [100] = [19.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 546 ms\n",
      "\n",
      "On call [020] out of [100] = [20.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 631 ms\n",
      "\n",
      "On call [021] out of [100] = [21.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 485 ms\n",
      "\n",
      "On call [022] out of [100] = [22.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 549 ms\n",
      "\n",
      "On call [023] out of [100] = [23.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 816 ms\n",
      "\n",
      "On call [024] out of [100] = [24.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 381 ms\n",
      "\n",
      "On call [025] out of [100] = [25.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 729 ms\n",
      "\n",
      "On call [026] out of [100] = [26.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 636 ms\n",
      "\n",
      "On call [027] out of [100] = [27.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 404 ms\n",
      "\n",
      "On call [028] out of [100] = [28.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 679 ms\n",
      "\n",
      "On call [029] out of [100] = [29.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 432 ms\n",
      "\n",
      "On call [030] out of [100] = [30.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 457 ms\n",
      "\n",
      "On call [031] out of [100] = [31.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 671 ms\n",
      "\n",
      "On call [032] out of [100] = [32.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 488 ms\n",
      "\n",
      "On call [033] out of [100] = [33.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 610 ms\n",
      "\n",
      "On call [034] out of [100] = [34.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 483 ms\n",
      "\n",
      "On call [035] out of [100] = [35.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 646 ms\n",
      "\n",
      "On call [036] out of [100] = [36.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 746 ms\n",
      "\n",
      "On call [037] out of [100] = [37.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 816 ms\n",
      "\n",
      "On call [038] out of [100] = [38.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 645 ms\n",
      "\n",
      "On call [039] out of [100] = [39.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 576 ms\n",
      "\n",
      "On call [040] out of [100] = [40.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 766 ms\n",
      "\n",
      "On call [041] out of [100] = [41.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 718 ms\n",
      "\n",
      "On call [042] out of [100] = [42.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 785 ms\n",
      "\n",
      "On call [043] out of [100] = [43.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 762 ms\n",
      "\n",
      "On call [044] out of [100] = [44.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 484 ms\n",
      "\n",
      "On call [045] out of [100] = [45.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 475 ms\n",
      "\n",
      "On call [046] out of [100] = [46.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 551 ms\n",
      "\n",
      "On call [047] out of [100] = [47.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 552 ms\n",
      "\n",
      "On call [048] out of [100] = [48.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 514 ms\n",
      "\n",
      "On call [049] out of [100] = [49.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 469 ms\n",
      "\n",
      "On call [050] out of [100] = [50.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 853 ms\n",
      "\n",
      "On call [051] out of [100] = [51.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 628 ms\n",
      "\n",
      "On call [052] out of [100] = [52.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 484 ms\n",
      "\n",
      "On call [053] out of [100] = [53.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 632 ms\n",
      "\n",
      "On call [054] out of [100] = [54.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 420 ms\n",
      "\n",
      "On call [055] out of [100] = [55.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 503 ms\n",
      "\n",
      "On call [056] out of [100] = [56.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 632 ms\n",
      "\n",
      "On call [057] out of [100] = [57.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 383 ms\n",
      "\n",
      "On call [058] out of [100] = [58.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 434 ms\n",
      "\n",
      "On call [059] out of [100] = [59.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 503 ms\n",
      "\n",
      "On call [060] out of [100] = [60.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 769 ms\n",
      "\n",
      "On call [061] out of [100] = [61.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 645 ms\n",
      "\n",
      "On call [062] out of [100] = [62.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 458 ms\n",
      "\n",
      "On call [063] out of [100] = [63.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 645 ms\n",
      "\n",
      "On call [064] out of [100] = [64.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 395 ms\n",
      "\n",
      "On call [065] out of [100] = [65.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 601 ms\n",
      "\n",
      "On call [066] out of [100] = [66.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 518 ms\n",
      "\n",
      "On call [067] out of [100] = [67.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 396 ms\n",
      "\n",
      "On call [068] out of [100] = [68.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 499 ms\n",
      "\n",
      "On call [069] out of [100] = [69.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 679 ms\n",
      "\n",
      "On call [070] out of [100] = [70.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 637 ms\n",
      "\n",
      "On call [071] out of [100] = [71.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 772 ms\n",
      "\n",
      "On call [072] out of [100] = [72.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 814 ms\n",
      "\n",
      "On call [073] out of [100] = [73.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 467 ms\n",
      "\n",
      "On call [074] out of [100] = [74.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 499 ms\n",
      "\n",
      "On call [075] out of [100] = [75.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 643 ms\n",
      "\n",
      "On call [076] out of [100] = [76.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 410 ms\n",
      "\n",
      "On call [077] out of [100] = [77.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 392 ms\n",
      "\n",
      "On call [078] out of [100] = [78.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 580 ms\n",
      "\n",
      "On call [079] out of [100] = [79.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 476 ms\n",
      "\n",
      "On call [080] out of [100] = [80.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 798 ms\n",
      "\n",
      "On call [081] out of [100] = [81.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 618 ms\n",
      "\n",
      "On call [082] out of [100] = [82.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 447 ms\n",
      "\n",
      "On call [083] out of [100] = [83.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 615 ms\n",
      "\n",
      "On call [084] out of [100] = [84.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 389 ms\n",
      "\n",
      "On call [085] out of [100] = [85.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 618 ms\n",
      "\n",
      "On call [086] out of [100] = [86.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 629 ms\n",
      "\n",
      "On call [087] out of [100] = [87.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 502 ms\n",
      "\n",
      "On call [088] out of [100] = [88.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 886 ms\n",
      "\n",
      "On call [089] out of [100] = [89.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 386 ms\n",
      "\n",
      "On call [090] out of [100] = [90.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 729 ms\n",
      "\n",
      "On call [091] out of [100] = [91.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 415 ms\n",
      "\n",
      "On call [092] out of [100] = [92.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 393 ms\n",
      "\n",
      "On call [093] out of [100] = [93.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 558 ms\n",
      "\n",
      "On call [094] out of [100] = [94.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 651 ms\n",
      "\n",
      "On call [095] out of [100] = [95.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 643 ms\n",
      "\n",
      "On call [096] out of [100] = [96.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 534 ms\n",
      "\n",
      "On call [097] out of [100] = [97.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 437 ms\n",
      "\n",
      "On call [098] out of [100] = [98.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 542 ms\n",
      "\n",
      "On call [099] out of [100] = [99.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 475 ms\n",
      "\n",
      "On call [100] out of [100] = [100.0%]... Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]...\n",
      "Asking LLM [ft:gpt-3.5-turbo-1106:deepily::8hmLrC7G]... Done! in 516 ms\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Stats\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "          Contains response 100.0%\n",
      "   Contains browser command 100.0%\n",
      "              Contains args 100.0%\n",
      "          Response is exact 57.0%\n",
      "Response has correct values 57.0%\n"
     ]
    }
   ],
   "source": [
    "validate_df = validate_responses( validate_df )\n",
    "\n",
    "print_stats( validate_df )\n",
    "\n",
    "# First run before attempting to force valid XML\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Stats: Phind\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 0.0%\n",
    "#           Contains response 98.0%\n",
    "#    Contains browser command 98.0%\n",
    "#               Contains args 98.0%\n",
    "#           Response is exact 0.0%\n",
    "# Response has correct values 43.0%\n",
    "\n",
    "# after attempting to force the first token output: <response>\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Stats: Phind\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 0.0%\n",
    "#           Contains response 100.0%\n",
    "#    Contains browser command 100.0%\n",
    "#               Contains args 100.0%\n",
    "#           Response is exact 0.0%\n",
    "# Response has correct values 39.0%\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Stats: GPT 3.5, before training\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#    Contains browser command 100.0%\n",
    "#               Contains args 100.0%\n",
    "#           Response is exact  88.0%\n",
    "# Response has correct values  88.0%\n",
    "# \n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Stats: GPT 3.5, after OVER training\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#    Contains browser command 100.0%\n",
    "#               Contains args 100.0%\n",
    "#           Response is exact 57.0%\n",
    "# Response has correct values 57.0%"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T01:34:21.033688Z",
     "start_time": "2024-01-17T01:33:21.707289Z"
    }
   },
   "id": "8ff6879acfe8210d"
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "data": {
      "text/plain": "65397     <response>\\n    <browser-command>search google...\n72078     <response>\\n    <browser-command>search google...\n59206     <response>\\n    <browser-command>search google...\n48642     <response>\\n    <browser-command>search google...\n114768    <response>\\n    <browser-command>search google...\nName: response, dtype: object"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df.response.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T01:36:39.026478Z",
     "start_time": "2024-01-17T01:36:38.979540Z"
    }
   },
   "id": "f5411f1724fdf5ae"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 80 entries, 32114 to 10744\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   instruction   80 non-null     object\n",
      " 1   input         80 non-null     object\n",
      " 2   output        80 non-null     object\n",
      " 3   prompt        80 non-null     object\n",
      " 4   gpt_messages  80 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 3.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:01:08.370748Z",
     "start_time": "2024-01-16T18:01:08.361117Z"
    }
   },
   "id": "31e8d6c98720256"
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "sampled_df = qna_df[ [ \"instruction\", \"input\", \"output\", \"prompt\", \"gpt_message\" ] ].sample( 1000, random_state=42 ).copy()\n",
    "\n",
    "# Split the dataframe into 80% train and (test+validate)\n",
    "train_df, test_validate_df = train_test_split( sampled_df, test_size=0.2, random_state=42 )\n",
    "\n",
    "# Then split (test+validate) into 10% test and 10% validate\n",
    "test_df, validate_df = train_test_split( test_validate_df, test_size=0.5, random_state=42 )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:40:04.444143Z",
     "start_time": "2024-01-16T18:40:04.386663Z"
    }
   },
   "id": "c3ab6816ce6104f1"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/var/genie-in-the-box/src/ephemera/prompts/data'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[115], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m project_root \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/var/model/genie-in-the-box\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      2\u001B[0m path \u001B[38;5;241m=\u001B[39m du\u001B[38;5;241m.\u001B[39mget_project_root() \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/src/ephemera/prompts/data/search-xml-train.jsonl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mtrain_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_json\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morient\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrecords\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlines\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m os\u001B[38;5;241m.\u001B[39mchmod( path, \u001B[38;5;241m0o666\u001B[39m )\n\u001B[1;32m      6\u001B[0m path \u001B[38;5;241m=\u001B[39m du\u001B[38;5;241m.\u001B[39mget_project_root() \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/src/ephemera/prompts/data/search-xml-validate.jsonl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py:2629\u001B[0m, in \u001B[0;36mNDFrame.to_json\u001B[0;34m(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)\u001B[0m\n\u001B[1;32m   2626\u001B[0m config\u001B[38;5;241m.\u001B[39mis_nonnegative_int(indent)\n\u001B[1;32m   2627\u001B[0m indent \u001B[38;5;241m=\u001B[39m indent \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 2629\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_json\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2630\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2631\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2632\u001B[0m \u001B[43m    \u001B[49m\u001B[43morient\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morient\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2633\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdate_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2634\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdouble_precision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdouble_precision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2635\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_ascii\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_ascii\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2636\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdate_unit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_unit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2637\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdefault_handler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_handler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2638\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlines\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlines\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2639\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2640\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2641\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2642\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2643\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2644\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py:212\u001B[0m, in \u001B[0;36mto_json\u001B[0;34m(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)\u001B[0m\n\u001B[1;32m    208\u001B[0m     s \u001B[38;5;241m=\u001B[39m convert_to_line_delimits(s)\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path_or_buf \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    211\u001B[0m     \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n\u001B[0;32m--> 212\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    213\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[1;32m    215\u001B[0m         handles\u001B[38;5;241m.\u001B[39mhandle\u001B[38;5;241m.\u001B[39mwrite(s)\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py:739\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    737\u001B[0m \u001B[38;5;66;03m# Only for write methods\u001B[39;00m\n\u001B[1;32m    738\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m is_path:\n\u001B[0;32m--> 739\u001B[0m     \u001B[43mcheck_parent_directory\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    741\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compression:\n\u001B[1;32m    742\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m compression \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzstd\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    743\u001B[0m         \u001B[38;5;66;03m# compression libraries do not like an explicit text-mode\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py:604\u001B[0m, in \u001B[0;36mcheck_parent_directory\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    602\u001B[0m parent \u001B[38;5;241m=\u001B[39m Path(path)\u001B[38;5;241m.\u001B[39mparent\n\u001B[1;32m    603\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parent\u001B[38;5;241m.\u001B[39mis_dir():\n\u001B[0;32m--> 604\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124mrf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot save file into a non-existent directory: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mOSError\u001B[0m: Cannot save file into a non-existent directory: '/var/genie-in-the-box/src/ephemera/prompts/data'"
     ]
    }
   ],
   "source": [
    "project_root = \"/var/model/genie-in-the-box\"\n",
    "path = du.get_project_root() + \"/src/ephemera/prompts/data/search-xml-train.jsonl\"\n",
    "train_df.to_json( path, orient=\"records\", lines=True )\n",
    "os.chmod( path, 0o666 )\n",
    "\n",
    "path = du.get_project_root() + \"/src/ephemera/prompts/data/search-xml-test.jsonl\"\n",
    "test_df.to_json( path, orient=\"records\", lines=True )\n",
    "os.chmod( path, 0o666 )\n",
    "\n",
    "path = du.get_project_root() + \"/src/ephemera/prompts/data/search-xml-validate.jsonl\"\n",
    "validate_df.to_json( path, orient=\"records\", lines=True )\n",
    "os.chmod( path, 0o666 )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:26:04.842104Z",
     "start_time": "2024-01-16T18:26:04.772072Z"
    }
   },
   "id": "20d7308b41938b39"
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "path = project_root + \"/src/ephemera/prompts/data/search-xml-train-gpt.jsonl\"\n",
    "train_df.gpt_messages.to_json( path, orient=\"records\", lines=True )\n",
    "os.chmod( path, 0o666 )\n",
    "\n",
    "path = project_root + \"/src/ephemera/prompts/data/search-xml-test-gpt.jsonl\"\n",
    "test_df.gpt_messages.to_json( path, orient=\"records\", lines=True )\n",
    "os.chmod( path, 0o666 )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:43:43.897333Z",
     "start_time": "2024-01-16T18:43:43.831279Z"
    }
   },
   "id": "641d9378e6ea3044"
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "data": {
      "text/plain": "61754     {'messages': [{'role': 'system', 'content': 'I...\n108836    {'messages': [{'role': 'system', 'content': 'I...\n112152    {'messages': [{'role': 'system', 'content': 'I...\n75196     {'messages': [{'role': 'system', 'content': 'I...\n7770      {'messages': [{'role': 'system', 'content': 'I...\nName: gpt_messages, dtype: object"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.gpt_messages.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:53:52.783709Z",
     "start_time": "2024-01-16T18:53:52.759257Z"
    }
   },
   "id": "40448bc22d1becc1"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 32M\r\n",
      "drwxr--r-- 4 1001 1001 4.0K Jan 16 18:26 .\r\n",
      "drwxr--r-- 4 1001 1001 4.0K Jan 11 04:58 ..\r\n",
      "-rwxr--r-- 1 1001 1001 6.1K Jun 20  2023 .DS_Store\r\n",
      "-rwxr--r-- 1 1001 1001 4.0K Jun 20  2023 ._.DS_Store\r\n",
      "-rw-rw-r-- 1 1001 1001    0 Oct 13 18:48 __init__.py\r\n",
      "drwxrwxr-x 2 1001 1001 4.0K Oct 13 18:48 fine-tuning-results\r\n",
      "drwxrwxr-x 2 1001 1001 4.0K Oct 13 18:48 jsonl\r\n",
      "-rw-rw-r-- 1 1001 1001 1.6K Oct 13 18:48 munger.py\r\n",
      "-rw-rw-r-- 1 1001 1001  36K Dec 18 19:16 search-terms.txt\r\n",
      "-rw-rw-rw- 1 root root 583K Jan 16 18:31 search-xml-train-gpt.jsonl\r\n",
      "-rw-rw-r-- 1 1001 1001 7.0K Oct 13 18:48 synthetic-data-load-url-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 7.3K Oct 13 18:48 synthetic-data-load-url-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  13K Oct 13 18:48 synthetic-data-none-of-the-above.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 8.5K Dec 17 00:42 synthetic-data-search-google-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  11K Oct 13 18:48 synthetic-data-search-google-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 8.5K Oct 13 18:48 synthetic-data-search-google-scholar-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  11K Oct 13 18:48 synthetic-data-search-google-scholar-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 7.6K Oct 13 18:48 synthetic-data-search-in-current-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001 8.9K Dec 17 02:09 synthetic-data-search-in-new-tab.txt\r\n",
      "-rw-rw-r-- 1 1001 1001  723 Oct 13 18:48 training-commands.map\r\n",
      "-rw-rw-rw- 1 1001 1001 3.1M Dec 21 22:34 voice-commands-xml-test.jsonl\r\n",
      "-rw-rw-rw- 1 1001 1001  25M Dec 21 20:33 voice-commands-xml-train.jsonl\r\n",
      "-rw-rw-rw- 1 1001 1001 3.1M Dec 21 22:34 voice-commands-xml-validate.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "! ls -alh /var/model/genie-in-the-box/src/ephemera/prompts/data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T18:31:59.514393Z",
     "start_time": "2024-01-16T18:31:59.364614Z"
    }
   },
   "id": "28dd26783fccfbaf"
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float16"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "getattr(torch, \"float16\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T22:38:44.204450Z",
     "start_time": "2024-01-16T22:38:42.648719Z"
    }
   },
   "id": "72ddbb19f1dd5c23"
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "def validate_responses( df ):\n",
    "    \n",
    "    rows = df.shape[ 0 ]\n",
    "    global call_counter\n",
    "    call_counter = 0\n",
    "    \n",
    "    # df[ \"response\" ]                    = df[ \"prompt\" ].apply( lambda cell: get_response_to_question( cell, rows ) )\n",
    "    df[ \"response\" ]                    = df[ \"gpt_message\" ].apply( lambda cell: get_gpt_response_to_question( cell, rows ) )\n",
    "    \n",
    "    # Validate the structure and content of the xml response\n",
    "    df[ \"response_xml_is_valid\" ]       = df[ \"response\" ].apply( lambda cell: is_valid_xml( cell, schema ) )\n",
    "    df[ \"contains_response\" ]           = df[ \"response\" ].apply( lambda cell: contains_valid_xml_tag( cell, \"response\" ) )\n",
    "    df[ \"contains_browser_command\" ]    = df[ \"response\" ].apply( lambda cell: contains_valid_xml_tag( cell, \"browser-command\" ) )\n",
    "    df[ \"contains_args\" ]               = df[ \"response\" ].apply( lambda cell: contains_valid_xml_tag( cell, \"args\" ) )\n",
    "    df[ \"response_is_exact\" ]           = df.apply( lambda row: is_response_exact( row[ \"response\" ], row[ \"output\" ] ), axis=1 )\n",
    "    df[ \"response_has_correct_values\" ] = df.apply( lambda row: contains_correct_response_values( row[ \"response\" ], row[ \"output\" ] ), axis=1 )\n",
    "    \n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-17T01:33:12.883146Z"
    }
   },
   "id": "2aa023022ebbaa6a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'/var/model/genie-in-the-box/src'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir( \"/var/model/genie-in-the-box/src\" )\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T16:04:47.153274Z",
     "start_time": "2024-01-29T16:04:47.089336Z"
    }
   },
   "id": "1ab93c26c44ea720"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from lib.utils.util_stopwatch import Stopwatch\n",
    "from ephemera.prompts.xml_fine_tuning_prompt_generator import XmlFineTuningPromptGenerator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T16:05:45.839427Z",
     "start_time": "2024-01-29T16:05:44.708769Z"
    }
   },
   "id": "87d38127f5eb8029"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands file for command [go to current tab] exists: True\n",
      "Commands file for command [go to new tab] exists: True\n",
      "Commands file for command [search google current tab] exists: True\n",
      "Commands file for command [search google new tab] exists: True\n",
      "Commands file for command [search google scholar current tab] exists: True\n",
      "Commands file for command [search google scholar new tab] exists: True\n",
      "Commands file for command [search current tab] exists: True\n",
      "Commands file for command [search new tab] exists: True\n",
      "Commands file for command [search perplexity current tab] exists: True\n",
      "Commands file for command [search perplexity new tab] exists: True\n",
      "Commands file for command [search phind current tab] exists: True\n",
      "Commands file for command [search phind new tab] exists: True\n",
      "\n",
      "Commands file for command [search using clipboard current tab] exists: True\n",
      "Commands file for command [search using clipboard new tab] exists: True\n",
      "Commands file for command [search google using clipboard current tab] exists: True\n",
      "Commands file for command [search google using clipboard new tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard current tab] exists: True\n",
      "Commands file for command [search google scholar using clipboard new tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard current tab] exists: True\n",
      "Commands file for command [search perplexity using clipboard new tab] exists: True\n",
      "Commands file for command [search phind using clipboard current tab] exists: True\n",
      "Commands file for command [search phind using clipboard new tab] exists: True\n",
      "Commands file for command [none] exists: True\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound command [go to current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound command [go to new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound command [search google current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound command [search google new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound command [search google scholar current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound command [search google scholar new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound command [search current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound command [search new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound command [search perplexity current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound command [search perplexity new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound command [search phind current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for compound command [search phind new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Pruning potential duplicates by 'input' values...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      " PRE 200,100 training inputs...\n",
      "POST 193,682 training inputs. Deleted 6,418 rows = 3.2% duplicate questions\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple command [search using clipboard current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple command [search using clipboard new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple command [search google using clipboard current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple command [search google using clipboard new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple command [search google scholar using clipboard current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple command [search google scholar using clipboard new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple command [search perplexity using clipboard current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple command [search perplexity using clipboard new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple command [search phind using clipboard current tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple command [search phind using clipboard new tab]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Building prompts for simple command [none]\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "....................\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Pruning potential duplicates by 'input' values...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      " PRE 2,200 training inputs...\n",
      "POST 2,075 training inputs. Deleted 125 rows = 5.7% duplicate questions\n",
      "WARNING: Sample size [2,200] > rows_post [2,075]. Returning all [2,075] rows.\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Command counts for all 14,075 training prompts\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "                                              command  input\n",
      "0                                   go to current tab   1248\n",
      "1                                       go to new tab   1216\n",
      "2                                                none    176\n",
      "3                                  search current tab   1158\n",
      "4                           search google current tab   1133\n",
      "5                               search google new tab   1167\n",
      "6                   search google scholar current tab   1236\n",
      "7                       search google scholar new tab   1236\n",
      "8   search google scholar using clipboard current tab    199\n",
      "9       search google scholar using clipboard new tab    104\n",
      "10          search google using clipboard current tab    198\n",
      "11              search google using clipboard new tab    200\n",
      "12                                     search new tab   1178\n",
      "13                      search perplexity current tab    601\n",
      "14                          search perplexity new tab    598\n",
      "15      search perplexity using clipboard current tab    199\n",
      "16          search perplexity using clipboard new tab    200\n",
      "17                           search phind current tab    595\n",
      "18                               search phind new tab    634\n",
      "19           search phind using clipboard current tab    200\n",
      "20               search phind using clipboard new tab    200\n",
      "21                 search using clipboard current tab    199\n",
      "22                     search using clipboard new tab    200\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Max, min, and mean prompt CHARACTER counts for all 14,075 training prompts\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Max  prompt length [2,869] characters\n",
      "Min  prompt length [2,694] characters\n",
      "Mean prompt length [2,748.7] characters\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Max, min, and mean prompt WORD counts for all 14,075 training prompts\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Max  prompt length [666] words\n",
      "Min  prompt length [633] words\n",
      "Mean prompt length [642.0] words\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "path_prefix = \"/var/model/genie-in-the-box\"\n",
    "\n",
    "xml_ftp_generator     = XmlFineTuningPromptGenerator( path_prefix=path_prefix, tgi_url=\"http://172.17.0.4:3000\", debug=False )\n",
    "all_qna_df            = xml_ftp_generator.build_all_training_prompts()\n",
    "\n",
    "# for line in compound_qna_df.prompt[ 0 ].split( \"\\n\" ): print( line )\n",
    "# for line in simple_command_qna_df.prompt[ 0 ].split( \"\\n\" ): print( line )\n",
    "# for line in all_qna_df.prompt[ 0 ].split( \"\\n\" ): print( line )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T16:10:40.457643Z",
     "start_time": "2024-01-29T16:10:37.816088Z"
    }
   },
   "id": "c6506f6601a812be"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "path_prefix = \"/var/model/genie-in-the-box\"\n",
    "xml_ftp_generator     = XmlFineTuningPromptGenerator( path_prefix=path_prefix, tgi_url=\"http://172.17.0.4:3000\", debug=False, silent=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T16:10:11.178289Z",
     "start_time": "2024-01-29T16:10:11.012298Z"
    }
   },
   "id": "ed0548558d410b92"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Writing train, test, validate splits to jsonl...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "   train_df.shape: 11,260 x 6\n",
      "    test_df.shape: 1,407 x 6\n",
      "validate_df.shape: 1,408 x 6\n",
      "Generating responses for 1,408 rows...\n",
      "Using TGI w/ model_name [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "Processing call [001] out of [1408] = [0.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [002] out of [1408] = [0.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [003] out of [1408] = [0.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [004] out of [1408] = [0.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [005] out of [1408] = [0.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 666 ms\n",
      "Tokens per second [85.6]\n",
      "Processing call [006] out of [1408] = [0.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 496 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [007] out of [1408] = [0.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 556 ms\n",
      "Tokens per second [82.7]\n",
      "Processing call [008] out of [1408] = [0.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [009] out of [1408] = [0.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 545 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [010] out of [1408] = [0.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [011] out of [1408] = [0.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [012] out of [1408] = [0.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 563 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [013] out of [1408] = [0.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [014] out of [1408] = [1.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [015] out of [1408] = [1.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 460 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [016] out of [1408] = [1.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [017] out of [1408] = [1.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [018] out of [1408] = [1.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [019] out of [1408] = [1.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [020] out of [1408] = [1.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [021] out of [1408] = [1.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [022] out of [1408] = [1.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [023] out of [1408] = [1.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 573 ms\n",
      "Tokens per second [83.8]\n",
      "Processing call [024] out of [1408] = [1.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 621 ms\n",
      "Tokens per second [85.3]\n",
      "Processing call [025] out of [1408] = [1.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 497 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [026] out of [1408] = [1.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 618 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [027] out of [1408] = [1.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [028] out of [1408] = [2.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [029] out of [1408] = [2.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 456 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [030] out of [1408] = [2.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [031] out of [1408] = [2.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 614 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [032] out of [1408] = [2.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [82.7]\n",
      "Processing call [033] out of [1408] = [2.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 734 ms\n",
      "Tokens per second [87.2]\n",
      "Processing call [034] out of [1408] = [2.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [035] out of [1408] = [2.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [036] out of [1408] = [2.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [037] out of [1408] = [2.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [038] out of [1408] = [2.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [039] out of [1408] = [2.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [040] out of [1408] = [2.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [041] out of [1408] = [2.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [042] out of [1408] = [3.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [043] out of [1408] = [3.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [044] out of [1408] = [3.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [045] out of [1408] = [3.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [046] out of [1408] = [3.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [047] out of [1408] = [3.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [048] out of [1408] = [3.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 449 ms\n",
      "Tokens per second [78.0]\n",
      "Processing call [049] out of [1408] = [3.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 507 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [050] out of [1408] = [3.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 661 ms\n",
      "Tokens per second [86.2]\n",
      "Processing call [051] out of [1408] = [3.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 497 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [052] out of [1408] = [3.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [053] out of [1408] = [3.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [82.5]\n",
      "Processing call [054] out of [1408] = [3.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [055] out of [1408] = [3.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 610 ms\n",
      "Tokens per second [85.2]\n",
      "Processing call [056] out of [1408] = [4.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 615 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [057] out of [1408] = [4.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [058] out of [1408] = [4.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [059] out of [1408] = [4.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [060] out of [1408] = [4.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [061] out of [1408] = [4.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [062] out of [1408] = [4.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 608 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [063] out of [1408] = [4.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 593 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [064] out of [1408] = [4.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [065] out of [1408] = [4.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [066] out of [1408] = [4.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [067] out of [1408] = [4.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [068] out of [1408] = [4.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [069] out of [1408] = [4.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [070] out of [1408] = [5.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [071] out of [1408] = [5.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 548 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [072] out of [1408] = [5.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [073] out of [1408] = [5.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [074] out of [1408] = [5.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [075] out of [1408] = [5.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [076] out of [1408] = [5.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [077] out of [1408] = [5.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 401 ms\n",
      "Tokens per second [74.8]\n",
      "Processing call [078] out of [1408] = [5.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 644 ms\n",
      "Tokens per second [85.4]\n",
      "Processing call [079] out of [1408] = [5.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [080] out of [1408] = [5.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 615 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [081] out of [1408] = [5.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [082] out of [1408] = [5.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [083] out of [1408] = [5.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [084] out of [1408] = [6.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [085] out of [1408] = [6.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 461 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [086] out of [1408] = [6.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [087] out of [1408] = [6.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [088] out of [1408] = [6.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [089] out of [1408] = [6.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 546 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [090] out of [1408] = [6.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 601 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [091] out of [1408] = [6.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [092] out of [1408] = [6.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 485 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [093] out of [1408] = [6.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 612 ms\n",
      "Tokens per second [85.0]\n",
      "Processing call [094] out of [1408] = [6.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [095] out of [1408] = [6.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [096] out of [1408] = [6.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 628 ms\n",
      "Tokens per second [86.0]\n",
      "Processing call [097] out of [1408] = [6.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 612 ms\n",
      "Tokens per second [85.0]\n",
      "Processing call [098] out of [1408] = [7.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [099] out of [1408] = [7.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [100] out of [1408] = [7.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 448 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [101] out of [1408] = [7.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [102] out of [1408] = [7.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [103] out of [1408] = [7.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 541 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [104] out of [1408] = [7.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [105] out of [1408] = [7.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [106] out of [1408] = [7.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [107] out of [1408] = [7.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [108] out of [1408] = [7.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [109] out of [1408] = [7.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [110] out of [1408] = [7.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 404 ms\n",
      "Tokens per second [74.3]\n",
      "Processing call [111] out of [1408] = [7.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [77.1]\n",
      "Processing call [112] out of [1408] = [8.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [113] out of [1408] = [8.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 615 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [114] out of [1408] = [8.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [77.5]\n",
      "Processing call [115] out of [1408] = [8.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 559 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [116] out of [1408] = [8.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [117] out of [1408] = [8.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 617 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [118] out of [1408] = [8.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [76.3]\n",
      "Processing call [119] out of [1408] = [8.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [120] out of [1408] = [8.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [121] out of [1408] = [8.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [122] out of [1408] = [8.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 606 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [123] out of [1408] = [8.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [124] out of [1408] = [8.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [125] out of [1408] = [8.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [126] out of [1408] = [8.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [127] out of [1408] = [9.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [128] out of [1408] = [9.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 601 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [129] out of [1408] = [9.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [130] out of [1408] = [9.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [131] out of [1408] = [9.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 558 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [132] out of [1408] = [9.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [75.8]\n",
      "Processing call [133] out of [1408] = [9.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [134] out of [1408] = [9.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [135] out of [1408] = [9.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [136] out of [1408] = [9.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 553 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [137] out of [1408] = [9.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [138] out of [1408] = [9.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [139] out of [1408] = [9.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [77.1]\n",
      "Processing call [140] out of [1408] = [9.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 552 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [141] out of [1408] = [10.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [142] out of [1408] = [10.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 643 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [143] out of [1408] = [10.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [144] out of [1408] = [10.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 542 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [145] out of [1408] = [10.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 613 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [146] out of [1408] = [10.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 611 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [147] out of [1408] = [10.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [148] out of [1408] = [10.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [149] out of [1408] = [10.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [150] out of [1408] = [10.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [151] out of [1408] = [10.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [152] out of [1408] = [10.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [153] out of [1408] = [10.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [154] out of [1408] = [10.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 637 ms\n",
      "Tokens per second [84.8]\n",
      "Processing call [155] out of [1408] = [11.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [156] out of [1408] = [11.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 580 ms\n",
      "Tokens per second [82.8]\n",
      "Processing call [157] out of [1408] = [11.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 553 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [158] out of [1408] = [11.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [159] out of [1408] = [11.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 597 ms\n",
      "Tokens per second [83.8]\n",
      "Processing call [160] out of [1408] = [11.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [161] out of [1408] = [11.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [162] out of [1408] = [11.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 628 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [163] out of [1408] = [11.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 490 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [164] out of [1408] = [11.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [165] out of [1408] = [11.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [166] out of [1408] = [11.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [167] out of [1408] = [11.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [168] out of [1408] = [11.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [169] out of [1408] = [12.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 449 ms\n",
      "Tokens per second [78.0]\n",
      "Processing call [170] out of [1408] = [12.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [171] out of [1408] = [12.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [172] out of [1408] = [12.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 634 ms\n",
      "Tokens per second [85.2]\n",
      "Processing call [173] out of [1408] = [12.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [174] out of [1408] = [12.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [175] out of [1408] = [12.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 649 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [176] out of [1408] = [12.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 450 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [177] out of [1408] = [12.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 594 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [178] out of [1408] = [12.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 633 ms\n",
      "Tokens per second [85.3]\n",
      "Processing call [179] out of [1408] = [12.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [180] out of [1408] = [12.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [181] out of [1408] = [12.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 557 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [182] out of [1408] = [12.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [183] out of [1408] = [13.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [184] out of [1408] = [13.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 616 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [185] out of [1408] = [13.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 496 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [186] out of [1408] = [13.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [187] out of [1408] = [13.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [188] out of [1408] = [13.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [189] out of [1408] = [13.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [190] out of [1408] = [13.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [191] out of [1408] = [13.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [192] out of [1408] = [13.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 449 ms\n",
      "Tokens per second [78.0]\n",
      "Processing call [193] out of [1408] = [13.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [194] out of [1408] = [13.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [195] out of [1408] = [13.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 622 ms\n",
      "Tokens per second [85.2]\n",
      "Processing call [196] out of [1408] = [13.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 703 ms\n",
      "Tokens per second [86.8]\n",
      "Processing call [197] out of [1408] = [14.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 496 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [198] out of [1408] = [14.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [199] out of [1408] = [14.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 645 ms\n",
      "Tokens per second [85.3]\n",
      "Processing call [200] out of [1408] = [14.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [201] out of [1408] = [14.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [202] out of [1408] = [14.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [203] out of [1408] = [14.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [204] out of [1408] = [14.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [205] out of [1408] = [14.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [206] out of [1408] = [14.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [207] out of [1408] = [14.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [208] out of [1408] = [14.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [209] out of [1408] = [14.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 496 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [210] out of [1408] = [14.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [211] out of [1408] = [15.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [212] out of [1408] = [15.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [213] out of [1408] = [15.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [214] out of [1408] = [15.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [215] out of [1408] = [15.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 500 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [216] out of [1408] = [15.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 623 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [217] out of [1408] = [15.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [218] out of [1408] = [15.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [219] out of [1408] = [15.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 541 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [220] out of [1408] = [15.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [221] out of [1408] = [15.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 448 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [222] out of [1408] = [15.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [223] out of [1408] = [15.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [224] out of [1408] = [15.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 545 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [225] out of [1408] = [16.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [226] out of [1408] = [16.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 545 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [227] out of [1408] = [16.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [228] out of [1408] = [16.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [229] out of [1408] = [16.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 500 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [230] out of [1408] = [16.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [231] out of [1408] = [16.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 551 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [232] out of [1408] = [16.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [233] out of [1408] = [16.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [234] out of [1408] = [16.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 646 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [235] out of [1408] = [16.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [236] out of [1408] = [16.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [237] out of [1408] = [16.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [238] out of [1408] = [16.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [239] out of [1408] = [17.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 490 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [240] out of [1408] = [17.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 555 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [241] out of [1408] = [17.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [242] out of [1408] = [17.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [243] out of [1408] = [17.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [244] out of [1408] = [17.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 545 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [245] out of [1408] = [17.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 590 ms\n",
      "Tokens per second [83.1]\n",
      "Processing call [246] out of [1408] = [17.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [247] out of [1408] = [17.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 600 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [248] out of [1408] = [17.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [249] out of [1408] = [17.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [250] out of [1408] = [17.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [251] out of [1408] = [17.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [252] out of [1408] = [17.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [253] out of [1408] = [18.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [254] out of [1408] = [18.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [255] out of [1408] = [18.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 460 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [256] out of [1408] = [18.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [257] out of [1408] = [18.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [258] out of [1408] = [18.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [259] out of [1408] = [18.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 642 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [260] out of [1408] = [18.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [261] out of [1408] = [18.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [262] out of [1408] = [18.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 631 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [263] out of [1408] = [18.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [264] out of [1408] = [18.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [265] out of [1408] = [18.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [266] out of [1408] = [18.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [267] out of [1408] = [19.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [268] out of [1408] = [19.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 496 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [269] out of [1408] = [19.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [270] out of [1408] = [19.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 625 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [271] out of [1408] = [19.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [272] out of [1408] = [19.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [273] out of [1408] = [19.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [274] out of [1408] = [19.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [275] out of [1408] = [19.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 402 ms\n",
      "Tokens per second [74.6]\n",
      "Processing call [276] out of [1408] = [19.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [277] out of [1408] = [19.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [278] out of [1408] = [19.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [279] out of [1408] = [19.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [280] out of [1408] = [19.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 461 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [281] out of [1408] = [20.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 609 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [282] out of [1408] = [20.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [283] out of [1408] = [20.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [284] out of [1408] = [20.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [285] out of [1408] = [20.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 456 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [286] out of [1408] = [20.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [287] out of [1408] = [20.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [288] out of [1408] = [20.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 605 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [289] out of [1408] = [20.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 497 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [290] out of [1408] = [20.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [291] out of [1408] = [20.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [292] out of [1408] = [20.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 610 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [293] out of [1408] = [20.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [294] out of [1408] = [20.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [295] out of [1408] = [21.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [296] out of [1408] = [21.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [297] out of [1408] = [21.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [298] out of [1408] = [21.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [299] out of [1408] = [21.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 450 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [300] out of [1408] = [21.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [301] out of [1408] = [21.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [302] out of [1408] = [21.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [303] out of [1408] = [21.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [304] out of [1408] = [21.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 645 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [305] out of [1408] = [21.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [306] out of [1408] = [21.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 644 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [307] out of [1408] = [21.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 405 ms\n",
      "Tokens per second [74.1]\n",
      "Processing call [308] out of [1408] = [21.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [309] out of [1408] = [21.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [310] out of [1408] = [22.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [311] out of [1408] = [22.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 599 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [312] out of [1408] = [22.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 449 ms\n",
      "Tokens per second [78.0]\n",
      "Processing call [313] out of [1408] = [22.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [314] out of [1408] = [22.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 637 ms\n",
      "Tokens per second [84.8]\n",
      "Processing call [315] out of [1408] = [22.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 588 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [316] out of [1408] = [22.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [317] out of [1408] = [22.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [318] out of [1408] = [22.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [319] out of [1408] = [22.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [320] out of [1408] = [22.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [321] out of [1408] = [22.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [322] out of [1408] = [22.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 629 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [323] out of [1408] = [22.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [324] out of [1408] = [23.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 550 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [325] out of [1408] = [23.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 616 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [326] out of [1408] = [23.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 451 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [327] out of [1408] = [23.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [328] out of [1408] = [23.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [329] out of [1408] = [23.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [330] out of [1408] = [23.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 500 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [331] out of [1408] = [23.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [332] out of [1408] = [23.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [333] out of [1408] = [23.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [334] out of [1408] = [23.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 490 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [335] out of [1408] = [23.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [336] out of [1408] = [23.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [337] out of [1408] = [23.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [338] out of [1408] = [24.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 652 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [339] out of [1408] = [24.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [340] out of [1408] = [24.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [341] out of [1408] = [24.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [342] out of [1408] = [24.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 602 ms\n",
      "Tokens per second [83.1]\n",
      "Processing call [343] out of [1408] = [24.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [344] out of [1408] = [24.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [345] out of [1408] = [24.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 619 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [346] out of [1408] = [24.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 551 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [347] out of [1408] = [24.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [348] out of [1408] = [24.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [349] out of [1408] = [24.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [350] out of [1408] = [24.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [351] out of [1408] = [24.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 460 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [352] out of [1408] = [25.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [353] out of [1408] = [25.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [354] out of [1408] = [25.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [355] out of [1408] = [25.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 631 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [356] out of [1408] = [25.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 592 ms\n",
      "Tokens per second [82.8]\n",
      "Processing call [357] out of [1408] = [25.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [358] out of [1408] = [25.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [359] out of [1408] = [25.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [360] out of [1408] = [25.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 542 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [361] out of [1408] = [25.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [362] out of [1408] = [25.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [363] out of [1408] = [25.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [364] out of [1408] = [25.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [365] out of [1408] = [25.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 550 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [366] out of [1408] = [26.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [367] out of [1408] = [26.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [368] out of [1408] = [26.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [369] out of [1408] = [26.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 624 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [370] out of [1408] = [26.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [371] out of [1408] = [26.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [372] out of [1408] = [26.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [373] out of [1408] = [26.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [374] out of [1408] = [26.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [375] out of [1408] = [26.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [376] out of [1408] = [26.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 657 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [377] out of [1408] = [26.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 584 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [378] out of [1408] = [26.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [379] out of [1408] = [26.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [380] out of [1408] = [27.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [381] out of [1408] = [27.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [382] out of [1408] = [27.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [383] out of [1408] = [27.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [384] out of [1408] = [27.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 596 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [385] out of [1408] = [27.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [386] out of [1408] = [27.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [387] out of [1408] = [27.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [388] out of [1408] = [27.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 638 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [389] out of [1408] = [27.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 500 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [390] out of [1408] = [27.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [391] out of [1408] = [27.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [392] out of [1408] = [27.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 549 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [393] out of [1408] = [27.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 400 ms\n",
      "Tokens per second [75.0]\n",
      "Processing call [394] out of [1408] = [28.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [395] out of [1408] = [28.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [396] out of [1408] = [28.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [397] out of [1408] = [28.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [398] out of [1408] = [28.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 628 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [399] out of [1408] = [28.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [400] out of [1408] = [28.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [401] out of [1408] = [28.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [402] out of [1408] = [28.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [403] out of [1408] = [28.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [404] out of [1408] = [28.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [405] out of [1408] = [28.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [406] out of [1408] = [28.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [407] out of [1408] = [28.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [408] out of [1408] = [29.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [409] out of [1408] = [29.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [410] out of [1408] = [29.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [411] out of [1408] = [29.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [412] out of [1408] = [29.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 606 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [413] out of [1408] = [29.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [414] out of [1408] = [29.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [415] out of [1408] = [29.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [416] out of [1408] = [29.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 490 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [417] out of [1408] = [29.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [418] out of [1408] = [29.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [419] out of [1408] = [29.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 460 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [420] out of [1408] = [29.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [421] out of [1408] = [29.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 619 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [422] out of [1408] = [30.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [423] out of [1408] = [30.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 451 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [424] out of [1408] = [30.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [425] out of [1408] = [30.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [426] out of [1408] = [30.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [427] out of [1408] = [30.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [428] out of [1408] = [30.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [429] out of [1408] = [30.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [430] out of [1408] = [30.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [431] out of [1408] = [30.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [432] out of [1408] = [30.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [433] out of [1408] = [30.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [434] out of [1408] = [30.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 451 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [435] out of [1408] = [30.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [436] out of [1408] = [31.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [437] out of [1408] = [31.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [438] out of [1408] = [31.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [439] out of [1408] = [31.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [440] out of [1408] = [31.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [441] out of [1408] = [31.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 610 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [442] out of [1408] = [31.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [443] out of [1408] = [31.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [444] out of [1408] = [31.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 639 ms\n",
      "Tokens per second [84.5]\n",
      "Processing call [445] out of [1408] = [31.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [446] out of [1408] = [31.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [447] out of [1408] = [31.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [448] out of [1408] = [31.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [449] out of [1408] = [31.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [450] out of [1408] = [32.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [451] out of [1408] = [32.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [452] out of [1408] = [32.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [453] out of [1408] = [32.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [454] out of [1408] = [32.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 549 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [455] out of [1408] = [32.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [456] out of [1408] = [32.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [457] out of [1408] = [32.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [458] out of [1408] = [32.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [459] out of [1408] = [32.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 615 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [460] out of [1408] = [32.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [461] out of [1408] = [32.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [462] out of [1408] = [32.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 609 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [463] out of [1408] = [32.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 508 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [464] out of [1408] = [33.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 551 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [465] out of [1408] = [33.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 448 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [466] out of [1408] = [33.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [467] out of [1408] = [33.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [468] out of [1408] = [33.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [469] out of [1408] = [33.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 721 ms\n",
      "Tokens per second [86.0]\n",
      "Processing call [470] out of [1408] = [33.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 662 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [471] out of [1408] = [33.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [472] out of [1408] = [33.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [473] out of [1408] = [33.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [474] out of [1408] = [33.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 642 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [475] out of [1408] = [33.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 611 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [476] out of [1408] = [33.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 490 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [477] out of [1408] = [33.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 671 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [478] out of [1408] = [33.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [479] out of [1408] = [34.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [480] out of [1408] = [34.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [481] out of [1408] = [34.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [482] out of [1408] = [34.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [483] out of [1408] = [34.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [484] out of [1408] = [34.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [485] out of [1408] = [34.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 509 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [486] out of [1408] = [34.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 621 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [487] out of [1408] = [34.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 589 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [488] out of [1408] = [34.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [77.2]\n",
      "Processing call [489] out of [1408] = [34.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [490] out of [1408] = [34.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [491] out of [1408] = [34.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [492] out of [1408] = [34.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 497 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [493] out of [1408] = [35.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [494] out of [1408] = [35.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [495] out of [1408] = [35.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [496] out of [1408] = [35.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [497] out of [1408] = [35.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 588 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [498] out of [1408] = [35.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 549 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [499] out of [1408] = [35.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [500] out of [1408] = [35.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 509 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [501] out of [1408] = [35.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 448 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [502] out of [1408] = [35.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 460 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [503] out of [1408] = [35.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [504] out of [1408] = [35.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 540 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [505] out of [1408] = [35.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [506] out of [1408] = [35.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [507] out of [1408] = [36.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [508] out of [1408] = [36.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 449 ms\n",
      "Tokens per second [78.0]\n",
      "Processing call [509] out of [1408] = [36.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [510] out of [1408] = [36.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 540 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [511] out of [1408] = [36.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 697 ms\n",
      "Tokens per second [86.1]\n",
      "Processing call [512] out of [1408] = [36.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 540 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [513] out of [1408] = [36.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 656 ms\n",
      "Tokens per second [85.4]\n",
      "Processing call [514] out of [1408] = [36.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [515] out of [1408] = [36.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 585 ms\n",
      "Tokens per second [83.8]\n",
      "Processing call [516] out of [1408] = [36.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 645 ms\n",
      "Tokens per second [85.3]\n",
      "Processing call [517] out of [1408] = [36.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [518] out of [1408] = [36.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [519] out of [1408] = [36.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [520] out of [1408] = [36.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [521] out of [1408] = [37.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [522] out of [1408] = [37.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [523] out of [1408] = [37.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [524] out of [1408] = [37.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [525] out of [1408] = [37.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 606 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [526] out of [1408] = [37.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [527] out of [1408] = [37.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [528] out of [1408] = [37.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 569 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [529] out of [1408] = [37.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 614 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [530] out of [1408] = [37.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [531] out of [1408] = [37.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [532] out of [1408] = [37.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [533] out of [1408] = [37.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [534] out of [1408] = [37.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [535] out of [1408] = [38.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [536] out of [1408] = [38.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [537] out of [1408] = [38.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [538] out of [1408] = [38.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 632 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [539] out of [1408] = [38.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 461 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [540] out of [1408] = [38.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [541] out of [1408] = [38.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [542] out of [1408] = [38.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [543] out of [1408] = [38.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [544] out of [1408] = [38.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [545] out of [1408] = [38.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [546] out of [1408] = [38.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [547] out of [1408] = [38.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [548] out of [1408] = [38.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [549] out of [1408] = [39.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [550] out of [1408] = [39.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [551] out of [1408] = [39.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [552] out of [1408] = [39.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [553] out of [1408] = [39.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [554] out of [1408] = [39.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [555] out of [1408] = [39.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [556] out of [1408] = [39.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [557] out of [1408] = [39.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [558] out of [1408] = [39.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [559] out of [1408] = [39.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [560] out of [1408] = [39.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [561] out of [1408] = [39.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [562] out of [1408] = [39.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [563] out of [1408] = [40.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 557 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [564] out of [1408] = [40.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [565] out of [1408] = [40.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [566] out of [1408] = [40.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [567] out of [1408] = [40.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [568] out of [1408] = [40.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [569] out of [1408] = [40.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 448 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [570] out of [1408] = [40.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [571] out of [1408] = [40.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 509 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [572] out of [1408] = [40.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 626 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [573] out of [1408] = [40.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [574] out of [1408] = [40.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [575] out of [1408] = [40.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [576] out of [1408] = [40.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 508 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [577] out of [1408] = [41.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [578] out of [1408] = [41.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [579] out of [1408] = [41.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [580] out of [1408] = [41.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [581] out of [1408] = [41.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [582] out of [1408] = [41.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 609 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [583] out of [1408] = [41.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [584] out of [1408] = [41.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [585] out of [1408] = [41.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [586] out of [1408] = [41.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 541 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [587] out of [1408] = [41.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [588] out of [1408] = [41.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [589] out of [1408] = [41.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 575 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [590] out of [1408] = [41.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [591] out of [1408] = [42.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [592] out of [1408] = [42.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [593] out of [1408] = [42.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [594] out of [1408] = [42.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [595] out of [1408] = [42.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [596] out of [1408] = [42.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [597] out of [1408] = [42.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [598] out of [1408] = [42.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [599] out of [1408] = [42.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 490 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [600] out of [1408] = [42.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [601] out of [1408] = [42.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [602] out of [1408] = [42.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [603] out of [1408] = [42.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [604] out of [1408] = [42.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [605] out of [1408] = [43.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [606] out of [1408] = [43.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [607] out of [1408] = [43.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [608] out of [1408] = [43.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [609] out of [1408] = [43.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 592 ms\n",
      "Tokens per second [84.5]\n",
      "Processing call [610] out of [1408] = [43.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [611] out of [1408] = [43.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [612] out of [1408] = [43.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [613] out of [1408] = [43.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [614] out of [1408] = [43.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [615] out of [1408] = [43.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [616] out of [1408] = [43.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 550 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [617] out of [1408] = [43.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [618] out of [1408] = [43.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 626 ms\n",
      "Tokens per second [83.1]\n",
      "Processing call [619] out of [1408] = [44.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [620] out of [1408] = [44.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [621] out of [1408] = [44.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [622] out of [1408] = [44.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [623] out of [1408] = [44.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [624] out of [1408] = [44.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 619 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [625] out of [1408] = [44.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [626] out of [1408] = [44.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [627] out of [1408] = [44.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [628] out of [1408] = [44.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [629] out of [1408] = [44.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [630] out of [1408] = [44.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [631] out of [1408] = [44.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [632] out of [1408] = [44.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [633] out of [1408] = [45.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [634] out of [1408] = [45.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [635] out of [1408] = [45.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [636] out of [1408] = [45.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [637] out of [1408] = [45.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [638] out of [1408] = [45.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [639] out of [1408] = [45.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 583 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [640] out of [1408] = [45.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [641] out of [1408] = [45.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 487 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [642] out of [1408] = [45.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 647 ms\n",
      "Tokens per second [85.0]\n",
      "Processing call [643] out of [1408] = [45.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [644] out of [1408] = [45.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 609 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [645] out of [1408] = [45.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 505 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [646] out of [1408] = [45.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 566 ms\n",
      "Tokens per second [83.0]\n",
      "Processing call [647] out of [1408] = [46.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [648] out of [1408] = [46.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [649] out of [1408] = [46.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [650] out of [1408] = [46.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [651] out of [1408] = [46.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [652] out of [1408] = [46.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 574 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [653] out of [1408] = [46.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [82.7]\n",
      "Processing call [654] out of [1408] = [46.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 643 ms\n",
      "Tokens per second [85.5]\n",
      "Processing call [655] out of [1408] = [46.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [656] out of [1408] = [46.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [657] out of [1408] = [46.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [658] out of [1408] = [46.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [659] out of [1408] = [46.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [660] out of [1408] = [46.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [661] out of [1408] = [46.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [662] out of [1408] = [47.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [663] out of [1408] = [47.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [664] out of [1408] = [47.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [665] out of [1408] = [47.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 591 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [666] out of [1408] = [47.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [667] out of [1408] = [47.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [668] out of [1408] = [47.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [669] out of [1408] = [47.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [670] out of [1408] = [47.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 584 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [671] out of [1408] = [47.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 456 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [672] out of [1408] = [47.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [673] out of [1408] = [47.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 404 ms\n",
      "Tokens per second [74.3]\n",
      "Processing call [674] out of [1408] = [47.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [675] out of [1408] = [47.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [676] out of [1408] = [48.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [677] out of [1408] = [48.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 614 ms\n",
      "Tokens per second [83.1]\n",
      "Processing call [678] out of [1408] = [48.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [679] out of [1408] = [48.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [680] out of [1408] = [48.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [681] out of [1408] = [48.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 694 ms\n",
      "Tokens per second [85.0]\n",
      "Processing call [682] out of [1408] = [48.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [683] out of [1408] = [48.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [684] out of [1408] = [48.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [685] out of [1408] = [48.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [686] out of [1408] = [48.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [687] out of [1408] = [48.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [688] out of [1408] = [48.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 605 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [689] out of [1408] = [48.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 540 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [690] out of [1408] = [49.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [691] out of [1408] = [49.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [692] out of [1408] = [49.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [693] out of [1408] = [49.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [694] out of [1408] = [49.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [695] out of [1408] = [49.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 654 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [696] out of [1408] = [49.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 551 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [697] out of [1408] = [49.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 455 ms\n",
      "Tokens per second [76.9]\n",
      "Processing call [698] out of [1408] = [49.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [699] out of [1408] = [49.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [700] out of [1408] = [49.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 625 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [701] out of [1408] = [49.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [702] out of [1408] = [49.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [703] out of [1408] = [49.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [76.4]\n",
      "Processing call [704] out of [1408] = [50.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [705] out of [1408] = [50.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [706] out of [1408] = [50.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [707] out of [1408] = [50.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [708] out of [1408] = [50.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [709] out of [1408] = [50.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [710] out of [1408] = [50.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [711] out of [1408] = [50.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 456 ms\n",
      "Tokens per second [76.8]\n",
      "Processing call [712] out of [1408] = [50.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [713] out of [1408] = [50.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [714] out of [1408] = [50.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 653 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [715] out of [1408] = [50.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [716] out of [1408] = [50.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [717] out of [1408] = [50.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [718] out of [1408] = [51.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [719] out of [1408] = [51.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 623 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [720] out of [1408] = [51.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [721] out of [1408] = [51.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [76.6]\n",
      "Processing call [722] out of [1408] = [51.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [723] out of [1408] = [51.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [724] out of [1408] = [51.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [725] out of [1408] = [51.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [726] out of [1408] = [51.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 584 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [727] out of [1408] = [51.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [82.3]\n",
      "Processing call [728] out of [1408] = [51.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [729] out of [1408] = [51.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [76.6]\n",
      "Processing call [730] out of [1408] = [51.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [731] out of [1408] = [51.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [732] out of [1408] = [52.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 456 ms\n",
      "Tokens per second [76.8]\n",
      "Processing call [733] out of [1408] = [52.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 497 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [734] out of [1408] = [52.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 585 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [735] out of [1408] = [52.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 456 ms\n",
      "Tokens per second [76.8]\n",
      "Processing call [736] out of [1408] = [52.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 509 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [737] out of [1408] = [52.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [738] out of [1408] = [52.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 605 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [739] out of [1408] = [52.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [740] out of [1408] = [52.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [741] out of [1408] = [52.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 597 ms\n",
      "Tokens per second [83.8]\n",
      "Processing call [742] out of [1408] = [52.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [743] out of [1408] = [52.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [744] out of [1408] = [52.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [745] out of [1408] = [52.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [746] out of [1408] = [53.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 496 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [747] out of [1408] = [53.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 643 ms\n",
      "Tokens per second [85.5]\n",
      "Processing call [748] out of [1408] = [53.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [749] out of [1408] = [53.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [750] out of [1408] = [53.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [751] out of [1408] = [53.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [752] out of [1408] = [53.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [753] out of [1408] = [53.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [754] out of [1408] = [53.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [755] out of [1408] = [53.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 618 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [756] out of [1408] = [53.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 605 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [757] out of [1408] = [53.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 589 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [758] out of [1408] = [53.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 487 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [759] out of [1408] = [53.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [760] out of [1408] = [54.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [761] out of [1408] = [54.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [85.5]\n",
      "Processing call [762] out of [1408] = [54.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [763] out of [1408] = [54.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [764] out of [1408] = [54.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 397 ms\n",
      "Tokens per second [75.6]\n",
      "Processing call [765] out of [1408] = [54.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 485 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [766] out of [1408] = [54.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 621 ms\n",
      "Tokens per second [85.3]\n",
      "Processing call [767] out of [1408] = [54.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [768] out of [1408] = [54.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 450 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [769] out of [1408] = [54.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 574 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [770] out of [1408] = [54.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [771] out of [1408] = [54.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [772] out of [1408] = [54.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [773] out of [1408] = [54.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 487 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [774] out of [1408] = [55.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 566 ms\n",
      "Tokens per second [83.0]\n",
      "Processing call [775] out of [1408] = [55.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 625 ms\n",
      "Tokens per second [84.8]\n",
      "Processing call [776] out of [1408] = [55.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [777] out of [1408] = [55.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 606 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [778] out of [1408] = [55.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [779] out of [1408] = [55.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [780] out of [1408] = [55.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [781] out of [1408] = [55.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [782] out of [1408] = [55.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 455 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [783] out of [1408] = [55.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 497 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [784] out of [1408] = [55.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [785] out of [1408] = [55.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [786] out of [1408] = [55.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [787] out of [1408] = [55.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [788] out of [1408] = [56.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 505 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [789] out of [1408] = [56.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [790] out of [1408] = [56.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [791] out of [1408] = [56.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [792] out of [1408] = [56.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [793] out of [1408] = [56.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [794] out of [1408] = [56.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [795] out of [1408] = [56.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [796] out of [1408] = [56.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [797] out of [1408] = [56.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [798] out of [1408] = [56.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [799] out of [1408] = [56.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [800] out of [1408] = [56.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [801] out of [1408] = [56.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 609 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [802] out of [1408] = [57.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [803] out of [1408] = [57.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [804] out of [1408] = [57.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [805] out of [1408] = [57.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 450 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [806] out of [1408] = [57.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 549 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [807] out of [1408] = [57.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 641 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [808] out of [1408] = [57.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 630 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [809] out of [1408] = [57.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [810] out of [1408] = [57.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [811] out of [1408] = [57.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [812] out of [1408] = [57.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [813] out of [1408] = [57.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [814] out of [1408] = [57.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [815] out of [1408] = [57.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [816] out of [1408] = [58.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [817] out of [1408] = [58.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [818] out of [1408] = [58.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 403 ms\n",
      "Tokens per second [74.4]\n",
      "Processing call [819] out of [1408] = [58.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 541 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [820] out of [1408] = [58.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [821] out of [1408] = [58.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [822] out of [1408] = [58.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [823] out of [1408] = [58.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [824] out of [1408] = [58.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [825] out of [1408] = [58.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [826] out of [1408] = [58.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 508 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [827] out of [1408] = [58.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 659 ms\n",
      "Tokens per second [85.0]\n",
      "Processing call [828] out of [1408] = [58.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [829] out of [1408] = [58.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [830] out of [1408] = [58.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [831] out of [1408] = [59.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 485 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [832] out of [1408] = [59.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 552 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [833] out of [1408] = [59.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [834] out of [1408] = [59.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 630 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [835] out of [1408] = [59.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [836] out of [1408] = [59.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [837] out of [1408] = [59.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 545 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [838] out of [1408] = [59.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 541 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [839] out of [1408] = [59.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [840] out of [1408] = [59.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [841] out of [1408] = [59.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [842] out of [1408] = [59.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [843] out of [1408] = [59.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [844] out of [1408] = [59.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [845] out of [1408] = [60.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [846] out of [1408] = [60.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 610 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [847] out of [1408] = [60.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [848] out of [1408] = [60.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 460 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [849] out of [1408] = [60.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 549 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [850] out of [1408] = [60.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 615 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [851] out of [1408] = [60.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [852] out of [1408] = [60.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 607 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [853] out of [1408] = [60.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [854] out of [1408] = [60.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [855] out of [1408] = [60.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [856] out of [1408] = [60.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 604 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [857] out of [1408] = [60.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [858] out of [1408] = [60.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [859] out of [1408] = [61.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [860] out of [1408] = [61.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [861] out of [1408] = [61.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [862] out of [1408] = [61.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [863] out of [1408] = [61.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [864] out of [1408] = [61.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [865] out of [1408] = [61.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [866] out of [1408] = [61.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [867] out of [1408] = [61.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [868] out of [1408] = [61.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [869] out of [1408] = [61.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [870] out of [1408] = [61.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [871] out of [1408] = [61.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [872] out of [1408] = [61.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [873] out of [1408] = [62.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [874] out of [1408] = [62.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [875] out of [1408] = [62.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 445 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [876] out of [1408] = [62.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 444 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [877] out of [1408] = [62.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [878] out of [1408] = [62.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [879] out of [1408] = [62.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 574 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [880] out of [1408] = [62.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 505 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [881] out of [1408] = [62.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [882] out of [1408] = [62.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [883] out of [1408] = [62.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [82.5]\n",
      "Processing call [884] out of [1408] = [62.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [885] out of [1408] = [62.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [886] out of [1408] = [62.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 615 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [887] out of [1408] = [63.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 452 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [888] out of [1408] = [63.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [889] out of [1408] = [63.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [890] out of [1408] = [63.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [891] out of [1408] = [63.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [892] out of [1408] = [63.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [893] out of [1408] = [63.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [894] out of [1408] = [63.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [895] out of [1408] = [63.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [896] out of [1408] = [63.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [897] out of [1408] = [63.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [898] out of [1408] = [63.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [899] out of [1408] = [63.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 552 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [900] out of [1408] = [63.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [901] out of [1408] = [64.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [902] out of [1408] = [64.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [903] out of [1408] = [64.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [904] out of [1408] = [64.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [905] out of [1408] = [64.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [906] out of [1408] = [64.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [907] out of [1408] = [64.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [908] out of [1408] = [64.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [909] out of [1408] = [64.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 693 ms\n",
      "Tokens per second [86.6]\n",
      "Processing call [910] out of [1408] = [64.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [911] out of [1408] = [64.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 485 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [912] out of [1408] = [64.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [913] out of [1408] = [64.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [914] out of [1408] = [64.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [915] out of [1408] = [65.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 455 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [916] out of [1408] = [65.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 596 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [917] out of [1408] = [65.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [918] out of [1408] = [65.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [919] out of [1408] = [65.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [920] out of [1408] = [65.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 594 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [921] out of [1408] = [65.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [922] out of [1408] = [65.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [923] out of [1408] = [65.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [924] out of [1408] = [65.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [925] out of [1408] = [65.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [926] out of [1408] = [65.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 451 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [927] out of [1408] = [65.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [928] out of [1408] = [65.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [929] out of [1408] = [66.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [930] out of [1408] = [66.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 632 ms\n",
      "Tokens per second [85.4]\n",
      "Processing call [931] out of [1408] = [66.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [932] out of [1408] = [66.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 626 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [933] out of [1408] = [66.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [934] out of [1408] = [66.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 623 ms\n",
      "Tokens per second [85.1]\n",
      "Processing call [935] out of [1408] = [66.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [936] out of [1408] = [66.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [937] out of [1408] = [66.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [938] out of [1408] = [66.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 634 ms\n",
      "Tokens per second [85.2]\n",
      "Processing call [939] out of [1408] = [66.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 553 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [940] out of [1408] = [66.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [941] out of [1408] = [66.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 400 ms\n",
      "Tokens per second [75.0]\n",
      "Processing call [942] out of [1408] = [66.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [943] out of [1408] = [67.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 653 ms\n",
      "Tokens per second [85.8]\n",
      "Processing call [944] out of [1408] = [67.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [945] out of [1408] = [67.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 633 ms\n",
      "Tokens per second [85.3]\n",
      "Processing call [946] out of [1408] = [67.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [947] out of [1408] = [67.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 445 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [948] out of [1408] = [67.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [949] out of [1408] = [67.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 439 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [950] out of [1408] = [67.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [951] out of [1408] = [67.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [952] out of [1408] = [67.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [953] out of [1408] = [67.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [954] out of [1408] = [67.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 617 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [955] out of [1408] = [67.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [956] out of [1408] = [67.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [957] out of [1408] = [68.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 444 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [958] out of [1408] = [68.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 576 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [959] out of [1408] = [68.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [960] out of [1408] = [68.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [82.7]\n",
      "Processing call [961] out of [1408] = [68.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [962] out of [1408] = [68.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [963] out of [1408] = [68.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [964] out of [1408] = [68.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [965] out of [1408] = [68.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [966] out of [1408] = [68.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [967] out of [1408] = [68.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 499 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [968] out of [1408] = [68.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 622 ms\n",
      "Tokens per second [85.2]\n",
      "Processing call [969] out of [1408] = [68.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 648 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [970] out of [1408] = [68.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [971] out of [1408] = [69.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 460 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [972] out of [1408] = [69.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 485 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [973] out of [1408] = [69.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [974] out of [1408] = [69.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 509 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [975] out of [1408] = [69.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [976] out of [1408] = [69.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [977] out of [1408] = [69.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 541 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [978] out of [1408] = [69.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [979] out of [1408] = [69.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 573 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [980] out of [1408] = [69.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [981] out of [1408] = [69.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 644 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [982] out of [1408] = [69.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 552 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [983] out of [1408] = [69.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [984] out of [1408] = [69.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [985] out of [1408] = [70.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [986] out of [1408] = [70.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [987] out of [1408] = [70.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 480 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [988] out of [1408] = [70.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 591 ms\n",
      "Tokens per second [82.8]\n",
      "Processing call [989] out of [1408] = [70.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [990] out of [1408] = [70.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [991] out of [1408] = [70.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 461 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [992] out of [1408] = [70.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 505 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [993] out of [1408] = [70.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [994] out of [1408] = [70.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [995] out of [1408] = [70.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [996] out of [1408] = [70.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 507 ms\n",
      "Tokens per second [75.0]\n",
      "Processing call [997] out of [1408] = [70.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [998] out of [1408] = [70.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [999] out of [1408] = [71.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1000] out of [1408] = [71.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 507 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1001] out of [1408] = [71.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 505 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [1002] out of [1408] = [71.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 444 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1003] out of [1408] = [71.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [1004] out of [1408] = [71.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 606 ms\n",
      "Tokens per second [84.2]\n",
      "Processing call [1005] out of [1408] = [71.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 402 ms\n",
      "Tokens per second [74.6]\n",
      "Processing call [1006] out of [1408] = [71.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 575 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [1007] out of [1408] = [71.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1008] out of [1408] = [71.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 651 ms\n",
      "Tokens per second [86.0]\n",
      "Processing call [1009] out of [1408] = [71.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 398 ms\n",
      "Tokens per second [75.4]\n",
      "Processing call [1010] out of [1408] = [71.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [1011] out of [1408] = [71.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1012] out of [1408] = [71.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 450 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [1013] out of [1408] = [71.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 526 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [1014] out of [1408] = [72.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 617 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [1015] out of [1408] = [72.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [1016] out of [1408] = [72.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1017] out of [1408] = [72.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [1018] out of [1408] = [72.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1019] out of [1408] = [72.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [1020] out of [1408] = [72.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 623 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [1021] out of [1408] = [72.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1022] out of [1408] = [72.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 509 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [1023] out of [1408] = [72.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [1024] out of [1408] = [72.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [77.1]\n",
      "Processing call [1025] out of [1408] = [72.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [1026] out of [1408] = [72.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 499 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1027] out of [1408] = [72.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 403 ms\n",
      "Tokens per second [74.4]\n",
      "Processing call [1028] out of [1408] = [73.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 605 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [1029] out of [1408] = [73.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1030] out of [1408] = [73.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [1031] out of [1408] = [73.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1032] out of [1408] = [73.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [1033] out of [1408] = [73.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [1034] out of [1408] = [73.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [1035] out of [1408] = [73.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1036] out of [1408] = [73.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 608 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1037] out of [1408] = [73.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1038] out of [1408] = [73.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1039] out of [1408] = [73.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1040] out of [1408] = [73.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1041] out of [1408] = [73.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [1042] out of [1408] = [74.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 537 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [1043] out of [1408] = [74.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1044] out of [1408] = [74.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 451 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [1045] out of [1408] = [74.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1046] out of [1408] = [74.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1047] out of [1408] = [74.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 668 ms\n",
      "Tokens per second [85.3]\n",
      "Processing call [1048] out of [1408] = [74.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1049] out of [1408] = [74.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1050] out of [1408] = [74.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 607 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [1051] out of [1408] = [74.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1052] out of [1408] = [74.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 560 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [1053] out of [1408] = [74.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 449 ms\n",
      "Tokens per second [78.0]\n",
      "Processing call [1054] out of [1408] = [74.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 500 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [1055] out of [1408] = [74.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 419 ms\n",
      "Tokens per second [76.4]\n",
      "Processing call [1056] out of [1408] = [75.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 497 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [1057] out of [1408] = [75.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1058] out of [1408] = [75.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 590 ms\n",
      "Tokens per second [83.1]\n",
      "Processing call [1059] out of [1408] = [75.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 585 ms\n",
      "Tokens per second [83.8]\n",
      "Processing call [1060] out of [1408] = [75.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [82.6]\n",
      "Processing call [1061] out of [1408] = [75.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1062] out of [1408] = [75.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 505 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [1063] out of [1408] = [75.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [85.5]\n",
      "Processing call [1064] out of [1408] = [75.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 647 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [1065] out of [1408] = [75.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 446 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [1066] out of [1408] = [75.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [1067] out of [1408] = [75.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1068] out of [1408] = [75.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [1069] out of [1408] = [75.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [1070] out of [1408] = [76.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [1071] out of [1408] = [76.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 610 ms\n",
      "Tokens per second [85.2]\n",
      "Processing call [1072] out of [1408] = [76.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [1073] out of [1408] = [76.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1074] out of [1408] = [76.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1075] out of [1408] = [76.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [1076] out of [1408] = [76.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 552 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [1077] out of [1408] = [76.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [1078] out of [1408] = [76.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [1079] out of [1408] = [76.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [1080] out of [1408] = [76.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 397 ms\n",
      "Tokens per second [75.6]\n",
      "Processing call [1081] out of [1408] = [76.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [1082] out of [1408] = [76.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [82.7]\n",
      "Processing call [1083] out of [1408] = [76.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [1084] out of [1408] = [77.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [1085] out of [1408] = [77.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [1086] out of [1408] = [77.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1087] out of [1408] = [77.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [1088] out of [1408] = [77.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 478 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1089] out of [1408] = [77.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 534 ms\n",
      "Tokens per second [82.4]\n",
      "Processing call [1090] out of [1408] = [77.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 398 ms\n",
      "Tokens per second [75.4]\n",
      "Processing call [1091] out of [1408] = [77.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 551 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [1092] out of [1408] = [77.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [1093] out of [1408] = [77.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [1094] out of [1408] = [77.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1095] out of [1408] = [77.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [1096] out of [1408] = [77.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 625 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [1097] out of [1408] = [77.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1098] out of [1408] = [78.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 624 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [1099] out of [1408] = [78.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 499 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1100] out of [1408] = [78.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [77.1]\n",
      "Processing call [1101] out of [1408] = [78.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1102] out of [1408] = [78.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1103] out of [1408] = [78.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1104] out of [1408] = [78.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1105] out of [1408] = [78.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 604 ms\n",
      "Tokens per second [82.8]\n",
      "Processing call [1106] out of [1408] = [78.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1107] out of [1408] = [78.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1108] out of [1408] = [78.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1109] out of [1408] = [78.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1110] out of [1408] = [78.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [1111] out of [1408] = [78.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1112] out of [1408] = [79.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [1113] out of [1408] = [79.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1114] out of [1408] = [79.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1115] out of [1408] = [79.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1116] out of [1408] = [79.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [1117] out of [1408] = [79.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1118] out of [1408] = [79.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1119] out of [1408] = [79.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [1120] out of [1408] = [79.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1121] out of [1408] = [79.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 500 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [1122] out of [1408] = [79.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 643 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [1123] out of [1408] = [79.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1124] out of [1408] = [79.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 603 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [1125] out of [1408] = [79.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [1126] out of [1408] = [80.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1127] out of [1408] = [80.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [1128] out of [1408] = [80.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1129] out of [1408] = [80.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 542 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [1130] out of [1408] = [80.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1131] out of [1408] = [80.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1132] out of [1408] = [80.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [1133] out of [1408] = [80.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 633 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [1134] out of [1408] = [80.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [1135] out of [1408] = [80.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1136] out of [1408] = [80.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [1137] out of [1408] = [80.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 646 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [1138] out of [1408] = [80.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1139] out of [1408] = [80.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1140] out of [1408] = [81.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [1141] out of [1408] = [81.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 532 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1142] out of [1408] = [81.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 612 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [1143] out of [1408] = [81.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1144] out of [1408] = [81.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 461 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1145] out of [1408] = [81.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [77.1]\n",
      "Processing call [1146] out of [1408] = [81.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 632 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1147] out of [1408] = [81.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1148] out of [1408] = [81.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 401 ms\n",
      "Tokens per second [74.8]\n",
      "Processing call [1149] out of [1408] = [81.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [1150] out of [1408] = [81.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1151] out of [1408] = [81.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1152] out of [1408] = [81.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1153] out of [1408] = [81.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 624 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [1154] out of [1408] = [82.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1155] out of [1408] = [82.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 625 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [1156] out of [1408] = [82.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [1157] out of [1408] = [82.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1158] out of [1408] = [82.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 405 ms\n",
      "Tokens per second [74.1]\n",
      "Processing call [1159] out of [1408] = [82.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1160] out of [1408] = [82.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 683 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [1161] out of [1408] = [82.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 551 ms\n",
      "Tokens per second [81.7]\n",
      "Processing call [1162] out of [1408] = [82.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1163] out of [1408] = [82.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [1164] out of [1408] = [82.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 545 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1165] out of [1408] = [82.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1166] out of [1408] = [82.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [1167] out of [1408] = [82.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 631 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [1168] out of [1408] = [83.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [1169] out of [1408] = [83.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [1170] out of [1408] = [83.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 552 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [1171] out of [1408] = [83.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 596 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [1172] out of [1408] = [83.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [1173] out of [1408] = [83.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1174] out of [1408] = [83.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 685 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [1175] out of [1408] = [83.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [1176] out of [1408] = [83.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [1177] out of [1408] = [83.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 614 ms\n",
      "Tokens per second [83.1]\n",
      "Processing call [1178] out of [1408] = [83.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 662 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [1179] out of [1408] = [83.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 664 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [1180] out of [1408] = [83.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 404 ms\n",
      "Tokens per second [74.3]\n",
      "Processing call [1181] out of [1408] = [83.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 632 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1182] out of [1408] = [83.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1183] out of [1408] = [84.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 557 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1184] out of [1408] = [84.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1185] out of [1408] = [84.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1186] out of [1408] = [84.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 571 ms\n",
      "Tokens per second [82.3]\n",
      "Processing call [1187] out of [1408] = [84.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1188] out of [1408] = [84.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 613 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [1189] out of [1408] = [84.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1190] out of [1408] = [84.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [1191] out of [1408] = [84.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1192] out of [1408] = [84.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 542 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [1193] out of [1408] = [84.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1194] out of [1408] = [84.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1195] out of [1408] = [84.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1196] out of [1408] = [84.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1197] out of [1408] = [85.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1198] out of [1408] = [85.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1199] out of [1408] = [85.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [1200] out of [1408] = [85.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 524 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1201] out of [1408] = [85.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1202] out of [1408] = [85.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1203] out of [1408] = [85.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 402 ms\n",
      "Tokens per second [74.6]\n",
      "Processing call [1204] out of [1408] = [85.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 495 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1205] out of [1408] = [85.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [1206] out of [1408] = [85.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 450 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [1207] out of [1408] = [85.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1208] out of [1408] = [85.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1209] out of [1408] = [85.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [77.1]\n",
      "Processing call [1210] out of [1408] = [85.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [1211] out of [1408] = [86.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1212] out of [1408] = [86.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 487 ms\n",
      "Tokens per second [78.0]\n",
      "Processing call [1213] out of [1408] = [86.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [1214] out of [1408] = [86.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [1215] out of [1408] = [86.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 634 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [1216] out of [1408] = [86.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1217] out of [1408] = [86.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [1218] out of [1408] = [86.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 517 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [1219] out of [1408] = [86.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1220] out of [1408] = [86.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [1221] out of [1408] = [86.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [1222] out of [1408] = [86.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1223] out of [1408] = [86.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 613 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [1224] out of [1408] = [86.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 499 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1225] out of [1408] = [87.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 552 ms\n",
      "Tokens per second [81.5]\n",
      "Processing call [1226] out of [1408] = [87.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 554 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [1227] out of [1408] = [87.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1228] out of [1408] = [87.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1229] out of [1408] = [87.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [77.1]\n",
      "Processing call [1230] out of [1408] = [87.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 631 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [1231] out of [1408] = [87.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1232] out of [1408] = [87.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 580 ms\n",
      "Tokens per second [82.8]\n",
      "Processing call [1233] out of [1408] = [87.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 471 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1234] out of [1408] = [87.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1235] out of [1408] = [87.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1236] out of [1408] = [87.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1237] out of [1408] = [87.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 598 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [1238] out of [1408] = [87.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 504 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1239] out of [1408] = [88.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [1240] out of [1408] = [88.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1241] out of [1408] = [88.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1242] out of [1408] = [88.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1243] out of [1408] = [88.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 463 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [1244] out of [1408] = [88.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [1245] out of [1408] = [88.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [76.6]\n",
      "Processing call [1246] out of [1408] = [88.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 461 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1247] out of [1408] = [88.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1248] out of [1408] = [88.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 601 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [1249] out of [1408] = [88.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [76.6]\n",
      "Processing call [1250] out of [1408] = [88.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [1251] out of [1408] = [88.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 505 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [1252] out of [1408] = [88.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1253] out of [1408] = [89.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 516 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1254] out of [1408] = [89.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [1255] out of [1408] = [89.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1256] out of [1408] = [89.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 513 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [1257] out of [1408] = [89.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 633 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [1258] out of [1408] = [89.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 464 ms\n",
      "Tokens per second [77.6]\n",
      "Processing call [1259] out of [1408] = [89.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1260] out of [1408] = [89.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1261] out of [1408] = [89.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1262] out of [1408] = [89.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 621 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [1263] out of [1408] = [89.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1264] out of [1408] = [89.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1265] out of [1408] = [89.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1266] out of [1408] = [89.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 482 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1267] out of [1408] = [90.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 510 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1268] out of [1408] = [90.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1269] out of [1408] = [90.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1270] out of [1408] = [90.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 544 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1271] out of [1408] = [90.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 576 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [1272] out of [1408] = [90.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 515 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [1273] out of [1408] = [90.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 542 ms\n",
      "Tokens per second [81.2]\n",
      "Processing call [1274] out of [1408] = [90.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 479 ms\n",
      "Tokens per second [77.2]\n",
      "Processing call [1275] out of [1408] = [90.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 623 ms\n",
      "Tokens per second [83.5]\n",
      "Processing call [1276] out of [1408] = [90.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [78.2]\n",
      "Processing call [1277] out of [1408] = [90.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 696 ms\n",
      "Tokens per second [84.8]\n",
      "Processing call [1278] out of [1408] = [90.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [77.3]\n",
      "Processing call [1279] out of [1408] = [90.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 664 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [1280] out of [1408] = [90.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [1281] out of [1408] = [91.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1282] out of [1408] = [91.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 498 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [1283] out of [1408] = [91.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1284] out of [1408] = [91.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [76.6]\n",
      "Processing call [1285] out of [1408] = [91.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [1286] out of [1408] = [91.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 523 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [1287] out of [1408] = [91.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1288] out of [1408] = [91.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [77.4]\n",
      "Processing call [1289] out of [1408] = [91.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [1290] out of [1408] = [91.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 583 ms\n",
      "Tokens per second [82.3]\n",
      "Processing call [1291] out of [1408] = [91.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 545 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1292] out of [1408] = [91.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 642 ms\n",
      "Tokens per second [84.1]\n",
      "Processing call [1293] out of [1408] = [91.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 453 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1294] out of [1408] = [91.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [1295] out of [1408] = [92.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [82.1]\n",
      "Processing call [1296] out of [1408] = [92.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 459 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [1297] out of [1408] = [92.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 674 ms\n",
      "Tokens per second [86.1]\n",
      "Processing call [1298] out of [1408] = [92.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1299] out of [1408] = [92.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 507 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1300] out of [1408] = [92.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 473 ms\n",
      "Tokens per second [80.3]\n",
      "Processing call [1301] out of [1408] = [92.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1302] out of [1408] = [92.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 643 ms\n",
      "Tokens per second [85.5]\n",
      "Processing call [1303] out of [1408] = [92.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 614 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [1304] out of [1408] = [92.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 481 ms\n",
      "Tokens per second [79.0]\n",
      "Processing call [1305] out of [1408] = [92.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 519 ms\n",
      "Tokens per second [80.9]\n",
      "Processing call [1306] out of [1408] = [92.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1307] out of [1408] = [92.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [1308] out of [1408] = [92.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1309] out of [1408] = [93.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 485 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1310] out of [1408] = [93.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1311] out of [1408] = [93.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 629 ms\n",
      "Tokens per second [85.9]\n",
      "Processing call [1312] out of [1408] = [93.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [82.2]\n",
      "Processing call [1313] out of [1408] = [93.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 611 ms\n",
      "Tokens per second [85.1]\n",
      "Processing call [1314] out of [1408] = [93.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 604 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [1315] out of [1408] = [93.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 465 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [1316] out of [1408] = [93.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 508 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1317] out of [1408] = [93.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 450 ms\n",
      "Tokens per second [77.8]\n",
      "Processing call [1318] out of [1408] = [93.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 489 ms\n",
      "Tokens per second [79.8]\n",
      "Processing call [1319] out of [1408] = [93.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [78.3]\n",
      "Processing call [1320] out of [1408] = [93.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 457 ms\n",
      "Tokens per second [78.8]\n",
      "Processing call [1321] out of [1408] = [93.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 528 ms\n",
      "Tokens per second [81.4]\n",
      "Processing call [1322] out of [1408] = [93.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 617 ms\n",
      "Tokens per second [84.3]\n",
      "Processing call [1323] out of [1408] = [94.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1324] out of [1408] = [94.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [1325] out of [1408] = [94.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 499 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1326] out of [1408] = [94.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 468 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1327] out of [1408] = [94.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 597 ms\n",
      "Tokens per second [83.8]\n",
      "Processing call [1328] out of [1408] = [94.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 477 ms\n",
      "Tokens per second [79.7]\n",
      "Processing call [1329] out of [1408] = [94.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 466 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1330] out of [1408] = [94.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1331] out of [1408] = [94.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [1332] out of [1408] = [94.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1333] out of [1408] = [94.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 567 ms\n",
      "Tokens per second [82.9]\n",
      "Processing call [1334] out of [1408] = [94.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1335] out of [1408] = [94.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 474 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1336] out of [1408] = [94.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 675 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [1337] out of [1408] = [95.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 536 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1338] out of [1408] = [95.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 535 ms\n",
      "Tokens per second [80.4]\n",
      "Processing call [1339] out of [1408] = [95.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [1340] out of [1408] = [95.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 533 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [1341] out of [1408] = [95.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 598 ms\n",
      "Tokens per second [83.6]\n",
      "Processing call [1342] out of [1408] = [95.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 573 ms\n",
      "Tokens per second [82.0]\n",
      "Processing call [1343] out of [1408] = [95.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 541 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [1344] out of [1408] = [95.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 543 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1345] out of [1408] = [95.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1346] out of [1408] = [95.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 682 ms\n",
      "Tokens per second [85.0]\n",
      "Processing call [1347] out of [1408] = [95.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 555 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [1348] out of [1408] = [95.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1349] out of [1408] = [95.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 616 ms\n",
      "Tokens per second [84.4]\n",
      "Processing call [1350] out of [1408] = [95.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 648 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [1351] out of [1408] = [96.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [81.6]\n",
      "Processing call [1352] out of [1408] = [96.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 506 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1353] out of [1408] = [96.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 661 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [1354] out of [1408] = [96.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 456 ms\n",
      "Tokens per second [76.8]\n",
      "Processing call [1355] out of [1408] = [96.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [80.1]\n",
      "Processing call [1356] out of [1408] = [96.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 475 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [1357] out of [1408] = [96.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 476 ms\n",
      "Tokens per second [77.7]\n",
      "Processing call [1358] out of [1408] = [96.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".............................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 701 ms\n",
      "Tokens per second [87.0]\n",
      "Processing call [1359] out of [1408] = [96.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 490 ms\n",
      "Tokens per second [79.6]\n",
      "Processing call [1360] out of [1408] = [96.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..............................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 398 ms\n",
      "Tokens per second [75.4]\n",
      "Processing call [1361] out of [1408] = [96.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 646 ms\n",
      "Tokens per second [85.1]\n",
      "Processing call [1362] out of [1408] = [96.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 697 ms\n",
      "Tokens per second [86.1]\n",
      "Processing call [1363] out of [1408] = [96.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [1364] out of [1408] = [96.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 448 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1365] out of [1408] = [96.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 500 ms\n",
      "Tokens per second [80.0]\n",
      "Processing call [1366] out of [1408] = [97.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [81.1]\n",
      "Processing call [1367] out of [1408] = [97.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [1368] out of [1408] = [97.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1369] out of [1408] = [97.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 531 ms\n",
      "Tokens per second [81.0]\n",
      "Processing call [1370] out of [1408] = [97.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1371] out of [1408] = [97.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [79.3]\n",
      "Processing call [1372] out of [1408] = [97.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 620 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1373] out of [1408] = [97.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 601 ms\n",
      "Tokens per second [83.2]\n",
      "Processing call [1374] out of [1408] = [97.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 493 ms\n",
      "Tokens per second [79.1]\n",
      "Processing call [1375] out of [1408] = [97.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 538 ms\n",
      "Tokens per second [81.8]\n",
      "Processing call [1376] out of [1408] = [97.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 488 ms\n",
      "Tokens per second [79.9]\n",
      "Processing call [1377] out of [1408] = [97.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 486 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1378] out of [1408] = [97.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 458 ms\n",
      "Tokens per second [78.6]\n",
      "Processing call [1379] out of [1408] = [97.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 483 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1380] out of [1408] = [98.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 470 ms\n",
      "Tokens per second [78.7]\n",
      "Processing call [1381] out of [1408] = [98.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 467 ms\n",
      "Tokens per second [79.2]\n",
      "Processing call [1382] out of [1408] = [98.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 511 ms\n",
      "Tokens per second [80.2]\n",
      "Processing call [1383] out of [1408] = [98.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1384] out of [1408] = [98.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 671 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [1385] out of [1408] = [98.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 484 ms\n",
      "Tokens per second [78.5]\n",
      "Processing call [1386] out of [1408] = [98.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 503 ms\n",
      "Tokens per second [79.5]\n",
      "Processing call [1387] out of [1408] = [98.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 461 ms\n",
      "Tokens per second [78.1]\n",
      "Processing call [1388] out of [1408] = [98.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 624 ms\n",
      "Tokens per second [84.9]\n",
      "Processing call [1389] out of [1408] = [98.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 564 ms\n",
      "Tokens per second [83.3]\n",
      "Processing call [1390] out of [1408] = [98.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 626 ms\n",
      "Tokens per second [84.7]\n",
      "Processing call [1391] out of [1408] = [98.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 615 ms\n",
      "Tokens per second [84.6]\n",
      "Processing call [1392] out of [1408] = [98.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 462 ms\n",
      "Tokens per second [77.9]\n",
      "Processing call [1393] out of [1408] = [98.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 632 ms\n",
      "Tokens per second [83.9]\n",
      "Processing call [1394] out of [1408] = [99.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 691 ms\n",
      "Tokens per second [85.4]\n",
      "Processing call [1395] out of [1408] = [99.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 529 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [1396] out of [1408] = [99.1%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 508 ms\n",
      "Tokens per second [80.7]\n",
      "Processing call [1397] out of [1408] = [99.2%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 522 ms\n",
      "Tokens per second [80.5]\n",
      "Processing call [1398] out of [1408] = [99.3%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".........................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 665 ms\n",
      "Tokens per second [85.7]\n",
      "Processing call [1399] out of [1408] = [99.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 633 ms\n",
      "Tokens per second [83.7]\n",
      "Processing call [1400] out of [1408] = [99.4%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".......................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 491 ms\n",
      "Tokens per second [79.4]\n",
      "Processing call [1401] out of [1408] = [99.5%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 520 ms\n",
      "Tokens per second [80.8]\n",
      "Processing call [1402] out of [1408] = [99.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...............................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 578 ms\n",
      "Tokens per second [81.3]\n",
      "Processing call [1403] out of [1408] = [99.6%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 525 ms\n",
      "Tokens per second [81.9]\n",
      "Processing call [1404] out of [1408] = [99.7%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 472 ms\n",
      "Tokens per second [78.4]\n",
      "Processing call [1405] out of [1408] = [99.8%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "......................................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 643 ms\n",
      "Tokens per second [84.0]\n",
      "Processing call [1406] out of [1408] = [99.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      ".....................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 469 ms\n",
      "Tokens per second [78.9]\n",
      "Processing call [1407] out of [1408] = [99.9%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "..........................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 521 ms\n",
      "Tokens per second [80.6]\n",
      "Processing call [1408] out of [1408] = [100.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "...................................\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [78.3]\n",
      "\n",
      "Generating responses for 1,408 rows... Done! in 12:08\n",
      "[517.5] ms per item\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation Stats for `mistralai/Mistral-7B-Instruct-v0.2-AWQ`\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "          Contains response 100.0%\n",
      " Contains <browser-command> 100.0%\n",
      "            Contains <args> 100.0%\n",
      "          Response is exact 99.6%\n",
      "Response has correct values 99.6%\n",
      " Browser command is correct 99.6%\n",
      "            Args is correct 99.9%\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "train_df, test_df, validate_df = xml_ftp_generator.get_train_test_validate_split( all_qna_df, sample_size=all_qna_df.shape[ 0 ], test_size=0.2, test_validate_size=0.5 )\n",
    "\n",
    "xml_ftp_generator.write_ttv_split_to_jsonl( train_df, test_df, validate_df )\n",
    "\n",
    "# validation block\n",
    "# validate_df    = pd.read_json( xml_ftp_generator.path_prefix + \"/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\", lines=True ).sample( 100, random_state=42 )\n",
    "# timer          = Stopwatch( msg=f\"Validating {validate_df.shape[ 0 ]:,} responses...\", silent=False )\n",
    "\n",
    "model_name     = \"mistralai/Mistral-7B-Instruct-v0.2-AWQ\"\n",
    "validate_df    = xml_ftp_generator.generate_responses( validate_df, switch=\"tgi\", model_name=model_name )\n",
    "validate_df    = xml_ftp_generator.validate_responses( validate_df )\n",
    "#\n",
    "# # model_name     = \"gpt-3.5-turbo-1106\"\n",
    "# # validate_df    = xml_ftp_generator.generate_responses( validate_df, switch=\"openai\", model_name= )\n",
    "# # validate_df    = xml_ftp_generator.validate_responses( validate_df )\n",
    "#\n",
    "xml_ftp_generator.print_validation_stats( validate_df, title=f\"Validation Stats for `{model_name}`\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T16:30:55.817913Z",
     "start_time": "2024-01-29T16:18:45.599229Z"
    }
   },
   "id": "22e8032eee2a36d6"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation Stats for `mistralai/Mistral-7B-Instruct-v0.2-AWQ`\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "          Contains response 100.0%\n",
      " Contains <browser-command> 100.0%\n",
      "            Contains <args> 100.0%\n",
      "          Response is exact 99.6%\n",
      "Response has correct values 99.6%\n",
      " Browser command is correct 99.6%\n",
      "            Args is correct 99.9%\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation Stats for `mistralai/Mistral-7B-Instruct-v0.2-AWQ`: Accuracy per command\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "                                            command     mean  sum  count\n",
      "                                 search current tab   99.08%  108    109\n",
      "                              search google new tab   98.96%   95     96\n",
      "                               search phind new tab   97.78%   44     45\n",
      "                 search using clipboard current tab   96.77%   30     31\n",
      "           search phind using clipboard current tab   95.83%   23     24\n",
      "      search google scholar using clipboard new tab   94.12%   16     17\n",
      "                                  go to current tab  100.00%  121    121\n",
      "                      search perplexity current tab  100.00%   48     48\n",
      "               search phind using clipboard new tab  100.00%   20     20\n",
      "                           search phind current tab  100.00%   56     56\n",
      "          search perplexity using clipboard new tab  100.00%   22     22\n",
      "      search perplexity using clipboard current tab  100.00%   22     22\n",
      "                          search perplexity new tab  100.00%   69     69\n",
      "              search google using clipboard new tab  100.00%   14     14\n",
      "                                     search new tab  100.00%  122    122\n",
      "                                      go to new tab  100.00%  122    122\n",
      "          search google using clipboard current tab  100.00%   11     11\n",
      "  search google scholar using clipboard current tab  100.00%   26     26\n",
      "                      search google scholar new tab  100.00%  144    144\n",
      "                  search google scholar current tab  100.00%  144    144\n",
      "                          search google current tab  100.00%  107    107\n",
      "                                               none  100.00%   19     19\n",
      "                     search using clipboard new tab  100.00%   19     19\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "foo = xml_ftp_generator.print_validation_stats( validate_df, title=f\"Validation Stats for `{model_name}`\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T18:55:30.132988Z",
     "start_time": "2024-01-29T18:55:29.847604Z"
    }
   },
   "id": "29753bb141448e9a"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(1408, 15)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T16:38:23.672721Z",
     "start_time": "2024-01-29T16:38:23.646259Z"
    }
   },
   "id": "b12fc8c32d542752"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "(23,)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df.command.unique().shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T16:39:38.681971Z",
     "start_time": "2024-01-29T16:39:38.569748Z"
    }
   },
   "id": "4e49e406f2a946bc"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search google current tab\n",
      "search google scholar using clipboard new tab\n",
      "search perplexity using clipboard new tab\n",
      "search google new tab\n",
      "search new tab\n",
      "search google scholar current tab\n",
      "go to new tab\n",
      "search google scholar new tab\n",
      "search current tab\n",
      "search phind new tab\n",
      "search phind using clipboard new tab\n",
      "go to current tab\n",
      "search phind current tab\n",
      "search perplexity new tab\n",
      "search google using clipboard new tab\n",
      "search perplexity current tab\n",
      "search google using clipboard current tab\n",
      "search using clipboard new tab\n",
      "search perplexity using clipboard current tab\n",
      "none\n",
      "search phind using clipboard current tab\n",
      "search google scholar using clipboard current tab\n",
      "search using clipboard current tab\n"
     ]
    }
   ],
   "source": [
    "for cmd in validate_df.command.unique(): print( cmd ) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T16:39:06.581622Z",
     "start_time": "2024-01-29T16:39:06.447661Z"
    }
   },
   "id": "f7d40d97d1d57999"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate stats on a grouped by command basis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c453baf6adf3b965"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "cols = [ \"command\", \"response_is_exact\" ]\n",
    "stats_df = validate_df[ cols ].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T17:03:24.432492Z",
     "start_time": "2024-01-29T17:03:24.348918Z"
    }
   },
   "id": "bf6af335cf491e88"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              command     mean  sum  count\n",
      "3                                  search current tab   99.08%  108    109\n",
      "5                               search google new tab   98.96%   95     96\n",
      "18                               search phind new tab   97.78%   44     45\n",
      "21                 search using clipboard current tab   96.77%   30     31\n",
      "19           search phind using clipboard current tab   95.83%   23     24\n",
      "9       search google scholar using clipboard new tab   94.12%   16     17\n",
      "0                                   go to current tab  100.00%  121    121\n",
      "13                      search perplexity current tab  100.00%   48     48\n",
      "20               search phind using clipboard new tab  100.00%   20     20\n",
      "17                           search phind current tab  100.00%   56     56\n",
      "16          search perplexity using clipboard new tab  100.00%   22     22\n",
      "15      search perplexity using clipboard current tab  100.00%   22     22\n",
      "14                          search perplexity new tab  100.00%   69     69\n",
      "11              search google using clipboard new tab  100.00%   14     14\n",
      "12                                     search new tab  100.00%  122    122\n",
      "1                                       go to new tab  100.00%  122    122\n",
      "10          search google using clipboard current tab  100.00%   11     11\n",
      "8   search google scholar using clipboard current tab  100.00%   26     26\n",
      "7                       search google scholar new tab  100.00%  144    144\n",
      "6                   search google scholar current tab  100.00%  144    144\n",
      "4                           search google current tab  100.00%  107    107\n",
      "2                                                none  100.00%   19     19\n",
      "22                     search using clipboard new tab  100.00%   19     19\n"
     ]
    }
   ],
   "source": [
    "# Group stats_df by command column And calculate 1) means for response_is_exact column, 2) sums for response_is_exact column, and 3) counts for response_is_exact column\n",
    "stats_df_grp = validate_df[ cols ].groupby( \"command\" )[ \"response_is_exact\" ].agg( [ \"mean\", \"sum\", \"count\" ] ).reset_index()\n",
    "stats_df_grp[ \"mean\" ] = stats_df_grp[ \"mean\" ].apply( lambda cell: f\"{ cell * 100:.2f}%\" )\n",
    "\n",
    "# Sort by response is exact ascending\n",
    "stats_df_grp = stats_df_grp.sort_values( \"mean\", ascending=False )\n",
    "\n",
    "print( stats_df_grp )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T17:03:25.336746Z",
     "start_time": "2024-01-29T17:03:25.282918Z"
    }
   },
   "id": "2889cae7fafaabcc"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "                                         command  response_xml_is_valid  \\\n8539                              search new tab                  False   \n13551   search phind using clipboard current tab                  False   \n14074                                       none                  False   \n12773      search google using clipboard new tab                  False   \n13388  search perplexity using clipboard new tab                  False   \n\n       contains_response  contains_browser_command  contains_args  \\\n8539                True                      True           True   \n13551               True                      True           True   \n14074               True                      True           True   \n12773               True                      True           True   \n13388               True                      True           True   \n\n       response_is_exact  response_has_correct_values  \\\n8539                True                         True   \n13551              False                        False   \n14074              False                        False   \n12773              False                        False   \n13388              False                        False   \n\n       browser_command_is_correct  args_is_correct  \n8539                         True             True  \n13551                       False            False  \n14074                       False            False  \n12773                       False            False  \n13388                       False            False  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>command</th>\n      <th>response_xml_is_valid</th>\n      <th>contains_response</th>\n      <th>contains_browser_command</th>\n      <th>contains_args</th>\n      <th>response_is_exact</th>\n      <th>response_has_correct_values</th>\n      <th>browser_command_is_correct</th>\n      <th>args_is_correct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8539</th>\n      <td>search new tab</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>13551</th>\n      <td>search phind using clipboard current tab</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>14074</th>\n      <td>none</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>12773</th>\n      <td>search google using clipboard new tab</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>13388</th>\n      <td>search perplexity using clipboard new tab</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['command', 'response_xml_is_valid', 'contains_response',\n",
    "       'contains_browser_command', 'contains_args', 'response_is_exact',\n",
    "       'response_has_correct_values', 'browser_command_is_correct',\n",
    "       'args_is_correct']\n",
    "stats_df = validate_df[ cols ]\n",
    "stats_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T21:58:41.208125Z",
     "start_time": "2024-01-23T21:58:41.108601Z"
    }
   },
   "id": "fb48024e63ddde90"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              command  response_is_exact\n0                                   go to current tab                0.0\n1                                       go to new tab                0.0\n2                                                none                0.0\n3                                  search current tab                1.0\n4                           search google current tab                1.0\n5                               search google new tab                1.0\n6                   search google scholar current tab                1.0\n7                       search google scholar new tab                1.0\n8   search google scholar using clipboard current tab                0.0\n9           search google using clipboard current tab                0.0\n10              search google using clipboard new tab                0.0\n11                                     search new tab                1.0\n12                      search perplexity current tab                0.0\n13                          search perplexity new tab                0.0\n14      search perplexity using clipboard current tab                0.0\n15          search perplexity using clipboard new tab                0.0\n16                           search phind current tab                0.0\n17                               search phind new tab                0.0\n18           search phind using clipboard current tab                0.0\n19               search phind using clipboard new tab                0.0\n20                 search using clipboard current tab                0.0\n21                     search using clipboard new tab                0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>command</th>\n      <th>response_is_exact</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>go to current tab</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>go to new tab</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>none</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>search current tab</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>search google current tab</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>search google new tab</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>search google scholar current tab</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>search google scholar new tab</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>search google scholar using clipboard current tab</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>search google using clipboard current tab</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>search google using clipboard new tab</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>search new tab</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>search perplexity current tab</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>search perplexity new tab</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>search perplexity using clipboard current tab</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>search perplexity using clipboard new tab</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>search phind current tab</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>search phind new tab</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>search phind using clipboard current tab</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>search phind using clipboard new tab</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>search using clipboard current tab</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>search using clipboard new tab</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_means_for = [ 'response_xml_is_valid', 'contains_response',\n",
    "#        'contains_browser_command', 'contains_args', 'response_is_exact',\n",
    "#        'response_has_correct_values', 'browser_command_is_correct',\n",
    "#        'args_is_correct']\n",
    "\n",
    "# Group stats_df by command column And calculate percentages for get_means_for columns\n",
    "stats_df.groupby( \"command\" )[ \"response_is_exact\" ].mean().reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T22:06:39.375889Z",
     "start_time": "2024-01-23T22:06:39.323163Z"
    }
   },
   "id": "c71ef9c1a0544876"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "319ca42267ada630"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
