{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# load auto reload module\n",
    "%load_ext autoreload"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T01:35:02.611958Z",
     "start_time": "2024-01-19T01:35:02.594760Z"
    }
   },
   "id": "18959734b7ebd73"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import json\n",
    "import wandb\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T01:35:06.423480Z",
     "start_time": "2024-01-19T01:35:03.710795Z"
    }
   },
   "id": "c213bdd2418b70f4"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28K\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Jan 18 19:13 .\r\n",
      "drwxr--r-- 34 1001 1001 4.0K Jan 18 15:17 ..\r\n",
      "drwxr-xr-x  3 root root 4.0K Jan 18 15:34 .locks\r\n",
      "drwxrwxr-x  5 1001 1001 4.0K Jan 18 19:47 Mistral-7B-Instruct-v0.2\r\n",
      "drwxr-xr-x  6 1001 1001 4.0K Jan 18 15:35 models--bigscience--bloom-560m\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Jan 18 15:59 models--mistralai--Mistral-7B-Instruct-v0.2\r\n",
      "-rw-r--r--  1 1001 1001    1 Jan 18 15:35 version.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Print current working directory\n",
    "# !ls -alh /var/model/Phind-CodeLlama-34B-v2\n",
    "# Change to /var/model/Phind-CodeLlama-34B-v2\n",
    "# os.chdir( \"/var/model/Phind-CodeLlama-34B-v2\" )\n",
    "# Print current working directory\n",
    "# os.getcwd()\n",
    "! ls -alh /var/model/models"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T01:35:06.552678Z",
     "start_time": "2024-01-19T01:35:06.426185Z"
    }
   },
   "id": "644a6196802f8630"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mricardo-felipe-ruiz\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T17:37:24.276313Z",
     "start_time": "2024-01-18T17:37:23.508246Z"
    }
   },
   "id": "d49e208217c2ea36"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=\"Mistral-7B-Instruct-v0.2\"\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=\"Mistral-7B-Instruct-v0.2\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T17:37:24.276634Z",
     "start_time": "2024-01-18T17:37:24.271076Z"
    }
   },
   "id": "d03f99f12eb04c9e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/genie-in-the-box/src\n"
     ]
    }
   ],
   "source": [
    "from xmlschema import XMLSchema\n",
    "\n",
    "os.chdir( \"/var/model/genie-in-the-box/src\" )\n",
    "print( os.getcwd() )\n",
    "import lib.utils.util         as du\n",
    "import lib.utils.util_xml     as dux\n",
    "import lib.utils.util_pytorch as dupt\n",
    "\n",
    "from ephemera.prompts.xml_fine_tuning_prompt_generator import XmlFineTuningPromptGenerator\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T01:35:15.706991Z",
     "start_time": "2024-01-19T01:35:15.482317Z"
    }
   },
   "id": "4e1faf28cf522ca7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load model and tokenizer in FP16?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bd5a4209826d64d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_base_model_and_tokenizer( model_path=\".\", tokenizer_path=\".\", use_bnb_cuantization=False, device_map=\"auto\" ):\n",
    "    \n",
    "    compute_dtype = getattr( torch, \"float16\" )\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype\n",
    "    )\n",
    "    if use_bnb_cuantization: \n",
    "\n",
    "        print( bnb_config )\n",
    "\n",
    "        # ¡OJO! Why were we turning off the cash here? It makes a big performance difference: 21 vs 14 tokens per second\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, quantization_config=bnb_config, device_map=device_map, low_cpu_mem_usage=True, use_cache=True, attn_implementation=\"flash_attention_2\"\n",
    "        )\n",
    "    else:\n",
    "        print( \"Loading without BitsAndBytesConfig...\" )\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, device_map=device_map, low_cpu_mem_usage=True, use_cache=True, attn_implementation=\"flash_attention_2\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "    \n",
    "    tokenizer              = AutoTokenizer.from_pretrained( tokenizer_path )\n",
    "    tokenizer.pad_token    = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    return base_model, tokenizer\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T01:36:02.383541Z",
     "start_time": "2024-01-19T01:36:02.368593Z"
    }
   },
   "id": "1bc8cfb9781119a5"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading without BitsAndBytesConfig...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5c3a2ad5cc14e85909a40a53b2edfca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir( \"/var/model/models/\" )\n",
    "os.getcwd()\n",
    "base_model, tokenizer = get_base_model_and_tokenizer( \n",
    "    model_path=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "    tokenizer_path=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "    use_bnb_cuantization=False, \n",
    "    device_map=\"auto\" \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T19:52:31.246928Z",
     "start_time": "2024-01-18T19:52:28.142785Z"
    }
   },
   "id": "926b612bcb0c1fdf"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralFlashAttention2(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm()\n        (post_attention_layernorm): MistralRMSNorm()\n      )\n    )\n    (norm): MistralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T21:12:18.236969Z",
     "start_time": "2024-01-18T21:12:18.225271Z"
    }
   },
   "id": "42d0f6e85a7c7639"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TEST model on validation dataset, BEFORE training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11a8cf22a4d89d12"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T19:54:26.235884Z",
     "start_time": "2024-01-18T19:54:26.213774Z"
    }
   },
   "id": "a354329a403dc278"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(100, 5)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df = pd.read_json( \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\", lines=True ).sample( 100, random_state=42 )\n",
    "validate_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T19:54:27.440075Z",
     "start_time": "2024-01-18T19:54:27.251033Z"
    }
   },
   "id": "8402364358189207"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# Path prefix allows us to find the raw text utilized in building the prompts\n",
    "xml_ftp_generator = XmlFineTuningPromptGenerator( path_prefix=\"/var/model/genie-in-the-box\", debug=True, verbose=False )\n",
    "\n",
    "validate_df = xml_ftp_generator.generate_responses( validate_df, tokenizer=tokenizer, model=base_model, switch=\"huggingface\", model_name=\"mistralai/Mistral-7B-Instruct-v0.2\" )\n",
    "validate_df = xml_ftp_generator.validate_responses( validate_df )\n",
    "\n",
    "xml_ftp_generator.print_validation_stats( validate_df, title=\"Validation stats BEFORE fine-tuning on Mistral-7B-Instruct-v0.2\" )\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# Creates insanely verbose outputs, no need to benchmark any further!\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# Response: [<response><browser-command>search google scholar current tab</browser-command><args><arg>URLError</arg></args></response>\n",
    "# \n",
    "#         Explanation:\n",
    "#         The human voice command \"Here, Google Scholar URLError\" can be broken down into the following parts:\n",
    "#         1. \"Here\" is likely an indication of the current tab, but it's not a necessary part of the command.\n",
    "#         2. \"Google Scholar\" is the search engine and the specific search type.\n",
    "#         3. \"URLError\" is likely an error message or an argument related to the command.\n",
    "# \n",
    "#         Based on this analysis, the correct standardized command is \"search google scholar current tab\" with the argument \"URLError\".</s> \n",
    "# \n",
    "#     I hope this explanation is clear and helpful. Let me know if you have any questions or need further clarification.\n",
    "# \n",
    "#     Best regards,\n",
    "#     Your helpful AI assistant.</s><response><browser-command>search google scholar current tab</browser-command><args><arg>URLError</arg></args></response>\n",
    "# \n",
    "# Explanation:\n",
    "# The human voice command \"Here, Google Scholar URLError\" can be broken down into the following parts:\n",
    "# 1. \"Here\" is likely an indication of the current tab, but it's not a necessary part of the command.\n",
    "# 2. \"Google Scholar\" is the search engine and the specific search type.\n",
    "# 3. \"URLError\" is likely an error message or an argument related to the command.\n",
    "# Based on this analysis, the correct standardized command is \"search google scholar current tab\" with the argument \"URLError\".</s>\n",
    "# \n",
    "# I hope this explanation is clear and helpful. Let me know if you have any questions or need further clarification.\n",
    "# \n",
    "# Best regards,\n",
    "# Your helpful AI assistant.</s><response><browser-command>search google scholar current tab</browser-command><args><arg>URLError</arg></args></response>\n",
    "# \n",
    "# Explanation:\n",
    "# The human voice command \"Here, Google Scholar URLError\" can be broken down into the following parts:\n",
    "# 1. \"Here\" is likely an indication of the current tab, but it's not a necessary part of the command.\n",
    "# 2. \"Google Scholar\" is the search engine and the specific search type.\n",
    "# 3. \"URLError\" is likely an error message or an argument related to the command.\n",
    "# Based on this analysis, the correct standardized command is \"search google scholar current tab\" with the argument \"URLError\".\n",
    "# \n",
    "# I hope this explanation is clear and helpful. Let me know if you have any questions or need further clarification.\n",
    "# \n",
    "# Best regards,\n",
    "# Your helpful AI assistant.</s><response xmlns=\"http://www.w3.org/2000/xmlns/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"><browser-command xsi:type=\"xsd:string\">search google scholar current tab</browser-command><args><arg xsi:type=\"xsd:string\">URLError</arg></args></response>\n",
    "# \n",
    "# Explanation:\n",
    "# The human voice command \"Here, Google Scholar URLError\" can be broken down into the following parts:\n",
    "# 1. \"Here\" is likely an indication of the current tab, but it's not a necessary part of the command.\n",
    "# 2. \"Google Scholar\" is the search engine and the specific search type.\n",
    "# 3. \"URLError\" is likely an error message or an argument related to the command.\n",
    "# Based on this analysis, the correct standardized command is \"search google scholar current tab\" with the argument \"URLError\". To ensure well-formed XML, I have added the XML namespaces and types to the response.\n",
    "# \n",
    "# I hope this explanation is clear and helpful. Let me know if you have any questions or need further clarification.\n",
    "# \n",
    "# Best regards,\n",
    "# Your helpful AI assistant.</s><response xmlns=\"http://www.w3.org/2000/xmlns/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aac1ce9b75d5790"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get training dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fcd418053c3052b"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-train.jsonl\"\n",
    "deepily_dataset_train = du.get_file_as_list( path )[ 0:1000 ]\n",
    "deepily_dataset_train = [ json.loads( line ) for line in deepily_dataset_train ]\n",
    "len( deepily_dataset_train )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T17:37:24.911258Z",
     "start_time": "2024-01-18T17:37:24.654936Z"
    }
   },
   "id": "ae76c88a9a5e791d"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "100"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-test.jsonl\"\n",
    "deepily_dataset_test = du.get_file_as_list( path )[ 0:100 ]\n",
    "deepily_dataset_test = [ json.loads( line ) for line in deepily_dataset_test ]\n",
    "len( deepily_dataset_test )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T17:37:25.001655Z",
     "start_time": "2024-01-18T17:37:24.912293Z"
    }
   },
   "id": "f1c868ec60880dcc"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Use the Task below and the Input given to write the Response, which is a programmatic instruction that can solve the following Task:\n",
    "def prompt_instruction_format( sample ):\n",
    "    \n",
    "  return f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write a Response that can solve the following Task:\n",
    "\n",
    "    ### Task:\n",
    "    {sample['instruction']}\n",
    "\n",
    "    ### Input:\n",
    "    {sample['input']}\n",
    "\n",
    "    ### Response:\n",
    "    {sample['output']}\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T17:37:25.001813Z",
     "start_time": "2024-01-18T17:37:24.949696Z"
    }
   },
   "id": "58e3fd8b1b81ce1d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "    Use the Task below and the Input given to write a Response that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
      "\n",
      "        You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\n",
      "\n",
      "        Requirement: You MUST NOT use python code to answer this question.\n",
      "        Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
      "        Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n",
      "\n",
      "    ### Input:\n",
      "    \n",
      "        Below is the raw human voice command transcription formatted using simple XML:\n",
      "        \n",
      "        <human>\n",
      "            <voice-command>Run a Azure Machine Learning platform search in the active window</voice-command>\n",
      "        </human>\n",
      "        \n",
      "        The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
      "        \n",
      "        <response>\n",
      "            <browser-command></browser-command>\n",
      "            <args></args>\n",
      "        </response>\n",
      "\n",
      "        Requirement: The first word of your response MUST be `<response>`\n",
      "\n",
      "    ### Response:\n",
      "    \n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>Azure Machine Learning platform</args>\n",
      "        </response>\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for line in prompt_instruction_format( deepily_dataset_test[ 0 ] ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T17:37:25.006727Z",
     "start_time": "2024-01-18T17:37:24.967868Z"
    }
   },
   "id": "e66898ac4c85e976"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up training arguments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "528b5d0d39d9b5df"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_config, PeftModel, PeftConfig, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64, \n",
    "    lora_alpha=32, \n",
    "    # When target_modules was disabled, it was causing detention layers to be assigned to the CPU, throwing this runtime error:\n",
    "    # RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! \n",
    "    # (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n",
    "    target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\" ], \n",
    "    lora_dropout=0.10, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T17:38:57.421747Z",
     "start_time": "2024-01-18T17:38:57.406534Z"
    }
   },
   "id": "393f4bbf9c6c3ca4"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'/var/model/models/Mistral-7B-Instruct-v0.2'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir( \"/var/model/models/Mistral-7B-Instruct-v0.2\" )\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T17:38:57.934073Z",
     "start_time": "2024-01-18T17:38:57.922595Z"
    }
   },
   "id": "abccd9b1f7ce3be5"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir=\"./training-results\", # Output directory where the model predictions and checkpoints will be stored\n",
    "    num_train_epochs=4, # Number of training epochs\n",
    "    per_device_train_batch_size=4, # Batch size per GPU for training\n",
    "    gradient_accumulation_steps=4,  # Number of update steps to accumulate the gradients for\n",
    "    gradient_checkpointing=True,# Enable gradient checkpointing\n",
    "    optim=\"paged_adamw_32bit\", # Optimizer to use\n",
    "    #save_steps=save_steps,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    \n",
    "    # Setting this may help with the warning message: The input hidden states seems to be silently casted in float32, \n",
    "    # this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n",
    "    fp16=False,\n",
    "    # Test to confirm that this works!\n",
    "    # BTW: according to PHIND, this may actually improve fine-tuning performance as well: https://www.phind.com/search?cache=ygn9dbyl0ij4kotmgns2nsrw\n",
    "    \n",
    "    bf16=True,\n",
    "    # tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    #max_steps=max_steps,\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    disable_tqdm=True,\n",
    "    report_to=\"wandb\",\n",
    "    seed=42\n",
    ")\n",
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=deepily_dataset_train,\n",
    "    eval_dataset=deepily_dataset_test,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=4096, #2048,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=prompt_instruction_format,\n",
    "    args=trainingArgs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T17:39:07.772054Z",
     "start_time": "2024-01-18T17:39:05.364909Z"
    }
   },
   "id": "e0ad2d859cefb9c4"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 170,082,304 || all params: 7,411,814,400 || trainable%: 2.29\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters( model ):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,} || all params: {all_param:,} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "    \n",
    "print_trainable_parameters( base_model )    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T17:39:08.329196Z",
     "start_time": "2024-01-18T17:39:08.316693Z"
    }
   },
   "id": "8275555ac7a5093d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3d13c347871afcd"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.2"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/var/model/models/Mistral-7B-Instruct-v0.2/wandb/run-20240118_173911-pxk4ofuw</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22/runs/pxk4ofuw' target=\"_blank\">deft-microwave-2</a></strong> to <a href='https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22/runs/pxk4ofuw' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22/runs/pxk4ofuw</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3428, 'learning_rate': 0.00018544194045464886, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1196, 'learning_rate': 0.00013348796121709862, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0789, 'learning_rate': 6.651203878290139e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0681, 'learning_rate': 1.4558059545351143e-05, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 742.5976, 'train_samples_per_second': 0.517, 'train_steps_per_second': 0.032, 'train_loss': 0.1375250555574894, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d52e34895b244eea966f9399f198cd2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▃▅▇█</td></tr><tr><td>train/global_step</td><td>▁▃▅▇█</td></tr><tr><td>train/learning_rate</td><td>█▆▃▁</td></tr><tr><td>train/loss</td><td>█▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>4.0</td></tr><tr><td>train/global_step</td><td>24</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0681</td></tr><tr><td>train/total_flos</td><td>6.87097056854016e+16</td></tr><tr><td>train/train_loss</td><td>0.13753</td></tr><tr><td>train/train_runtime</td><td>742.5976</td></tr><tr><td>train/train_samples_per_second</td><td>0.517</td></tr><tr><td>train/train_steps_per_second</td><td>0.032</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">deft-microwave-2</strong> at: <a href='https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22/runs/pxk4ofuw' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22/runs/pxk4ofuw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240118_173911-pxk4ofuw/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "#stop reporting to wandb\n",
    "wandb.finish()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()\n",
    "\n",
    "print( \"Model saved\" )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T17:51:40.078075Z",
     "start_time": "2024-01-18T17:39:10.951816Z"
    }
   },
   "id": "597fd4a8fa0f143f"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T17:49:55.354783Z",
     "start_time": "2023-11-16T17:49:55.314638Z"
    }
   },
   "id": "4f1d623ab4bc7b2e"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "2936"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "# base_model = None \n",
    "# adapter_plus_model = None\n",
    "torch.cuda.empty_cache() \n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T19:45:19.893083Z",
     "start_time": "2024-01-18T19:45:19.708749Z"
    }
   },
   "id": "23550e0e9f04149f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RESTART 1st time & load model and tokenizer in FP16"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf69f614e0a96699"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'/var/model/models'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir( \"/var/model/models/\" )\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T18:51:49.378519Z",
     "start_time": "2024-01-18T18:51:49.367491Z"
    }
   },
   "id": "46ec1294438f94de"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading without BitsAndBytesConfig...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c23171122eec484ca3b80d3c99b8374b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model, tokenizer = get_base_model_and_tokenizer( \n",
    "    model_path=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "    tokenizer_path=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "    use_bnb_cuantization=False, \n",
    "    device_map=\"auto\" \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T18:51:58.198978Z",
     "start_time": "2024-01-18T18:51:55.004208Z"
    }
   },
   "id": "a1cdb4598a75c4f7"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "'/var/model/models'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T18:56:11.634750Z",
     "start_time": "2024-01-18T18:56:11.596123Z"
    }
   },
   "id": "50c4d97554991fa1"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from peft import PeftModel, AutoPeftModelForCausalLM\n",
    "\n",
    "adapter_plus_model = PeftModel.from_pretrained( base_model, \"Mistral-7B-Instruct-v0.2/training-results\", use_flash_attention_2=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T18:56:20.293179Z",
     "start_time": "2024-01-18T18:56:18.187794Z"
    }
   },
   "id": "34796358a34b99c6"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator\n",
    "# \n",
    "# accelerator = Accelerator()\n",
    "# \n",
    "# adapter_plus_model = accelerator.prepare( adapter_plus_model )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:02:07.679677Z",
     "start_time": "2024-01-17T22:02:07.599158Z"
    }
   },
   "id": "8a8382a512c938fd"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.norm.weight: cuda:1\n",
      "base_model.model.lm_head.base_layer.weight: cuda:1\n",
      "base_model.model.lm_head.lora_A.default.weight: cuda:1\n",
      "base_model.model.lm_head.lora_B.default.weight: cuda:1\n"
     ]
    }
   ],
   "source": [
    "dupt.print_device_allocation( adapter_plus_model )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T18:53:36.361942Z",
     "start_time": "2024-01-18T18:53:36.353051Z"
    }
   },
   "id": "4a4d57dcf172749f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TEST model on validation dataset using adapter loaded on top"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78308e05900f5eef"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T18:53:49.408316Z",
     "start_time": "2024-01-18T18:53:49.398922Z"
    }
   },
   "id": "d58ff8811961a074"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(100, 5)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df = pd.read_json( \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\", lines=True ).sample( 100, random_state=42 )\n",
    "validate_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T18:54:29.607507Z",
     "start_time": "2024-01-18T18:54:29.378417Z"
    }
   },
   "id": "21de2f127247b122"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands file for [search new tab] exists: True\n",
      "Commands file for [search current tab] exists: True\n",
      "Commands file for [search google new tab] exists: True\n",
      "Commands file for [search google current tab] exists: True\n",
      "Commands file for [search google scholar new tab] exists: True\n",
      "Commands file for [search google scholar current tab] exists: True\n",
      "\n",
      "Using HuggingFace model_name [mistralai/Mistral-7B-Instruct-v0.2] in memory...\n",
      "\n",
      "Processing call [001] out of [100] = [1.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,425 ms\n",
      "Tokens per second [68.5]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>URLError</args></response>]\n",
      "\n",
      "Processing call [002] out of [100] = [2.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,347 ms\n",
      "Tokens per second [96.3]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [003] out of [100] = [3.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,433 ms\n",
      "Tokens per second [98.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>AI in supply chain optimization: How does AI contribute to supply chain optimization?</args></response>]\n",
      "\n",
      "Processing call [004] out of [100] = [4.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,846 ms\n",
      "Tokens per second [90.5]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>OverflowError</args></response>]\n",
      "\n",
      "Processing call [005] out of [100] = [5.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,891 ms\n",
      "Tokens per second [92.0]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Connection Aborted Error</args></response>]\n",
      "\n",
      "Processing call [006] out of [100] = [6.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,892 ms\n",
      "Tokens per second [83.5]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>healthy snack ideas</args></response>]\n",
      "\n",
      "Processing call [007] out of [100] = [7.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,024 ms\n",
      "Tokens per second [90.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>how to play guitar chords</args></response>]\n",
      "\n",
      "Processing call [008] out of [100] = [8.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,854 ms\n",
      "Tokens per second [91.7]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>EnvironmentError</args></response>]\n",
      "\n",
      "Processing call [009] out of [100] = [9.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,852 ms\n",
      "Tokens per second [88.0]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Invalid Operation</args></response>]\n",
      "\n",
      "Processing call [010] out of [100] = [10.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,171 ms\n",
      "Tokens per second [83.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>how do i cure dandruff?</args></response>]\n",
      "\n",
      "Processing call [011] out of [100] = [11.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,852 ms\n",
      "Tokens per second [85.9]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>exchange rates today</args></response>]\n",
      "\n",
      "Processing call [012] out of [100] = [12.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,400 ms\n",
      "Tokens per second [82.5]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Out Of Bounds Timedelta: Out of bounds for timedelta</args></response>]\n",
      "\n",
      "Processing call [013] out of [100] = [13.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,990 ms\n",
      "Tokens per second [91.5]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Responsible AI practices</args></response>]\n",
      "\n",
      "Processing call [014] out of [100] = [14.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,729 ms\n",
      "Tokens per second [89.4]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>ROC Curve analysis: How is ROC curve analysis utilized to evaluate the performance of binary classifiers?</args></response>]\n",
      "\n",
      "Processing call [015] out of [100] = [15.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,849 ms\n",
      "Tokens per second [85.5]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI in gaming</args></response>]\n",
      "\n",
      "Processing call [016] out of [100] = [16.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,981 ms\n",
      "Tokens per second [90.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>AI ethics in research</args></response>]\n",
      "\n",
      "Processing call [017] out of [100] = [17.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,765 ms\n",
      "Tokens per second [88.4]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Invalid Operation</args></response>]\n",
      "\n",
      "Processing call [018] out of [100] = [18.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,074 ms\n",
      "Tokens per second [89.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Syntax Warning: Syntax issue warning</args></response>]\n",
      "\n",
      "Processing call [019] out of [100] = [19.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,852 ms\n",
      "Tokens per second [88.6]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>UnicodeTranslateError</args></response>]\n",
      "\n",
      "Processing call [020] out of [100] = [20.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,847 ms\n",
      "Tokens per second [86.6]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Import Warning</args></response>]\n",
      "\n",
      "Processing call [021] out of [100] = [21.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,169 ms\n",
      "Tokens per second [87.1]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Joining and concatenating in Pandas</args></response>]\n",
      "\n",
      "Processing call [022] out of [100] = [22.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,891 ms\n",
      "Tokens per second [87.8]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>current news in tech</args></response>]\n",
      "\n",
      "Processing call [023] out of [100] = [23.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,971 ms\n",
      "Tokens per second [90.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>DIY home improvement projects</args></response>]\n",
      "\n",
      "Processing call [024] out of [100] = [24.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,106 ms\n",
      "Tokens per second [88.8]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>what is the capital of Australia?</args></response>]\n",
      "\n",
      "Processing call [025] out of [100] = [25.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,845 ms\n",
      "Tokens per second [92.1]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Cross-Validation techniques</args></response>]\n",
      "\n",
      "Processing call [026] out of [100] = [26.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,933 ms\n",
      "Tokens per second [87.4]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Pandas and regular expressions</args></response>]\n",
      "\n",
      "Processing call [027] out of [100] = [27.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,474 ms\n",
      "Tokens per second [90.9]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>What are user-defined warnings in Python, and how can they be effectively used?</args></response>]\n",
      "\n",
      "Processing call [028] out of [100] = [28.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,065 ms\n",
      "Tokens per second [90.6]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>shopping for groceries online</args></response>]\n",
      "\n",
      "Processing call [029] out of [100] = [29.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,979 ms\n",
      "Tokens per second [94.5]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>DIY home improvement projects</args></response>]\n",
      "\n",
      "Processing call [030] out of [100] = [30.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,902 ms\n",
      "Tokens per second [89.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Resource Warning</args></response>]\n",
      "\n",
      "Processing call [031] out of [100] = [31.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,935 ms\n",
      "Tokens per second [86.3]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Apache Spark data processing</args></response>]\n",
      "\n",
      "Processing call [032] out of [100] = [32.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,383 ms\n",
      "Tokens per second [99.5]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Responsible AI practices: How can organizations implement responsible AI practices?</args></response>]\n",
      "\n",
      "Processing call [033] out of [100] = [33.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,893 ms\n",
      "Tokens per second [89.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>current news in tech</args></response>]\n",
      "\n",
      "Processing call [034] out of [100] = [34.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,022 ms\n",
      "Tokens per second [78.6]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [035] out of [100] = [35.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,936 ms\n",
      "Tokens per second [91.9]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Connection Aborted Error</args></response>]\n",
      "\n",
      "Processing call [036] out of [100] = [36.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,386 ms\n",
      "Tokens per second [88.4]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>What are memory-related errors in Python, and how can they be prevented?</args></response>]\n",
      "\n",
      "Processing call [037] out of [100] = [37.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,065 ms\n",
      "Tokens per second [90.6]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Normalization vs. Standardization</args></response>]\n",
      "\n",
      "Processing call [038] out of [100] = [38.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,575 ms\n",
      "Tokens per second [91.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>What steps can be taken to address HTTP-related errors in web programming with Python?</args></response>]\n",
      "\n",
      "Processing call [039] out of [100] = [39.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,848 ms\n",
      "Tokens per second [84.4]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>ZeroDivisionError</args></response>]\n",
      "\n",
      "Processing call [040] out of [100] = [40.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,635 ms\n",
      "Tokens per second [97.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>AI and blockchain integration: What are the benefits of integrating AI with blockchain technology?</args></response>]\n",
      "\n",
      "Processing call [041] out of [100] = [41.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,938 ms\n",
      "Tokens per second [93.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>best restaurants near me</args></response>]\n",
      "\n",
      "Processing call [042] out of [100] = [42.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,032 ms\n",
      "Tokens per second [84.2]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Deep Learning in genomics</args></response>]\n",
      "\n",
      "Processing call [043] out of [100] = [43.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,845 ms\n",
      "Tokens per second [85.6]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>NotImplementedError</args></response>]\n",
      "\n",
      "Processing call [044] out of [100] = [44.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,053 ms\n",
      "Tokens per second [88.7]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>GANs recent advancements</args></response>]\n",
      "\n",
      "Processing call [045] out of [100] = [45.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,904 ms\n",
      "Tokens per second [84.6]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>how to clean your room</args></response>]\n",
      "\n",
      "Processing call [046] out of [100] = [46.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,946 ms\n",
      "Tokens per second [86.8]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Using Pandas with big data</args></response>]\n",
      "\n",
      "Processing call [047] out of [100] = [47.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,772 ms\n",
      "Tokens per second [85.8]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>OverflowError</args></response>]\n",
      "\n",
      "Processing call [048] out of [100] = [48.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,988 ms\n",
      "Tokens per second [88.5]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>best hiking trails near me</args></response>]\n",
      "\n",
      "Processing call [049] out of [100] = [49.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,804 ms\n",
      "Tokens per second [86.5]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Bytes Warning</args></response>]\n",
      "\n",
      "Processing call [050] out of [100] = [50.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,433 ms\n",
      "Tokens per second [98.6]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Confusion Matrix interpretation: How do you interpret a confusion matrix in a classification problem?</args></response>]\n",
      "\n",
      "Processing call [051] out of [100] = [51.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,849 ms\n",
      "Tokens per second [86.5]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>AI in human resources</args></response>]\n",
      "\n",
      "Processing call [052] out of [100] = [52.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,023 ms\n",
      "Tokens per second [91.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Visualizing data with Pandas</args></response>]\n",
      "\n",
      "Processing call [053] out of [100] = [53.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,024 ms\n",
      "Tokens per second [93.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Aggregation functions in Pandas</args></response>]\n",
      "\n",
      "Processing call [054] out of [100] = [54.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,800 ms\n",
      "Tokens per second [90.6]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Runtime Error</args></response>]\n",
      "\n",
      "Processing call [055] out of [100] = [55.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,115 ms\n",
      "Tokens per second [87.5]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Exporting data from Pandas to Excel</args></response>]\n",
      "\n",
      "Processing call [056] out of [100] = [56.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,341 ms\n",
      "Tokens per second [91.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How can you resolve errors related to ZIP file operations in Python?</args></response>]\n",
      "\n",
      "Processing call [057] out of [100] = [57.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,112 ms\n",
      "Tokens per second [90.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>User Warning: User-defined warning</args></response>]\n",
      "\n",
      "Processing call [058] out of [100] = [58.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,887 ms\n",
      "Tokens per second [93.3]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>AI in natural language generation</args></response>]\n",
      "\n",
      "Processing call [059] out of [100] = [59.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,385 ms\n",
      "Tokens per second [91.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How do you resolve issues when a required module is not found in Python?</args></response>]\n",
      "\n",
      "Processing call [060] out of [100] = [60.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,650 ms\n",
      "Tokens per second [97.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Apache Spark data processing: How does Apache Spark handle large-scale data processing efficiently?</args></response>]\n",
      "\n",
      "Processing call [061] out of [100] = [61.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,076 ms\n",
      "Tokens per second [91.0]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Rolling and expanding windows in Pandas</args></response>]\n",
      "\n",
      "Processing call [062] out of [100] = [62.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,387 ms\n",
      "Tokens per second [98.4]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Image Recognition technologies: How have image recognition technologies evolved in recent years?</args></response>]\n",
      "\n",
      "Processing call [063] out of [100] = [63.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,698 ms\n",
      "Tokens per second [99.7]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>What techniques can you use to resolve import errors that occur when Python cannot find a module or its members during import?</args></response>]\n",
      "\n",
      "Processing call [064] out of [100] = [64.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,023 ms\n",
      "Tokens per second [98.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Runtime Error: Runtime exception occurred</args></response>]\n",
      "\n",
      "Processing call [065] out of [100] = [65.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,031 ms\n",
      "Tokens per second [85.2]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Data imputation with Pandas</args></response>]\n",
      "\n",
      "Processing call [066] out of [100] = [66.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,852 ms\n",
      "Tokens per second [88.6]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>find new music</args></response>]\n",
      "\n",
      "Processing call [067] out of [100] = [67.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,841 ms\n",
      "Tokens per second [88.0]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>InterruptedError</args></response>]\n",
      "\n",
      "Processing call [068] out of [100] = [68.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,465 ms\n",
      "Tokens per second [94.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>How do you handle arithmetic errors in Python, especially with numeric calculations?</args></response>]\n",
      "\n",
      "Processing call [069] out of [100] = [69.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,420 ms\n",
      "Tokens per second [102.1]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>AI in agriculture technology: How is AI being utilized in modern agricultural technology?</args></response>]\n",
      "\n",
      "Processing call [070] out of [100] = [70.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,898 ms\n",
      "Tokens per second [91.1]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Unicode Warning</args></response>]\n",
      "\n",
      "Processing call [071] out of [100] = [71.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,580 ms\n",
      "Tokens per second [93.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>How do you address errors related to attempting to access non-existent files in Python?</args></response>]\n",
      "\n",
      "Processing call [072] out of [100] = [72.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,949 ms\n",
      "Tokens per second [93.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Google Cloud AI services</args></response>]\n",
      "\n",
      "Processing call [073] out of [100] = [73.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,858 ms\n",
      "Tokens per second [95.2]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Feature Engineering best practices</args></response>]\n",
      "\n",
      "Processing call [074] out of [100] = [74.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,855 ms\n",
      "Tokens per second [86.3]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AssertionError</args></response>]\n",
      "\n",
      "Processing call [075] out of [100] = [75.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,033 ms\n",
      "Tokens per second [85.6]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Performance tuning in Pandas</args></response>]\n",
      "\n",
      "Processing call [076] out of [100] = [76.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,660 ms\n",
      "Tokens per second [87.2]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI-driven chatbots: How are AI-driven chatbots enhancing customer service experiences?</args></response>]\n",
      "\n",
      "Processing call [077] out of [100] = [77.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,947 ms\n",
      "Tokens per second [80.6]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>best sci-fi novels</args></response>]\n",
      "\n",
      "Processing call [078] out of [100] = [78.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,078 ms\n",
      "Tokens per second [93.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Frequency conversion in time series data</args></response>]\n",
      "\n",
      "Processing call [079] out of [100] = [79.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,801 ms\n",
      "Tokens per second [87.2]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>SyntaxError</args></response>]\n",
      "\n",
      "Processing call [080] out of [100] = [80.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,937 ms\n",
      "Tokens per second [86.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>why cats purr</args></response>]\n",
      "\n",
      "Processing call [081] out of [100] = [81.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,121 ms\n",
      "Tokens per second [88.6]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>User Warning: User-defined warning</args></response>]\n",
      "\n",
      "Processing call [082] out of [100] = [82.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,888 ms\n",
      "Tokens per second [91.6]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>FileExistsError</args></response>]\n",
      "\n",
      "Processing call [083] out of [100] = [83.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,937 ms\n",
      "Tokens per second [88.3]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Using lambda functions in Pandas</args></response>]\n",
      "\n",
      "Processing call [084] out of [100] = [84.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,384 ms\n",
      "Tokens per second [94.8]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>How do you manage situations leading to an unexpected exit of generators in Python?</args></response>]\n",
      "\n",
      "Processing call [085] out of [100] = [85.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,564 ms\n",
      "Tokens per second [89.7]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Explainable AI: Why is explainable AI important, and how is it achieved?</args></response>]\n",
      "\n",
      "Processing call [086] out of [100] = [86.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,471 ms\n",
      "Tokens per second [97.5]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>What steps can be taken to resolve errors related to aborted connections in Python?</args></response>]\n",
      "\n",
      "Processing call [087] out of [100] = [87.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,935 ms\n",
      "Tokens per second [92.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>File Not Found Error</args></response>]\n",
      "\n",
      "Processing call [088] out of [100] = [88.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,845 ms\n",
      "Tokens per second [91.1]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Connection Reset Error</args></response>]\n",
      "\n",
      "Processing call [089] out of [100] = [89.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,517 ms\n",
      "Tokens per second [89.4]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>What are the causes of floating point errors in Python, and how can they be minimized?</args></response>]\n",
      "\n",
      "Processing call [090] out of [100] = [90.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,476 ms\n",
      "Tokens per second [90.5]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI ethics in research: What are the key ethical considerations in AI research?</args></response>]\n",
      "\n",
      "Processing call [091] out of [100] = [91.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,979 ms\n",
      "Tokens per second [92.0]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>latest breakthroughs in medicine</args></response>]\n",
      "\n",
      "Processing call [092] out of [100] = [92.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,976 ms\n",
      "Tokens per second [87.6]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Data manipulation in Pandas</args></response>]\n",
      "\n",
      "Processing call [093] out of [100] = [93.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,473 ms\n",
      "Tokens per second [100.3]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>What are the best practices for managing timeout errors in Python, especially in network requests?</args></response>]\n",
      "\n",
      "Processing call [094] out of [100] = [94.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,978 ms\n",
      "Tokens per second [84.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI-driven chatbots</args></response>]\n",
      "\n",
      "Processing call [095] out of [100] = [95.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,023 ms\n",
      "Tokens per second [86.0]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Data anonymization in Pandas</args></response>]\n",
      "\n",
      "Processing call [096] out of [100] = [96.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,478 ms\n",
      "Tokens per second [94.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How do you handle situations where a feature or method is not yet implemented in Python?</args></response>]\n",
      "\n",
      "Processing call [097] out of [100] = [97.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,976 ms\n",
      "Tokens per second [88.6]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>what is the stock market?</args></response>]\n",
      "\n",
      "Processing call [098] out of [100] = [98.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,933 ms\n",
      "Tokens per second [85.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>IBM Watson AI tools</args></response>]\n",
      "\n",
      "Processing call [099] out of [100] = [99.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 1,979 ms\n",
      "Tokens per second [91.5]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>System Error: Internal Python system issue</args></response>]\n",
      "\n",
      "Processing call [100] out of [100] = [100.0%]... \n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]...\n",
      "Asking LLM [Phind/Phind-CodeLlama-34B-v2]... Done! in 2,429 ms\n",
      "Tokens per second [106.6]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Feature Engineering best practices: How does feature engineering improve the performance of machine learning models?</args></response>]\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "# Path prefix allows us to find the raw text utilized in building the prompts\n",
    "xml_ftp_generator = XmlFineTuningPromptGenerator( path_prefix=\"/var/model/genie-in-the-box\", debug=True, verbose=False )\n",
    "\n",
    "validate_df = xml_ftp_generator.generate_responses( validate_df, tokenizer=tokenizer, model=adapter_plus_model, switch=\"huggingface\", model_name=\"mistralai/Mistral-7B-Instruct-v0.2\" )\n",
    "validate_df = xml_ftp_generator.validate_responses( validate_df )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T19:00:29.801996Z",
     "start_time": "2024-01-18T18:56:59.788926Z"
    }
   },
   "id": "2153d4846d07b5d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xml_ftp_generator.print_validation_stats( validate_df )\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation Stats w/ adapter loaded on top\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#    Contains browser command 100.0%\n",
    "#               Contains args 100.0%\n",
    "#           Response is exact 100.0%\n",
    "# Response has correct values 100.0%\n",
    "#  Browser command is correct 100.0%\n",
    "#             Args is correct 100.0%"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "877b632be4a30367"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aeced5d6b29ca55d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perform a naïve merge & write to disk"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "554df92b15666503"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "os.chdir( \"/var/model/models/Mistral-7B-Instruct-v0.2\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T19:13:32.356555Z",
     "start_time": "2024-01-18T19:13:32.339679Z"
    }
   },
   "id": "1daa10cd14621325"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "adapter_plus_model = adapter_plus_model.merge_and_unload()\n",
    "adapter_plus_model.save_pretrained( \"./merged/\", safe_serialization=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T19:12:21.024320Z",
     "start_time": "2024-01-18T19:12:07.503445Z"
    }
   },
   "id": "7438bad14a9b0c12"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "('./merged-naive/tokenizer_config.json',\n './merged-naive/special_tokens_map.json',\n './merged-naive/tokenizer.model',\n './merged-naive/added_tokens.json',\n './merged-naive/tokenizer.json')"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained( \"./merged/\", safe_serialization=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T19:14:36.475863Z",
     "start_time": "2024-01-18T19:14:36.430911Z"
    }
   },
   "id": "1328c02bea1c4669"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RESTART 2nd time & load NAIVELY merged model + tokenizer in FP16"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d2de18b760eb37d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/models/Mistral-7B-Instruct-v0.2/merged\n",
      "Loading without BitsAndBytesConfig...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c16bb02f505b441a866c30c43c759b10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir( \"/var/model/models/Mistral-7B-Instruct-v0.2/merged\" )\n",
    "print( os.getcwd() )\n",
    "\n",
    "base_model, tokenizer = get_base_model_and_tokenizer( \n",
    "    use_bnb_cuantization=False, \n",
    "    device_map=\"cuda:1\" \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T01:36:46.092153Z",
     "start_time": "2024-01-19T01:36:43.173535Z"
    }
   },
   "id": "40f85a90c6d1f92e"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(100, 5)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "validate_df = pd.read_json( \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\", lines=True ).sample( 100, random_state=42 )\n",
    "validate_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T01:37:06.332213Z",
     "start_time": "2024-01-19T01:37:06.144085Z"
    }
   },
   "id": "c237964b9637c5c3"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "print( base_model.device )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T01:37:07.317658Z",
     "start_time": "2024-01-19T01:37:07.293425Z"
    }
   },
   "id": "ebca787ad2268aae"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands file for [search new tab] exists: True\n",
      "Commands file for [search current tab] exists: True\n",
      "Commands file for [search google new tab] exists: True\n",
      "Commands file for [search google current tab] exists: True\n",
      "Commands file for [search google scholar new tab] exists: True\n",
      "Commands file for [search google scholar current tab] exists: True\n",
      "\n",
      "Generating responses for 100 rows...\n",
      "Using HuggingFace model_name [mistralai/Mistral-7B-Instruct-v0.2] in memory...\n",
      "\n",
      "Processing call [001] out of [100] = [1.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,564 ms\n",
      "Tokens per second [106.1]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>URLError</args></response>]\n",
      "\n",
      "Processing call [002] out of [100] = [2.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,532 ms\n",
      "Tokens per second [147.5]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>How can system exit errors be handled gracefully in Python applications?</args></response>]\n",
      "\n",
      "Processing call [003] out of [100] = [3.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,589 ms\n",
      "Tokens per second [150.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>AI in supply chain optimization: How does AI contribute to supply chain optimization?</args></response>]\n",
      "\n",
      "Processing call [004] out of [100] = [4.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,208 ms\n",
      "Tokens per second [138.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>OverflowError</args></response>]\n",
      "\n",
      "Processing call [005] out of [100] = [5.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,235 ms\n",
      "Tokens per second [140.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Connection Aborted Error</args></response>]\n",
      "\n",
      "Processing call [006] out of [100] = [6.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,235 ms\n",
      "Tokens per second [127.8]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>healthy snack ideas</args></response>]\n",
      "\n",
      "Processing call [007] out of [100] = [7.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,323 ms\n",
      "Tokens per second [138.3]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>how to play guitar chords</args></response>]\n",
      "\n",
      "Processing call [008] out of [100] = [8.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,207 ms\n",
      "Tokens per second [140.8]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>EnvironmentError</args></response>]\n",
      "\n",
      "Processing call [009] out of [100] = [9.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,178 ms\n",
      "Tokens per second [138.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Invalid Operation</args></response>]\n",
      "\n",
      "Processing call [010] out of [100] = [10.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,415 ms\n",
      "Tokens per second [127.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>how do i cure dandruff?</args></response>]\n",
      "\n",
      "Processing call [011] out of [100] = [11.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,207 ms\n",
      "Tokens per second [131.7]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>exchange rates today</args></response>]\n",
      "\n",
      "Processing call [012] out of [100] = [12.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,560 ms\n",
      "Tokens per second [126.9]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Out Of Bounds Timedelta: Out of bounds for timedelta</args></response>]\n",
      "\n",
      "Processing call [013] out of [100] = [13.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,300 ms\n",
      "Tokens per second [140.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Responsible AI practices</args></response>]\n",
      "\n",
      "Processing call [014] out of [100] = [14.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,733 ms\n",
      "Tokens per second [140.8]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>ROC Curve analysis: How is ROC curve analysis utilized to evaluate the performance of binary classifiers?</args></response>]\n",
      "\n",
      "Processing call [015] out of [100] = [15.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,208 ms\n",
      "Tokens per second [130.8]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI in gaming</args></response>]\n",
      "\n",
      "Processing call [016] out of [100] = [16.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,295 ms\n",
      "Tokens per second [138.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>AI ethics in research</args></response>]\n",
      "\n",
      "Processing call [017] out of [100] = [17.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,150 ms\n",
      "Tokens per second [135.7]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Invalid Operation</args></response>]\n",
      "\n",
      "Processing call [018] out of [100] = [18.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,352 ms\n",
      "Tokens per second [137.6]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Syntax Warning: Syntax issue warning</args></response>]\n",
      "\n",
      "Processing call [019] out of [100] = [19.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,207 ms\n",
      "Tokens per second [135.9]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>UnicodeTranslateError</args></response>]\n",
      "\n",
      "Processing call [020] out of [100] = [20.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,208 ms\n",
      "Tokens per second [132.5]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Import Warning</args></response>]\n",
      "\n",
      "Processing call [021] out of [100] = [21.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,415 ms\n",
      "Tokens per second [133.6]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Joining and concatenating in Pandas</args></response>]\n",
      "\n",
      "Processing call [022] out of [100] = [22.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,236 ms\n",
      "Tokens per second [134.3]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>current news in tech</args></response>]\n",
      "\n",
      "Processing call [023] out of [100] = [23.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,265 ms\n",
      "Tokens per second [141.5]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>DIY home improvement projects</args></response>]\n",
      "\n",
      "Processing call [024] out of [100] = [24.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,356 ms\n",
      "Tokens per second [137.9]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>what is the capital of Australia?</args></response>]\n",
      "\n",
      "Processing call [025] out of [100] = [25.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,206 ms\n",
      "Tokens per second [141.0]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Cross-Validation techniques</args></response>]\n",
      "\n",
      "Processing call [026] out of [100] = [26.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,265 ms\n",
      "Tokens per second [133.6]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Pandas and regular expressions</args></response>]\n",
      "\n",
      "Processing call [027] out of [100] = [27.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,617 ms\n",
      "Tokens per second [139.1]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>What are user-defined warnings in Python, and how can they be effectively used?</args></response>]\n",
      "\n",
      "Processing call [028] out of [100] = [28.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,356 ms\n",
      "Tokens per second [137.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>shopping for groceries online</args></response>]\n",
      "\n",
      "Processing call [029] out of [100] = [29.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,293 ms\n",
      "Tokens per second [144.6]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>DIY home improvement projects</args></response>]\n",
      "\n",
      "Processing call [030] out of [100] = [30.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,236 ms\n",
      "Tokens per second [137.5]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Resource Warning</args></response>]\n",
      "\n",
      "Processing call [031] out of [100] = [31.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,264 ms\n",
      "Tokens per second [132.1]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Apache Spark data processing</args></response>]\n",
      "\n",
      "Processing call [032] out of [100] = [32.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,560 ms\n",
      "Tokens per second [151.9]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Responsible AI practices: How can organizations implement responsible AI practices?</args></response>]\n",
      "\n",
      "Processing call [033] out of [100] = [33.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,235 ms\n",
      "Tokens per second [137.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>current news in tech</args></response>]\n",
      "\n",
      "Processing call [034] out of [100] = [34.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,322 ms\n",
      "Tokens per second [120.3]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>AWS SageMaker for ML</args></response>]\n",
      "\n",
      "Processing call [035] out of [100] = [35.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,263 ms\n",
      "Tokens per second [140.9]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Connection Aborted Error</args></response>]\n",
      "\n",
      "Processing call [036] out of [100] = [36.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,560 ms\n",
      "Tokens per second [135.3]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>What are memory-related errors in Python, and how can they be prevented?</args></response>]\n",
      "\n",
      "Processing call [037] out of [100] = [37.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,329 ms\n",
      "Tokens per second [140.7]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Normalization vs. Standardization</args></response>]\n",
      "\n",
      "Processing call [038] out of [100] = [38.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,618 ms\n",
      "Tokens per second [145.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>What steps can be taken to address HTTP-related errors in web programming with Python?</args></response>]\n",
      "\n",
      "Processing call [039] out of [100] = [39.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,207 ms\n",
      "Tokens per second [129.2]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>ZeroDivisionError</args></response>]\n",
      "\n",
      "Processing call [040] out of [100] = [40.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,704 ms\n",
      "Tokens per second [150.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>AI and blockchain integration: What are the benefits of integrating AI with blockchain technology?</args></response>]\n",
      "\n",
      "Processing call [041] out of [100] = [41.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,265 ms\n",
      "Tokens per second [143.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>best restaurants near me</args></response>]\n",
      "\n",
      "Processing call [042] out of [100] = [42.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,265 ms\n",
      "Tokens per second [135.2]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Deep Learning in genomics</args></response>]\n",
      "\n",
      "Processing call [043] out of [100] = [43.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,208 ms\n",
      "Tokens per second [130.8]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>NotImplementedError</args></response>]\n",
      "\n",
      "Processing call [044] out of [100] = [44.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,328 ms\n",
      "Tokens per second [137.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>GANs recent advancements</args></response>]\n",
      "\n",
      "Processing call [045] out of [100] = [45.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,236 ms\n",
      "Tokens per second [130.3]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>how to clean your room</args></response>]\n",
      "\n",
      "Processing call [046] out of [100] = [46.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,264 ms\n",
      "Tokens per second [133.7]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Using Pandas with big data</args></response>]\n",
      "\n",
      "Processing call [047] out of [100] = [47.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,149 ms\n",
      "Tokens per second [132.3]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>OverflowError</args></response>]\n",
      "\n",
      "Processing call [048] out of [100] = [48.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,293 ms\n",
      "Tokens per second [136.1]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>best hiking trails near me</args></response>]\n",
      "\n",
      "Processing call [049] out of [100] = [49.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,179 ms\n",
      "Tokens per second [132.3]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Bytes Warning</args></response>]\n",
      "\n",
      "Processing call [050] out of [100] = [50.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,594 ms\n",
      "Tokens per second [150.6]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Confusion Matrix interpretation: How do you interpret a confusion matrix in a classification problem?</args></response>]\n",
      "\n",
      "Processing call [051] out of [100] = [51.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,207 ms\n",
      "Tokens per second [132.6]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>AI in human resources</args></response>]\n",
      "\n",
      "Processing call [052] out of [100] = [52.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,324 ms\n",
      "Tokens per second [140.5]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Visualizing data with Pandas</args></response>]\n",
      "\n",
      "Processing call [053] out of [100] = [53.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,322 ms\n",
      "Tokens per second [143.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Aggregation functions in Pandas</args></response>]\n",
      "\n",
      "Processing call [054] out of [100] = [54.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,178 ms\n",
      "Tokens per second [138.4]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Runtime Error</args></response>]\n",
      "\n",
      "Processing call [055] out of [100] = [55.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,397 ms\n",
      "Tokens per second [132.4]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Exporting data from Pandas to Excel</args></response>]\n",
      "\n",
      "Processing call [056] out of [100] = [56.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,531 ms\n",
      "Tokens per second [139.8]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How can you resolve errors related to ZIP file operations in Python?</args></response>]\n",
      "\n",
      "Processing call [057] out of [100] = [57.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,381 ms\n",
      "Tokens per second [139.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>User Warning: User-defined warning</args></response>]\n",
      "\n",
      "Processing call [058] out of [100] = [58.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,237 ms\n",
      "Tokens per second [142.3]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>AI in natural language generation</args></response>]\n",
      "\n",
      "Processing call [059] out of [100] = [59.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,559 ms\n",
      "Tokens per second [139.8]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How do you resolve issues when a required module is not found in Python?</args></response>]\n",
      "\n",
      "Processing call [060] out of [100] = [60.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,732 ms\n",
      "Tokens per second [148.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Apache Spark data processing: How does Apache Spark handle large-scale data processing efficiently?</args></response>]\n",
      "\n",
      "Processing call [061] out of [100] = [61.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,352 ms\n",
      "Tokens per second [139.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Rolling and expanding windows in Pandas</args></response>]\n",
      "\n",
      "Processing call [062] out of [100] = [62.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,559 ms\n",
      "Tokens per second [150.7]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Image Recognition technologies: How have image recognition technologies evolved in recent years?</args></response>]\n",
      "\n",
      "Processing call [063] out of [100] = [63.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,761 ms\n",
      "Tokens per second [152.8]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>What techniques can you use to resolve import errors that occur when Python cannot find a module or its members during import?</args></response>]\n",
      "\n",
      "Processing call [064] out of [100] = [64.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,324 ms\n",
      "Tokens per second [150.3]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Runtime Error: Runtime exception occurred</args></response>]\n",
      "\n",
      "Processing call [065] out of [100] = [65.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,330 ms\n",
      "Tokens per second [130.1]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Data imputation with Pandas</args></response>]\n",
      "\n",
      "Processing call [066] out of [100] = [66.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,207 ms\n",
      "Tokens per second [135.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>find new music</args></response>]\n",
      "\n",
      "Processing call [067] out of [100] = [67.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,207 ms\n",
      "Tokens per second [134.2]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>InterruptedError</args></response>]\n",
      "\n",
      "Processing call [068] out of [100] = [68.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,617 ms\n",
      "Tokens per second [144.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>How do you handle arithmetic errors in Python, especially with numeric calculations?</args></response>]\n",
      "\n",
      "Processing call [069] out of [100] = [69.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,588 ms\n",
      "Tokens per second [155.5]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>AI in agriculture technology: How is AI being utilized in modern agricultural technology?</args></response>]\n",
      "\n",
      "Processing call [070] out of [100] = [70.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,236 ms\n",
      "Tokens per second [139.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Unicode Warning</args></response>]\n",
      "\n",
      "Processing call [071] out of [100] = [71.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,674 ms\n",
      "Tokens per second [144.0]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>How do you address errors related to attempting to access non-existent files in Python?</args></response>]\n",
      "\n",
      "Processing call [072] out of [100] = [72.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,265 ms\n",
      "Tokens per second [143.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Google Cloud AI services</args></response>]\n",
      "\n",
      "Processing call [073] out of [100] = [73.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,208 ms\n",
      "Tokens per second [146.5]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Feature Engineering best practices</args></response>]\n",
      "\n",
      "Processing call [074] out of [100] = [74.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,206 ms\n",
      "Tokens per second [132.7]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AssertionError</args></response>]\n",
      "\n",
      "Processing call [075] out of [100] = [75.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,323 ms\n",
      "Tokens per second [131.5]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Performance tuning in Pandas</args></response>]\n",
      "\n",
      "Processing call [076] out of [100] = [76.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,733 ms\n",
      "Tokens per second [133.9]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI-driven chatbots: How are AI-driven chatbots enhancing customer service experiences?</args></response>]\n",
      "\n",
      "Processing call [077] out of [100] = [77.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,265 ms\n",
      "Tokens per second [124.1]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>best sci-fi novels</args></response>]\n",
      "\n",
      "Processing call [078] out of [100] = [78.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,357 ms\n",
      "Tokens per second [143.0]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Frequency conversion in time series data</args></response>]\n",
      "\n",
      "Processing call [079] out of [100] = [79.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,179 ms\n",
      "Tokens per second [133.2]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>SyntaxError</args></response>]\n",
      "\n",
      "Processing call [080] out of [100] = [80.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,265 ms\n",
      "Tokens per second [132.0]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>why cats purr</args></response>]\n",
      "\n",
      "Processing call [081] out of [100] = [81.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,387 ms\n",
      "Tokens per second [135.5]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>User Warning: User-defined warning</args></response>]\n",
      "\n",
      "Processing call [082] out of [100] = [82.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,236 ms\n",
      "Tokens per second [140.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>FileExistsError</args></response>]\n",
      "\n",
      "Processing call [083] out of [100] = [83.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,264 ms\n",
      "Tokens per second [135.3]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Using lambda functions in Pandas</args></response>]\n",
      "\n",
      "Processing call [084] out of [100] = [84.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,559 ms\n",
      "Tokens per second [145.0]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>How do you manage situations leading to an unexpected exit of generators in Python?</args></response>]\n",
      "\n",
      "Processing call [085] out of [100] = [85.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,675 ms\n",
      "Tokens per second [137.3]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Explainable AI: Why is explainable AI important, and how is it achieved?</args></response>]\n",
      "\n",
      "Processing call [086] out of [100] = [86.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,617 ms\n",
      "Tokens per second [148.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>What steps can be taken to resolve errors related to aborted connections in Python?</args></response>]\n",
      "\n",
      "Processing call [087] out of [100] = [87.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,264 ms\n",
      "Tokens per second [140.8]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>File Not Found Error</args></response>]\n",
      "\n",
      "Processing call [088] out of [100] = [88.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,208 ms\n",
      "Tokens per second [139.1]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Connection Reset Error</args></response>]\n",
      "\n",
      "Processing call [089] out of [100] = [89.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,646 ms\n",
      "Tokens per second [136.7]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>What are the causes of floating point errors in Python, and how can they be minimized?</args></response>]\n",
      "\n",
      "Processing call [090] out of [100] = [90.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,616 ms\n",
      "Tokens per second [138.6]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI ethics in research: What are the key ethical considerations in AI research?</args></response>]\n",
      "\n",
      "Processing call [091] out of [100] = [91.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,294 ms\n",
      "Tokens per second [140.6]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>latest breakthroughs in medicine</args></response>]\n",
      "\n",
      "Processing call [092] out of [100] = [92.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,294 ms\n",
      "Tokens per second [133.7]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Data manipulation in Pandas</args></response>]\n",
      "\n",
      "Processing call [093] out of [100] = [93.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,622 ms\n",
      "Tokens per second [152.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>What are the best practices for managing timeout errors in Python, especially in network requests?</args></response>]\n",
      "\n",
      "Processing call [094] out of [100] = [94.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,295 ms\n",
      "Tokens per second [129.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI-driven chatbots</args></response>]\n",
      "\n",
      "Processing call [095] out of [100] = [95.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,324 ms\n",
      "Tokens per second [131.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Data anonymization in Pandas</args></response>]\n",
      "\n",
      "Processing call [096] out of [100] = [96.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,617 ms\n",
      "Tokens per second [144.7]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How do you handle situations where a feature or method is not yet implemented in Python?</args></response>]\n",
      "\n",
      "Processing call [097] out of [100] = [97.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,295 ms\n",
      "Tokens per second [135.1]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>what is the stock market?</args></response>]\n",
      "\n",
      "Processing call [098] out of [100] = [98.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,271 ms\n",
      "Tokens per second [129.8]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>IBM Watson AI tools</args></response>]\n",
      "\n",
      "Processing call [099] out of [100] = [99.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,294 ms\n",
      "Tokens per second [139.9]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>System Error: Internal Python system issue</args></response>]\n",
      "\n",
      "Processing call [100] out of [100] = [100.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,589 ms\n",
      "Tokens per second [163.0]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Feature Engineering best practices: How does feature engineering improve the performance of machine learning models?</args></response>]\n",
      "\n",
      "\n",
      "Generating responses for 100 rows... Done! in 02:16\n",
      "[1367.5] ms per item\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "# Path prefix allows us to find the raw text utilized in building the prompts\n",
    "xml_ftp_generator = XmlFineTuningPromptGenerator( path_prefix=\"/var/model/genie-in-the-box\", debug=True, verbose=False )\n",
    "\n",
    "validate_df = xml_ftp_generator.generate_responses( validate_df, tokenizer=tokenizer, model=base_model, switch=\"huggingface\", model_name=\"mistralai/Mistral-7B-Instruct-v0.2\", device=\"cuda:1\" )\n",
    "validate_df = xml_ftp_generator.validate_responses( validate_df )\n",
    "\n",
    "\n",
    "# Using 4 bit quantization\n",
    "# Generating responses for 100 rows... Done! in 03:16\n",
    "# [1963.5] ms per item"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T01:40:01.411559Z",
     "start_time": "2024-01-19T01:37:44.448980Z"
    }
   },
   "id": "c90168badbbb4f37"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation Stats\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "          Contains response 100.0%\n",
      "   Contains browser command 100.0%\n",
      "              Contains args 100.0%\n",
      "          Response is exact 100.0%\n",
      "Response has correct values 100.0%\n",
      " Browser command is correct 100.0%\n",
      "            Args is correct 100.0%\n"
     ]
    }
   ],
   "source": [
    "xml_ftp_generator.print_validation_stats( validate_df )\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation Stats After naïve merge into bfloat 16 loaded model. Distributed onto ONE GPU: cuda:1\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#    Contains browser command 100.0%\n",
    "#               Contains args 100.0%\n",
    "#           Response is exact 100.0%\n",
    "# Response has correct values 100.0%\n",
    "#  Browser command is correct 100.0%\n",
    "#             Args is correct 100.0%\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation Stats After naïve merge into bfloat 16 loaded model. Distributed onto both GPUs\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#    Contains browser command 100.0%\n",
    "#               Contains args 100.0%\n",
    "#           Response is exact 100.0%\n",
    "# Response has correct values 100.0%\n",
    "#  Browser command is correct 100.0%\n",
    "#             Args is correct 100.0%"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T23:09:14.706557Z",
     "start_time": "2024-01-18T23:09:14.701138Z"
    }
   },
   "id": "1ea31d974c4ce0c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8c7dff030d69dfab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
