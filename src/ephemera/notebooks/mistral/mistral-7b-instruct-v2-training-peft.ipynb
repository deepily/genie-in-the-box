{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# load auto reload module\n",
    "%load_ext autoreload"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:30:40.595892Z",
     "start_time": "2024-01-21T00:30:40.569391Z"
    }
   },
   "id": "18959734b7ebd73"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import json\n",
    "import wandb\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:30:41.870334Z",
     "start_time": "2024-01-21T00:30:41.157428Z"
    }
   },
   "id": "c213bdd2418b70f4"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/genie-in-the-box/src\n"
     ]
    }
   ],
   "source": [
    "from xmlschema import XMLSchema\n",
    "\n",
    "os.chdir( \"/var/model/genie-in-the-box/src\" )\n",
    "print( os.getcwd() )\n",
    "import lib.utils.util         as du\n",
    "import lib.utils.util_xml     as dux\n",
    "import lib.utils.util_pytorch as dupt\n",
    "\n",
    "from ephemera.prompts.xml_fine_tuning_prompt_generator import XmlFineTuningPromptGenerator\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:30:42.096893Z",
     "start_time": "2024-01-21T00:30:41.872747Z"
    }
   },
   "id": "4e1faf28cf522ca7"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28K\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Jan 18 19:13 .\r\n",
      "drwxr--r-- 35 1001 1001 4.0K Jan 20 22:24 ..\r\n",
      "drwxr-xr-x  3 root root 4.0K Jan 18 15:34 .locks\r\n",
      "drwxrwxr-x  8 1001 1001 4.0K Jan 20 23:58 Mistral-7B-Instruct-v0.2\r\n",
      "drwxr-xr-x  6 1001 1001 4.0K Jan 18 15:35 models--bigscience--bloom-560m\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Jan 18 15:59 models--mistralai--Mistral-7B-Instruct-v0.2\r\n",
      "-rw-r--r--  1 1001 1001    1 Jan 18 15:35 version.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Print current working directory\n",
    "# !ls -alh /var/model/Phind-CodeLlama-34B-v2\n",
    "# Change to /var/model/Phind-CodeLlama-34B-v2\n",
    "# os.chdir( \"/var/model/Phind-CodeLlama-34B-v2\" )\n",
    "# Print current working directory\n",
    "# os.getcwd()\n",
    "! ls -alh /var/model/models"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:30:42.720686Z",
     "start_time": "2024-01-21T00:30:42.570782Z"
    }
   },
   "id": "644a6196802f8630"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load model and tokenizer in FP16?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab9fedda738ba991"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_base_model_and_tokenizer( model_path=\".\", tokenizer_path=\".\", use_bnb_cuantization=False, device_map=\"auto\" ):\n",
    "    \n",
    "    compute_dtype = getattr( torch, \"float16\" )\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype\n",
    "    )\n",
    "    if use_bnb_cuantization: \n",
    "\n",
    "        print( bnb_config )\n",
    "\n",
    "        # ¡OJO! Why were we turning off the cash here? It makes a big performance difference: 21 vs 14 tokens per second\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, quantization_config=bnb_config, device_map=device_map, low_cpu_mem_usage=True, use_cache=True, attn_implementation=\"flash_attention_2\"\n",
    "        )\n",
    "    else:\n",
    "        print( \"Loading without BitsAndBytesConfig...\" )\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, device_map=device_map, low_cpu_mem_usage=True, use_cache=True, attn_implementation=\"flash_attention_2\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "    \n",
    "    tokenizer              = AutoTokenizer.from_pretrained( tokenizer_path )\n",
    "    tokenizer.pad_token    = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    return base_model, tokenizer\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T22:52:51.662399Z",
     "start_time": "2024-01-20T22:52:51.609531Z"
    }
   },
   "id": "1bc8cfb9781119a5"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/models\n",
      "Loading without BitsAndBytesConfig...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4850a13907f447ed90f533c519c296dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir( \"/var/model/models/\" )\n",
    "print( os.getcwd() )\n",
    "base_model, tokenizer = get_base_model_and_tokenizer( \n",
    "    model_path=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "    tokenizer_path=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "    use_bnb_cuantization=False, \n",
    "    device_map=\"auto\" \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T20:49:09.381679Z",
     "start_time": "2024-01-19T20:49:05.969064Z"
    }
   },
   "id": "926b612bcb0c1fdf"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def run_validation( model, tokenizer, model_name=\"mistralai/Mistral-7B-Instruct-v0.2\", device=\"cuda:1\" ):\n",
    "\n",
    "    df = pd.read_json( \n",
    "        \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\", lines=True \n",
    "    ).sample( 100, random_state=42 )\n",
    "    \n",
    "    print( \"validate_df.shape\", df.shape )\n",
    "\n",
    "    xml_ftp_generator = XmlFineTuningPromptGenerator( path_prefix=\"/var/model/genie-in-the-box\", debug=True, verbose=False )\n",
    "    \n",
    "    df = xml_ftp_generator.generate_responses( \n",
    "        df, tokenizer=tokenizer, model=model, switch=\"huggingface\", model_name=model_name, device=device \n",
    "    )\n",
    "    df = xml_ftp_generator.validate_responses( df )\n",
    "    \n",
    "    xml_ftp_generator.print_validation_stats( df, title=f\"Validation stats for model {model_name}\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:30:00.828295Z",
     "start_time": "2024-01-21T00:30:00.815690Z"
    }
   },
   "id": "2e34ef54107ef3e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up W & B"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b38e3e5bfe2baa3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mricardo-felipe-ruiz\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-19T20:48:50.322317Z"
    }
   },
   "id": "d49e208217c2ea36"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=\"Mistral-7B-Instruct-v0.2\"\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=\"Mistral-7B-Instruct-v0.2\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-19T20:48:51.417210Z"
    }
   },
   "id": "d03f99f12eb04c9e"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'/var/model/models'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T20:49:14.465013Z",
     "start_time": "2024-01-19T20:49:14.438315Z"
    }
   },
   "id": "ebc81a5aee56fa09"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28K\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Jan 18 19:13 .\r\n",
      "drwxr--r-- 35 1001 1001 4.0K Jan 19 15:59 ..\r\n",
      "drwxr-xr-x  3 root root 4.0K Jan 18 15:34 .locks\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Jan 19 16:24 Mistral-7B-Instruct-v0.2\r\n",
      "drwxr-xr-x  6 1001 1001 4.0K Jan 18 15:35 models--bigscience--bloom-560m\r\n",
      "drwxrwxr-x  6 1001 1001 4.0K Jan 18 15:59 models--mistralai--Mistral-7B-Instruct-v0.2\r\n",
      "-rw-r--r--  1 1001 1001    1 Jan 18 15:35 version.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls -alh"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T20:49:20.966299Z",
     "start_time": "2024-01-19T20:49:20.796382Z"
    }
   },
   "id": "22fc635ecb15e0a7"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralFlashAttention2(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm()\n        (post_attention_layernorm): MistralRMSNorm()\n      )\n    )\n    (norm): MistralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T20:49:22.627165Z",
     "start_time": "2024-01-19T20:49:22.611659Z"
    }
   },
   "id": "42d0f6e85a7c7639"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TEST model on validation dataset, BEFORE training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11a8cf22a4d89d12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_validation( base_model, tokenizer, model_name=\"mistralai/Mistral-7B-Instruct-v0.2\" )\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73f0d6ead74579e8"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# Creates insanely verbose outputs, no need to benchmark any further!\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# Response: [<response><browser-command>search google scholar current tab</browser-command><args><arg>URLError</arg></args></response>\n",
    "# \n",
    "#         Explanation:\n",
    "#         The human voice command \"Here, Google Scholar URLError\" can be broken down into the following parts:\n",
    "#         1. \"Here\" is likely an indication of the current tab, but it's not a necessary part of the command.\n",
    "#         2. \"Google Scholar\" is the search engine and the specific search type.\n",
    "#         3. \"URLError\" is likely an error message or an argument related to the command.\n",
    "# \n",
    "#         Based on this analysis, the correct standardized command is \"search google scholar current tab\" with the argument \"URLError\".</s> \n",
    "# \n",
    "#     I hope this explanation is clear and helpful. Let me know if you have any questions or need further clarification.\n",
    "# \n",
    "#     Best regards,\n",
    "#     Your helpful AI assistant.</s><response><browser-command>search google scholar current tab</browser-command><args><arg>URLError</arg></args></response>\n",
    "# \n",
    "# Explanation:\n",
    "# The human voice command \"Here, Google Scholar URLError\" can be broken down into the following parts:\n",
    "# 1. \"Here\" is likely an indication of the current tab, but it's not a necessary part of the command.\n",
    "# 2. \"Google Scholar\" is the search engine and the specific search type.\n",
    "# 3. \"URLError\" is likely an error message or an argument related to the command.\n",
    "# Based on this analysis, the correct standardized command is \"search google scholar current tab\" with the argument \"URLError\".</s>\n",
    "# \n",
    "# I hope this explanation is clear and helpful. Let me know if you have any questions or need further clarification.\n",
    "# \n",
    "# Best regards,\n",
    "# Your helpful AI assistant.</s><response><browser-command>search google scholar current tab</browser-command><args><arg>URLError</arg></args></response>\n",
    "# \n",
    "# Explanation:\n",
    "# The human voice command \"Here, Google Scholar URLError\" can be broken down into the following parts:\n",
    "# 1. \"Here\" is likely an indication of the current tab, but it's not a necessary part of the command.\n",
    "# 2. \"Google Scholar\" is the search engine and the specific search type.\n",
    "# 3. \"URLError\" is likely an error message or an argument related to the command.\n",
    "# Based on this analysis, the correct standardized command is \"search google scholar current tab\" with the argument \"URLError\".\n",
    "# \n",
    "# I hope this explanation is clear and helpful. Let me know if you have any questions or need further clarification.\n",
    "# \n",
    "# Best regards,\n",
    "# Your helpful AI assistant.</s><response xmlns=\"http://www.w3.org/2000/xmlns/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\"><browser-command xsi:type=\"xsd:string\">search google scholar current tab</browser-command><args><arg xsi:type=\"xsd:string\">URLError</arg></args></response>\n",
    "# \n",
    "# Explanation:\n",
    "# The human voice command \"Here, Google Scholar URLError\" can be broken down into the following parts:\n",
    "# 1. \"Here\" is likely an indication of the current tab, but it's not a necessary part of the command.\n",
    "# 2. \"Google Scholar\" is the search engine and the specific search type.\n",
    "# 3. \"URLError\" is likely an error message or an argument related to the command.\n",
    "# Based on this analysis, the correct standardized command is \"search google scholar current tab\" with the argument \"URLError\". To ensure well-formed XML, I have added the XML namespaces and types to the response.\n",
    "# \n",
    "# I hope this explanation is clear and helpful. Let me know if you have any questions or need further clarification.\n",
    "# \n",
    "# Best regards,\n",
    "# Your helpful AI assistant.</s><response xmlns=\"http://www.w3.org/2000/xmlns/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T19:17:31.596176Z",
     "start_time": "2024-01-19T19:17:31.573600Z"
    }
   },
   "id": "aac1ce9b75d5790"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get training dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fcd418053c3052b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "8000"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-train.jsonl\"\n",
    "deepily_dataset_train = du.get_file_as_list( path )[ 0:10000 ]\n",
    "deepily_dataset_train = [ json.loads( line ) for line in deepily_dataset_train ]\n",
    "len( deepily_dataset_train )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T20:49:29.733685Z",
     "start_time": "2024-01-19T20:49:29.601074Z"
    }
   },
   "id": "ae76c88a9a5e791d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-test.jsonl\"\n",
    "deepily_dataset_test = du.get_file_as_list( path )[ 0:1000 ]\n",
    "deepily_dataset_test = [ json.loads( line ) for line in deepily_dataset_test ]\n",
    "len( deepily_dataset_test )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T20:49:30.056736Z",
     "start_time": "2024-01-19T20:49:30.028891Z"
    }
   },
   "id": "f1c868ec60880dcc"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Use the Task below and the Input given to write the Response, which is a programmatic instruction that can solve the following Task:\n",
    "def prompt_instruction_format( sample ):\n",
    "    \n",
    "  return f\"\"\"### Instruction:\n",
    "    Use the Task below and the Input given to write a Response that can solve the following Task:\n",
    "\n",
    "    ### Task:\n",
    "    {sample['instruction']}\n",
    "\n",
    "    ### Input:\n",
    "    {sample['input']}\n",
    "\n",
    "    ### Response:\n",
    "    {sample['output']}\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T20:49:30.771642Z",
     "start_time": "2024-01-19T20:49:30.760284Z"
    }
   },
   "id": "58e3fd8b1b81ce1d"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "    Use the Task below and the Input given to write a Response that can solve the following Task:\n",
      "\n",
      "    ### Task:\n",
      "    Your job is to discern the intent of a human voice command transcription and translate it into a standardized command that a browser on your computer would understand.\n",
      "\n",
      "        You will be given a human voice command and a list of possible standardized commands. You must choose the correct standardized command from the following list: `'search new tab', 'search current tab', 'search google new tab', 'search google current tab', 'search google scholar new tab', 'search google scholar current tab' and 'none'`.\n",
      "\n",
      "        Requirement: You MUST NOT use python code to answer this question.\n",
      "        Requirement: You MUST use your linguistic knowledge and intuition to answer this question.\n",
      "        Hint: Anything that isn't a part of the command itself should be treated as arguments related to the command.\n",
      "\n",
      "    ### Input:\n",
      "    \n",
      "        Below is the raw human voice command transcription formatted using simple XML:\n",
      "        \n",
      "        <human>\n",
      "            <voice-command>Perform Google Scholar search with How can syntax warnings be resolved in Python, especially those indicating possible code errors? in the current window</voice-command>\n",
      "        </human>\n",
      "        \n",
      "        The standardized command that you translate MUST be returned wrapped in simple, well-formed XML:\n",
      "        \n",
      "        <response>\n",
      "            <browser-command></browser-command>\n",
      "            <args></args>\n",
      "        </response>\n",
      "\n",
      "        Requirement: The first word of your response MUST be `<response>`\n",
      "\n",
      "    ### Response:\n",
      "    \n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>How can syntax warnings be resolved in Python, especially those indicating possible code errors?</args>\n",
      "        </response>\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for line in prompt_instruction_format( deepily_dataset_test[ 0 ] ).split( \"\\n\" ): print( line )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T20:49:32.496946Z",
     "start_time": "2024-01-19T20:49:32.468549Z"
    }
   },
   "id": "e66898ac4c85e976"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up training arguments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "528b5d0d39d9b5df"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_config, PeftModel, PeftConfig, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64, \n",
    "    lora_alpha=32, \n",
    "    # When target_modules was disabled, it was causing detention layers to be assigned to the CPU, throwing this runtime error:\n",
    "    # RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! \n",
    "    # (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n",
    "    target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\" ], \n",
    "    lora_dropout=0.10, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T20:49:36.624420Z",
     "start_time": "2024-01-19T20:49:36.607635Z"
    }
   },
   "id": "393f4bbf9c6c3ca4"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "'/var/model/models/Mistral-7B-Instruct-v0.2'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir( \"/var/model/models/Mistral-7B-Instruct-v0.2\" )\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T20:49:38.604960Z",
     "start_time": "2024-01-19T20:49:38.581803Z"
    }
   },
   "id": "abccd9b1f7ce3be5"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir=\"./training-results\", # Output directory where the model predictions and checkpoints will be stored\n",
    "    num_train_epochs=2, # Number of training epochs\n",
    "    per_device_train_batch_size=5, # Batch size per GPU for training\n",
    "    gradient_accumulation_steps=5,  # Number of update steps to accumulate the gradients for\n",
    "    gradient_checkpointing=True,# Enable gradient checkpointing\n",
    "    optim=\"paged_adamw_32bit\", # Optimizer to use\n",
    "    #save_steps=save_steps,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    \n",
    "    # Setting this may help with the warning message: The input hidden states seems to be silently casted in float32, \n",
    "    # this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n",
    "    fp16=False,\n",
    "    # Test to confirm that this works!\n",
    "    # BTW: according to PHIND, this may actually improve fine-tuning performance as well: https://www.phind.com/search?cache=ygn9dbyl0ij4kotmgns2nsrw\n",
    "    \n",
    "    bf16=True,\n",
    "    # tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    #max_steps=max_steps,\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    disable_tqdm=True,\n",
    "    report_to=\"wandb\",\n",
    "    seed=42\n",
    ")\n",
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=deepily_dataset_train,\n",
    "    eval_dataset=deepily_dataset_test,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=4096, #2048,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=prompt_instruction_format,\n",
    "    args=trainingArgs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T20:49:44.629576Z",
     "start_time": "2024-01-19T20:49:41.444001Z"
    }
   },
   "id": "e0ad2d859cefb9c4"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 170,082,304 || all params: 7,411,814,400 || trainable%: 2.29\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters( model ):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,} || all params: {all_param:,} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "    \n",
    "print_trainable_parameters( base_model )    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T20:49:44.645707Z",
     "start_time": "2024-01-19T20:49:44.636646Z"
    }
   },
   "id": "8275555ac7a5093d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3d13c347871afcd"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.2"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/var/model/models/Mistral-7B-Instruct-v0.2/wandb/run-20240119_204946-3gmbxji3</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22/runs/3gmbxji3' target=\"_blank\">deep-elevator-8</a></strong> to <a href='https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22/runs/3gmbxji3' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22/runs/3gmbxji3</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3592, 'learning_rate': 0.00019876883405951377, 'epoch': 0.16}\n",
      "{'loss': 0.1313, 'learning_rate': 0.0001913545457642601, 'epoch': 0.32}\n",
      "{'loss': 0.0829, 'learning_rate': 0.0001777145961456971, 'epoch': 0.48}\n",
      "{'loss': 0.068, 'learning_rate': 0.00015877852522924732, 'epoch': 0.65}\n",
      "{'loss': 0.0609, 'learning_rate': 0.00013583679495453, 'epoch': 0.81}\n",
      "{'loss': 0.0553, 'learning_rate': 0.00011045284632676536, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0498, 'learning_rate': 8.435655349597689e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0468, 'learning_rate': 5.9326335692419995e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0454, 'learning_rate': 3.7067960895016275e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0443, 'learning_rate': 1.9098300562505266e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0436, 'learning_rate': 6.6419573502798374e-06, 'epoch': 1.77}\n",
      "{'loss': 0.0437, 'learning_rate': 5.478104631726711e-07, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2989.6932, 'train_samples_per_second': 0.518, 'train_steps_per_second': 0.021, 'train_loss': 0.08454943304100344, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2dfcdcdd07e94f4d8b4f1aff5a7fc9bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▂▃▃▄▅▅▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▃▄▅▅▆▇▇██</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▅▄▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>62</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0437</td></tr><tr><td>train/total_flos</td><td>2.7734386409472e+17</td></tr><tr><td>train/train_loss</td><td>0.08455</td></tr><tr><td>train/train_runtime</td><td>2989.6932</td></tr><tr><td>train/train_samples_per_second</td><td>0.518</td></tr><tr><td>train/train_steps_per_second</td><td>0.021</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">deep-elevator-8</strong> at: <a href='https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22/runs/3gmbxji3' target=\"_blank\">https://wandb.ai/ricardo-felipe-ruiz/%22Mistral-7B-Instruct-v0.2%22/runs/3gmbxji3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240119_204946-3gmbxji3/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "#stop reporting to wandb\n",
    "wandb.finish()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()\n",
    "\n",
    "print( \"Model saved\" )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T21:39:49.040874Z",
     "start_time": "2024-01-19T20:49:46.885924Z"
    }
   },
   "id": "597fd4a8fa0f143f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-19T20:34:57.626098Z"
    }
   },
   "id": "4f1d623ab4bc7b2e"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "'/var/model/models/Mistral-7B-Instruct-v0.2'"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T21:41:57.985109Z",
     "start_time": "2024-01-19T21:41:57.970171Z"
    }
   },
   "id": "d9582f7d77b9b269"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "565"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "# base_model = None \n",
    "# adapter_plus_model = None\n",
    "torch.cuda.empty_cache() \n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T19:35:32.051851Z",
     "start_time": "2024-01-19T19:35:31.806980Z"
    }
   },
   "id": "23550e0e9f04149f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RESTART 1st time & load model and tokenizer in FP16"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf69f614e0a96699"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'/var/model/models'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir( \"/var/model/models/\" )\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T21:43:41.845734Z",
     "start_time": "2024-01-19T21:43:41.836728Z"
    }
   },
   "id": "46ec1294438f94de"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading without BitsAndBytesConfig...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "455403a4375e477dac6350b598dc6e9c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model, tokenizer = get_base_model_and_tokenizer( \n",
    "    model_path=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "    tokenizer_path=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "    use_bnb_cuantization=False, \n",
    "    device_map=\"auto\" \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T21:44:00.538426Z",
     "start_time": "2024-01-19T21:43:57.248135Z"
    }
   },
   "id": "a1cdb4598a75c4f7"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'/var/model/models'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T21:44:04.310899Z",
     "start_time": "2024-01-19T21:44:04.290862Z"
    }
   },
   "id": "50c4d97554991fa1"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from peft import PeftModel, AutoPeftModelForCausalLM\n",
    "\n",
    "adapter_plus_model = PeftModel.from_pretrained( base_model, \"Mistral-7B-Instruct-v0.2/training-results\", use_flash_attention_2=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T21:44:27.562301Z",
     "start_time": "2024-01-19T21:44:25.167416Z"
    }
   },
   "id": "34796358a34b99c6"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator\n",
    "# \n",
    "# accelerator = Accelerator()\n",
    "# \n",
    "# adapter_plus_model = accelerator.prepare( adapter_plus_model )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T22:02:07.679677Z",
     "start_time": "2024-01-17T22:02:07.599158Z"
    }
   },
   "id": "8a8382a512c938fd"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.0.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.1.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.2.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.3.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.4.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.5.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.6.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.7.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.8.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.9.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.10.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.11.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.12.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.13.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.14.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: cuda:0\n",
      "base_model.model.model.layers.15.input_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight: cuda:0\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.16.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.17.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.18.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.19.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.20.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.21.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.22.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.23.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.24.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.25.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.26.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.27.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.28.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.29.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.30.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.base_layer.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: cuda:1\n",
      "base_model.model.model.layers.31.input_layernorm.weight: cuda:1\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight: cuda:1\n",
      "base_model.model.model.norm.weight: cuda:1\n",
      "base_model.model.lm_head.base_layer.weight: cuda:1\n",
      "base_model.model.lm_head.lora_A.default.weight: cuda:1\n",
      "base_model.model.lm_head.lora_B.default.weight: cuda:1\n"
     ]
    }
   ],
   "source": [
    "dupt.print_device_allocation( adapter_plus_model )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T21:44:29.936712Z",
     "start_time": "2024-01-19T21:44:29.921146Z"
    }
   },
   "id": "4a4d57dcf172749f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TEST model on validation dataset using adapter loaded on top"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78308e05900f5eef"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate_df.shape (100, 5)\n",
      "Commands file for [search new tab] exists: True\n",
      "Commands file for [search current tab] exists: True\n",
      "Commands file for [search google new tab] exists: True\n",
      "Commands file for [search google current tab] exists: True\n",
      "Commands file for [search google scholar new tab] exists: True\n",
      "Commands file for [search google scholar current tab] exists: True\n",
      "\n",
      "Generating responses for 100 rows...\n",
      "Using HuggingFace model_name [mistralai/Mistral-7B-Instruct-v0.2] in memory...\n",
      "\n",
      "Processing call [001] out of [100] = [1.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,250 ms\n",
      "Tokens per second [65.3]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>TabError</args></response>]\n",
      "\n",
      "Processing call [002] out of [100] = [2.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,457 ms\n",
      "Tokens per second [95.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>How can you handle reference errors, especially with weak references, in Python?</args></response>]\n",
      "\n",
      "Processing call [003] out of [100] = [3.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,863 ms\n",
      "Tokens per second [83.2]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>IndentationError</args></response>]\n",
      "\n",
      "Processing call [004] out of [100] = [4.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,906 ms\n",
      "Tokens per second [90.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>BERT model improvements</args></response>]\n",
      "\n",
      "Processing call [005] out of [100] = [5.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,133 ms\n",
      "Tokens per second [85.8]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Creating histograms in Pandas</args></response>]\n",
      "\n",
      "Processing call [006] out of [100] = [6.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,449 ms\n",
      "Tokens per second [98.4]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>What are the best practices for managing timeout errors in Python, especially in network requests?</args></response>]\n",
      "\n",
      "Processing call [007] out of [100] = [7.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,905 ms\n",
      "Tokens per second [87.1]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Process Lookup Error</args></response>]\n",
      "\n",
      "Processing call [008] out of [100] = [8.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,811 ms\n",
      "Tokens per second [86.7]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>UserWarning</args></response>]\n",
      "\n",
      "Processing call [009] out of [100] = [9.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,809 ms\n",
      "Tokens per second [88.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AttributeError</args></response>]\n",
      "\n",
      "Processing call [010] out of [100] = [10.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,856 ms\n",
      "Tokens per second [90.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>pop music</args></response>]\n",
      "\n",
      "Processing call [011] out of [100] = [11.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,351 ms\n",
      "Tokens per second [91.9]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>How can you manage errors that occur due to missing files in Python programs?</args></response>]\n",
      "\n",
      "Processing call [012] out of [100] = [12.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,538 ms\n",
      "Tokens per second [95.4]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Data Science career trends: What are the current career trends in the field of data science?</args></response>]\n",
      "\n",
      "Processing call [013] out of [100] = [13.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,814 ms\n",
      "Tokens per second [88.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>UserWarning</args></response>]\n",
      "\n",
      "Processing call [014] out of [100] = [14.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,045 ms\n",
      "Tokens per second [92.4]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Generating summary statistics in Pandas</args></response>]\n",
      "\n",
      "Processing call [015] out of [100] = [15.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,582 ms\n",
      "Tokens per second [96.0]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Google Cloud AI services: What AI services does Google Cloud offer for developers and data scientists?</args></response>]\n",
      "\n",
      "Processing call [016] out of [100] = [16.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,899 ms\n",
      "Tokens per second [85.3]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>how to paint a room</args></response>]\n",
      "\n",
      "Processing call [017] out of [100] = [17.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,176 ms\n",
      "Tokens per second [93.3]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Unsupported Function Call: Function not supported for this dtype</args></response>]\n",
      "\n",
      "Processing call [018] out of [100] = [18.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,855 ms\n",
      "Tokens per second [93.3]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>PermissionError</args></response>]\n",
      "\n",
      "Processing call [019] out of [100] = [19.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,531 ms\n",
      "Tokens per second [93.6]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>What are the reasons for attribute-related errors in Python objects and how can they be fixed?</args></response>]\n",
      "\n",
      "Processing call [020] out of [100] = [20.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,077 ms\n",
      "Tokens per second [89.6]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Data deduplication in Pandas</args></response>]\n",
      "\n",
      "Processing call [021] out of [100] = [21.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,811 ms\n",
      "Tokens per second [87.2]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>OS Error</args></response>]\n",
      "\n",
      "Processing call [022] out of [100] = [22.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,400 ms\n",
      "Tokens per second [100.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Responsible AI practices: How can organizations implement responsible AI practices?</args></response>]\n",
      "\n",
      "Processing call [023] out of [100] = [23.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,072 ms\n",
      "Tokens per second [84.0]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>popular music genres</args></response>]\n",
      "\n",
      "Processing call [024] out of [100] = [24.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,850 ms\n",
      "Tokens per second [84.3]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Blocking IO Error</args></response>]\n",
      "\n",
      "Processing call [025] out of [100] = [25.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,078 ms\n",
      "Tokens per second [88.1]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Convolutional Neural Networks</args></response>]\n",
      "\n",
      "Processing call [026] out of [100] = [26.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,113 ms\n",
      "Tokens per second [87.6]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Data partitioning in Pandas</args></response>]\n",
      "\n",
      "Processing call [027] out of [100] = [27.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,571 ms\n",
      "Tokens per second [95.3]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>What are runtime warnings in Python, and how can they be used to identify potential issues?</args></response>]\n",
      "\n",
      "Processing call [028] out of [100] = [28.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,802 ms\n",
      "Tokens per second [84.9]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Syntax Warning</args></response>]\n",
      "\n",
      "Processing call [029] out of [100] = [29.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,394 ms\n",
      "Tokens per second [90.6]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>What are the best practices for handling I/O blocking errors in Python?</args></response>]\n",
      "\n",
      "Processing call [030] out of [100] = [30.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,534 ms\n",
      "Tokens per second [93.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI for health diagnostics: How does AI contribute to advancements in health diagnostics?</args></response>]\n",
      "\n",
      "Processing call [031] out of [100] = [31.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,440 ms\n",
      "Tokens per second [101.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Deep Learning optimization: What strategies are used for optimizing deep learning models?</args></response>]\n",
      "\n",
      "Processing call [032] out of [100] = [32.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,486 ms\n",
      "Tokens per second [95.7]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>How can you resolve recursion errors due to excessive recursive calls in Python?</args></response>]\n",
      "\n",
      "Processing call [033] out of [100] = [33.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,854 ms\n",
      "Tokens per second [91.7]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Buffer Error</args></response>]\n",
      "\n",
      "Processing call [034] out of [100] = [34.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,851 ms\n",
      "Tokens per second [89.1]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>classical music</args></response>]\n",
      "\n",
      "Processing call [035] out of [100] = [35.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,153 ms\n",
      "Tokens per second [87.8]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Aggregation functions in Pandas</args></response>]\n",
      "\n",
      "Processing call [036] out of [100] = [36.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,972 ms\n",
      "Tokens per second [87.7]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>AI in manufacturing</args></response>]\n",
      "\n",
      "Processing call [037] out of [100] = [37.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,929 ms\n",
      "Tokens per second [89.7]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>ResourceWarning</args></response>]\n",
      "\n",
      "Processing call [038] out of [100] = [38.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,043 ms\n",
      "Tokens per second [82.7]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>why do zebras have stripes</args></response>]\n",
      "\n",
      "Processing call [039] out of [100] = [39.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,851 ms\n",
      "Tokens per second [89.7]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>BytesWarning</args></response>]\n",
      "\n",
      "Processing call [040] out of [100] = [40.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,079 ms\n",
      "Tokens per second [91.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>OS Error: Operating system error</args></response>]\n",
      "\n",
      "Processing call [041] out of [100] = [41.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,168 ms\n",
      "Tokens per second [89.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Exploratory data analysis with Pandas</args></response>]\n",
      "\n",
      "Processing call [042] out of [100] = [42.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,894 ms\n",
      "Tokens per second [88.7]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>PendingDeprecationWarning</args></response>]\n",
      "\n",
      "Processing call [043] out of [100] = [43.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,033 ms\n",
      "Tokens per second [88.5]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Binning data in Pandas</args></response>]\n",
      "\n",
      "Processing call [044] out of [100] = [44.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,940 ms\n",
      "Tokens per second [96.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>latest space exploration news</args></response>]\n",
      "\n",
      "Processing call [045] out of [100] = [45.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,989 ms\n",
      "Tokens per second [86.5]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>how to compost at home</args></response>]\n",
      "\n",
      "Processing call [046] out of [100] = [46.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,814 ms\n",
      "Tokens per second [85.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>URL Error</args></response>]\n",
      "\n",
      "Processing call [047] out of [100] = [47.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,703 ms\n",
      "Tokens per second [89.2]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AWS SageMaker for ML: How does AWS SageMaker facilitate machine learning model development?</args></response>]\n",
      "\n",
      "Processing call [048] out of [100] = [48.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,946 ms\n",
      "Tokens per second [92.0]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI in natural language generation</args></response>]\n",
      "\n",
      "Processing call [049] out of [100] = [49.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,026 ms\n",
      "Tokens per second [92.8]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Time series analysis in Pandas</args></response>]\n",
      "\n",
      "Processing call [050] out of [100] = [50.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,986 ms\n",
      "Tokens per second [84.5]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Bytes Warning: Bytecode issue</args></response>]\n",
      "\n",
      "Processing call [051] out of [100] = [51.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,034 ms\n",
      "Tokens per second [89.5]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>OS Error: Operating system error</args></response>]\n",
      "\n",
      "Processing call [052] out of [100] = [52.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,922 ms\n",
      "Tokens per second [87.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI in public safety</args></response>]\n",
      "\n",
      "Processing call [053] out of [100] = [53.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,991 ms\n",
      "Tokens per second [89.9]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Using Pandas for predictive modeling</args></response>]\n",
      "\n",
      "Processing call [054] out of [100] = [54.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,855 ms\n",
      "Tokens per second [88.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>new car reviews</args></response>]\n",
      "\n",
      "Processing call [055] out of [100] = [55.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,089 ms\n",
      "Tokens per second [81.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>how to plant tomatoes</args></response>]\n",
      "\n",
      "Processing call [056] out of [100] = [56.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,052 ms\n",
      "Tokens per second [83.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI in retail analytics</args></response>]\n",
      "\n",
      "Processing call [057] out of [100] = [57.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,989 ms\n",
      "Tokens per second [87.5]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Handling large datasets with Pandas</args></response>]\n",
      "\n",
      "Processing call [058] out of [100] = [58.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,437 ms\n",
      "Tokens per second [94.8]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>What are the steps to resolve permission-related errors in file operations in Python?</args></response>]\n",
      "\n",
      "Processing call [059] out of [100] = [59.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,418 ms\n",
      "Tokens per second [91.0]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>How can you manage errors that occur due to missing files in Python programs?</args></response>]\n",
      "\n",
      "Processing call [060] out of [100] = [60.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,440 ms\n",
      "Tokens per second [93.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI in sports analytics: How is AI used in sports analytics to improve performance?</args></response>]\n",
      "\n",
      "Processing call [061] out of [100] = [61.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,922 ms\n",
      "Tokens per second [90.5]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>ConnectionResetError</args></response>]\n",
      "\n",
      "Processing call [062] out of [100] = [62.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,851 ms\n",
      "Tokens per second [86.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>sports</args></response>]\n",
      "\n",
      "Processing call [063] out of [100] = [63.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,996 ms\n",
      "Tokens per second [89.7]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>AI-driven fraud detection</args></response>]\n",
      "\n",
      "Processing call [064] out of [100] = [64.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,616 ms\n",
      "Tokens per second [91.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Real-time data streaming: How do you handle real-time data streaming in big data projects?</args></response>]\n",
      "\n",
      "Processing call [065] out of [100] = [65.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,796 ms\n",
      "Tokens per second [90.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Outlier Detection methods: What methods are most effective for outlier detection in univariate datasets?</args></response>]\n",
      "\n",
      "Processing call [066] out of [100] = [66.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,083 ms\n",
      "Tokens per second [95.5]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Runtime Warning: Runtime behavior warning</args></response>]\n",
      "\n",
      "Processing call [067] out of [100] = [67.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,169 ms\n",
      "Tokens per second [81.6]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>how do i cure dandruff?</args></response>]\n",
      "\n",
      "Processing call [068] out of [100] = [68.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,505 ms\n",
      "Tokens per second [91.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI in digital marketing: What role does AI play in digital marketing strategies?</args></response>]\n",
      "\n",
      "Processing call [069] out of [100] = [69.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,944 ms\n",
      "Tokens per second [89.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>ArithmeticError</args></response>]\n",
      "\n",
      "Processing call [070] out of [100] = [70.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,857 ms\n",
      "Tokens per second [87.8]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>exchange rates today</args></response>]\n",
      "\n",
      "Processing call [071] out of [100] = [71.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,945 ms\n",
      "Tokens per second [85.9]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Merging strategies in Pandas</args></response>]\n",
      "\n",
      "Processing call [072] out of [100] = [72.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,903 ms\n",
      "Tokens per second [91.4]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [073] out of [100] = [73.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,618 ms\n",
      "Tokens per second [95.5]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>What strategies can you use to fix type errors that arise from operations on incompatible data types in Python?</args></response>]\n",
      "\n",
      "Processing call [074] out of [100] = [74.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,017 ms\n",
      "Tokens per second [78.8]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>DIY home decor ideas</args></response>]\n",
      "\n",
      "Processing call [075] out of [100] = [75.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,553 ms\n",
      "Tokens per second [80.7]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI patent trends: What are the current trends in AI patents?</args></response>]\n",
      "\n",
      "Processing call [076] out of [100] = [76.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,437 ms\n",
      "Tokens per second [95.2]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI in sports analytics: How is AI used in sports analytics to improve performance?</args></response>]\n",
      "\n",
      "Processing call [077] out of [100] = [77.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,583 ms\n",
      "Tokens per second [93.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Data Science career trends: What are the current career trends in the field of data science?</args></response>]\n",
      "\n",
      "Processing call [078] out of [100] = [78.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,994 ms\n",
      "Tokens per second [87.8]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Gradient Boosting</args></response>]\n",
      "\n",
      "Processing call [079] out of [100] = [79.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,893 ms\n",
      "Tokens per second [95.1]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Connection Reset Error</args></response>]\n",
      "\n",
      "Processing call [080] out of [100] = [80.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,948 ms\n",
      "Tokens per second [93.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Computer Vision applications</args></response>]\n",
      "\n",
      "Processing call [081] out of [100] = [81.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,905 ms\n",
      "Tokens per second [84.0]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Ethical AI guidelines</args></response>]\n",
      "\n",
      "Processing call [082] out of [100] = [82.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,896 ms\n",
      "Tokens per second [90.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>new book releases</args></response>]\n",
      "\n",
      "Processing call [083] out of [100] = [83.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,123 ms\n",
      "Tokens per second [86.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>why don't people like me?</args></response>]\n",
      "\n",
      "Processing call [084] out of [100] = [84.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,111 ms\n",
      "Tokens per second [89.1]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>weather forecast washington dc</args></response>]\n",
      "\n",
      "Processing call [085] out of [100] = [85.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,993 ms\n",
      "Tokens per second [85.8]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>AI in e-commerce personalization</args></response>]\n",
      "\n",
      "Processing call [086] out of [100] = [86.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,904 ms\n",
      "Tokens per second [91.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>new dress styles</args></response>]\n",
      "\n",
      "Processing call [087] out of [100] = [87.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,992 ms\n",
      "Tokens per second [83.8]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Performance tuning in Pandas</args></response>]\n",
      "\n",
      "Processing call [088] out of [100] = [88.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,983 ms\n",
      "Tokens per second [85.7]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Data imputation with Pandas</args></response>]\n",
      "\n",
      "Processing call [089] out of [100] = [89.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,526 ms\n",
      "Tokens per second [97.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How do you address deprecation warnings in Python to ensure code compatibility with future versions?</args></response>]\n",
      "\n",
      "Processing call [090] out of [100] = [90.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,948 ms\n",
      "Tokens per second [88.3]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Explainable AI</args></response>]\n",
      "\n",
      "Processing call [091] out of [100] = [91.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,032 ms\n",
      "Tokens per second [97.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Runtime Error: Runtime exception occurred</args></response>]\n",
      "\n",
      "Processing call [092] out of [100] = [92.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,852 ms\n",
      "Tokens per second [85.9]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Unicode Decode Error</args></response>]\n",
      "\n",
      "Processing call [093] out of [100] = [93.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,894 ms\n",
      "Tokens per second [82.9]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>FloatingPointError</args></response>]\n",
      "\n",
      "Processing call [094] out of [100] = [94.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,094 ms\n",
      "Tokens per second [82.6]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Saving Pandas DataFrame to CSV</args></response>]\n",
      "\n",
      "Processing call [095] out of [100] = [95.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,903 ms\n",
      "Tokens per second [83.6]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>why cats purr</args></response>]\n",
      "\n",
      "Processing call [096] out of [100] = [96.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,310 ms\n",
      "Tokens per second [92.2]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>How do you handle warnings related to Unicode issues in Python?</args></response>]\n",
      "\n",
      "Processing call [097] out of [100] = [97.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,485 ms\n",
      "Tokens per second [85.3]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>What are import warnings in Python, and how can they be addressed?</args></response>]\n",
      "\n",
      "Processing call [098] out of [100] = [98.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,942 ms\n",
      "Tokens per second [87.0]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>why does bread rise</args></response>]\n",
      "\n",
      "Processing call [099] out of [100] = [99.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,010 ms\n",
      "Tokens per second [86.1]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>how to bake chocolate chip cookies</args></response>]\n",
      "\n",
      "Processing call [100] out of [100] = [100.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,025 ms\n",
      "Tokens per second [91.4]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Overflow Error: Value too large to convert</args></response>]\n",
      "\n",
      "\n",
      "Generating responses for 100 rows... Done! in 03:30\n",
      "[2109.4] ms per item\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation Stats\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "          Contains response 100.0%\n",
      "   Contains browser command 100.0%\n",
      "              Contains args 100.0%\n",
      "          Response is exact 99.0%\n",
      "Response has correct values 99.0%\n",
      " Browser command is correct 100.0%\n",
      "            Args is correct 99.0%\n"
     ]
    }
   ],
   "source": [
    "run_validation( adapter_plus_model, tokenizer, model_name=\"mistralai/Mistral-7B-Instruct-v0.2\", device=\"cuda:1\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T21:48:27.195909Z",
     "start_time": "2024-01-19T21:44:56.033813Z"
    }
   },
   "id": "6b4abb04bf27ab82"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perform a 16bit merge & write to disk"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "554df92b15666503"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "os.chdir( \"/var/model/models/Mistral-7B-Instruct-v0.2\" )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T21:53:29.748935Z",
     "start_time": "2024-01-19T21:53:29.743681Z"
    }
   },
   "id": "1daa10cd14621325"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "adapter_plus_model = adapter_plus_model.merge_and_unload()\n",
    "adapter_plus_model.save_pretrained( \"./merged/\", safe_serialization=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T21:53:54.525245Z",
     "start_time": "2024-01-19T21:53:41.641035Z"
    }
   },
   "id": "7438bad14a9b0c12"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "('./merged/tokenizer_config.json',\n './merged/special_tokens_map.json',\n './merged/tokenizer.model',\n './merged/added_tokens.json',\n './merged/tokenizer.json')"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained( \"./merged/\", safe_serialization=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T21:53:54.551177Z",
     "start_time": "2024-01-19T21:53:54.525900Z"
    }
   },
   "id": "1328c02bea1c4669"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RESTART 2nd time & load merged model + tokenizer in FP16"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d2de18b760eb37d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/models/Mistral-7B-Instruct-v0.2/merged-00-2024.01.19\n",
      "Loading without BitsAndBytesConfig...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eccd463a26ec4673b0f447ef7834cd47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir( \"/var/model/models/Mistral-7B-Instruct-v0.2/merged-00-2024.01.19\" )\n",
    "print( os.getcwd() )\n",
    "\n",
    "base_model, tokenizer = get_base_model_and_tokenizer( \n",
    "    use_bnb_cuantization=False, \n",
    "    device_map=\"cuda:1\" \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T22:58:42.605032Z",
     "start_time": "2024-01-20T22:58:34.301071Z"
    }
   },
   "id": "40f85a90c6d1f92e"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate_df.shape (100, 5)\n",
      "Commands file for [search new tab] exists: True\n",
      "Commands file for [search current tab] exists: True\n",
      "Commands file for [search google new tab] exists: True\n",
      "Commands file for [search google current tab] exists: True\n",
      "Commands file for [search google scholar new tab] exists: True\n",
      "Commands file for [search google scholar current tab] exists: True\n",
      "\n",
      "Generating responses for 100 rows...\n",
      "Using HuggingFace model_name [mistralai/Mistral-7B-Instruct-v0.2] in memory...\n",
      "\n",
      "Processing call [001] out of [100] = [1.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 2,138 ms\n",
      "Tokens per second [68.8]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>TabError</args></response>]\n",
      "\n",
      "Processing call [002] out of [100] = [2.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,571 ms\n",
      "Tokens per second [148.9]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>How can you handle reference errors, especially with weak references, in Python?</args></response>]\n",
      "\n",
      "Processing call [003] out of [100] = [3.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,193 ms\n",
      "Tokens per second [129.9]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>IndentationError</args></response>]\n",
      "\n",
      "Processing call [004] out of [100] = [4.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,224 ms\n",
      "Tokens per second [141.3]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>BERT model improvements</args></response>]\n",
      "\n",
      "Processing call [005] out of [100] = [5.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,372 ms\n",
      "Tokens per second [133.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Creating histograms in Pandas</args></response>]\n",
      "\n",
      "Processing call [006] out of [100] = [6.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,570 ms\n",
      "Tokens per second [153.5]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>What are the best practices for managing timeout errors in Python, especially in network requests?</args></response>]\n",
      "\n",
      "Processing call [007] out of [100] = [7.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,222 ms\n",
      "Tokens per second [135.8]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Process Lookup Error</args></response>]\n",
      "\n",
      "Processing call [008] out of [100] = [8.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,164 ms\n",
      "Tokens per second [134.9]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>UserWarning</args></response>]\n",
      "\n",
      "Processing call [009] out of [100] = [9.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,164 ms\n",
      "Tokens per second [137.5]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AttributeError</args></response>]\n",
      "\n",
      "Processing call [010] out of [100] = [10.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,193 ms\n",
      "Tokens per second [140.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>pop music</args></response>]\n",
      "\n",
      "Processing call [011] out of [100] = [11.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,513 ms\n",
      "Tokens per second [142.7]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>How can you manage errors that occur due to missing files in Python programs?</args></response>]\n",
      "\n",
      "Processing call [012] out of [100] = [12.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,628 ms\n",
      "Tokens per second [148.6]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Data Science career trends: What are the current career trends in the field of data science?</args></response>]\n",
      "\n",
      "Processing call [013] out of [100] = [13.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,164 ms\n",
      "Tokens per second [138.3]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>UserWarning</args></response>]\n",
      "\n",
      "Processing call [014] out of [100] = [14.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,309 ms\n",
      "Tokens per second [144.4]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Generating summary statistics in Pandas</args></response>]\n",
      "\n",
      "Processing call [015] out of [100] = [15.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,627 ms\n",
      "Tokens per second [152.4]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Google Cloud AI services: What AI services does Google Cloud offer for developers and data scientists?</args></response>]\n",
      "\n",
      "Processing call [016] out of [100] = [16.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,223 ms\n",
      "Tokens per second [132.5]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>how to paint a room</args></response>]\n",
      "\n",
      "Processing call [017] out of [100] = [17.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,400 ms\n",
      "Tokens per second [145.0]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Unsupported Function Call: Function not supported for this dtype</args></response>]\n",
      "\n",
      "Processing call [018] out of [100] = [18.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,195 ms\n",
      "Tokens per second [144.8]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>PermissionError</args></response>]\n",
      "\n",
      "Processing call [019] out of [100] = [19.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,627 ms\n",
      "Tokens per second [145.7]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>What are the reasons for attribute-related errors in Python objects and how can they be fixed?</args></response>]\n",
      "\n",
      "Processing call [020] out of [100] = [20.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,338 ms\n",
      "Tokens per second [139.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Data deduplication in Pandas</args></response>]\n",
      "\n",
      "Processing call [021] out of [100] = [21.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,164 ms\n",
      "Tokens per second [135.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>OS Error</args></response>]\n",
      "\n",
      "Processing call [022] out of [100] = [22.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,543 ms\n",
      "Tokens per second [156.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Responsible AI practices: How can organizations implement responsible AI practices?</args></response>]\n",
      "\n",
      "Processing call [023] out of [100] = [23.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,280 ms\n",
      "Tokens per second [135.9]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>popular music genres</args></response>]\n",
      "\n",
      "Processing call [024] out of [100] = [24.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,194 ms\n",
      "Tokens per second [130.7]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Blocking IO Error</args></response>]\n",
      "\n",
      "Processing call [025] out of [100] = [25.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,341 ms\n",
      "Tokens per second [136.5]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Convolutional Neural Networks</args></response>]\n",
      "\n",
      "Processing call [026] out of [100] = [26.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,307 ms\n",
      "Tokens per second [141.5]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Data partitioning in Pandas</args></response>]\n",
      "\n",
      "Processing call [027] out of [100] = [27.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,655 ms\n",
      "Tokens per second [148.0]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>What are runtime warnings in Python, and how can they be used to identify potential issues?</args></response>]\n",
      "\n",
      "Processing call [028] out of [100] = [28.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,164 ms\n",
      "Tokens per second [131.4]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Syntax Warning</args></response>]\n",
      "\n",
      "Processing call [029] out of [100] = [29.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,542 ms\n",
      "Tokens per second [140.7]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>What are the best practices for handling I/O blocking errors in Python?</args></response>]\n",
      "\n",
      "Processing call [030] out of [100] = [30.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,627 ms\n",
      "Tokens per second [146.3]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI for health diagnostics: How does AI contribute to advancements in health diagnostics?</args></response>]\n",
      "\n",
      "Processing call [031] out of [100] = [31.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,570 ms\n",
      "Tokens per second [157.3]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Deep Learning optimization: What strategies are used for optimizing deep learning models?</args></response>]\n",
      "\n",
      "Processing call [032] out of [100] = [32.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,599 ms\n",
      "Tokens per second [148.8]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>How can you resolve recursion errors due to excessive recursive calls in Python?</args></response>]\n",
      "\n",
      "Processing call [033] out of [100] = [33.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,193 ms\n",
      "Tokens per second [142.5]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Buffer Error</args></response>]\n",
      "\n",
      "Processing call [034] out of [100] = [34.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,193 ms\n",
      "Tokens per second [138.3]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>classical music</args></response>]\n",
      "\n",
      "Processing call [035] out of [100] = [35.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,308 ms\n",
      "Tokens per second [144.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Aggregation functions in Pandas</args></response>]\n",
      "\n",
      "Processing call [036] out of [100] = [36.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,228 ms\n",
      "Tokens per second [140.9]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>AI in manufacturing</args></response>]\n",
      "\n",
      "Processing call [037] out of [100] = [37.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,194 ms\n",
      "Tokens per second [144.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>ResourceWarning</args></response>]\n",
      "\n",
      "Processing call [038] out of [100] = [38.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,314 ms\n",
      "Tokens per second [128.6]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>why do zebras have stripes</args></response>]\n",
      "\n",
      "Processing call [039] out of [100] = [39.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,194 ms\n",
      "Tokens per second [139.0]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>BytesWarning</args></response>]\n",
      "\n",
      "Processing call [040] out of [100] = [40.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,341 ms\n",
      "Tokens per second [141.7]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>OS Error: Operating system error</args></response>]\n",
      "\n",
      "Processing call [041] out of [100] = [41.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,399 ms\n",
      "Tokens per second [139.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Exploratory data analysis with Pandas</args></response>]\n",
      "\n",
      "Processing call [042] out of [100] = [42.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,223 ms\n",
      "Tokens per second [137.4]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>PendingDeprecationWarning</args></response>]\n",
      "\n",
      "Processing call [043] out of [100] = [43.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,313 ms\n",
      "Tokens per second [137.1]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Binning data in Pandas</args></response>]\n",
      "\n",
      "Processing call [044] out of [100] = [44.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,250 ms\n",
      "Tokens per second [149.6]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>latest space exploration news</args></response>]\n",
      "\n",
      "Processing call [045] out of [100] = [45.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,279 ms\n",
      "Tokens per second [134.5]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>how to compost at home</args></response>]\n",
      "\n",
      "Processing call [046] out of [100] = [46.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,165 ms\n",
      "Tokens per second [133.0]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>URL Error</args></response>]\n",
      "\n",
      "Processing call [047] out of [100] = [47.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,740 ms\n",
      "Tokens per second [138.5]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AWS SageMaker for ML: How does AWS SageMaker facilitate machine learning model development?</args></response>]\n",
      "\n",
      "Processing call [048] out of [100] = [48.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,251 ms\n",
      "Tokens per second [143.1]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI in natural language generation</args></response>]\n",
      "\n",
      "Processing call [049] out of [100] = [49.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,307 ms\n",
      "Tokens per second [143.8]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Time series analysis in Pandas</args></response>]\n",
      "\n",
      "Processing call [050] out of [100] = [50.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,279 ms\n",
      "Tokens per second [131.4]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Bytes Warning: Bytecode issue</args></response>]\n",
      "\n",
      "Processing call [051] out of [100] = [51.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,313 ms\n",
      "Tokens per second [138.6]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>OS Error: Operating system error</args></response>]\n",
      "\n",
      "Processing call [052] out of [100] = [52.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,221 ms\n",
      "Tokens per second [138.4]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI in public safety</args></response>]\n",
      "\n",
      "Processing call [053] out of [100] = [53.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,279 ms\n",
      "Tokens per second [140.0]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Using Pandas for predictive modeling</args></response>]\n",
      "\n",
      "Processing call [054] out of [100] = [54.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,194 ms\n",
      "Tokens per second [138.2]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>new car reviews</args></response>]\n",
      "\n",
      "Processing call [055] out of [100] = [55.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,250 ms\n",
      "Tokens per second [136.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>how to plant tomatoes</args></response>]\n",
      "\n",
      "Processing call [056] out of [100] = [56.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,222 ms\n",
      "Tokens per second [140.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI in retail analytics</args></response>]\n",
      "\n",
      "Processing call [057] out of [100] = [57.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,279 ms\n",
      "Tokens per second [136.0]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Handling large datasets with Pandas</args></response>]\n",
      "\n",
      "Processing call [058] out of [100] = [58.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,568 ms\n",
      "Tokens per second [147.3]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>What are the steps to resolve permission-related errors in file operations in Python?</args></response>]\n",
      "\n",
      "Processing call [059] out of [100] = [59.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,513 ms\n",
      "Tokens per second [145.4]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>How can you manage errors that occur due to missing files in Python programs?</args></response>]\n",
      "\n",
      "Processing call [060] out of [100] = [60.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,570 ms\n",
      "Tokens per second [145.2]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI in sports analytics: How is AI used in sports analytics to improve performance?</args></response>]\n",
      "\n",
      "Processing call [061] out of [100] = [61.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,223 ms\n",
      "Tokens per second [142.3]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>ConnectionResetError</args></response>]\n",
      "\n",
      "Processing call [062] out of [100] = [62.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,195 ms\n",
      "Tokens per second [133.9]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>sports</args></response>]\n",
      "\n",
      "Processing call [063] out of [100] = [63.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,285 ms\n",
      "Tokens per second [139.3]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>AI-driven fraud detection</args></response>]\n",
      "\n",
      "Processing call [064] out of [100] = [64.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,684 ms\n",
      "Tokens per second [142.5]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Real-time data streaming: How do you handle real-time data streaming in big data projects?</args></response>]\n",
      "\n",
      "Processing call [065] out of [100] = [65.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,799 ms\n",
      "Tokens per second [141.2]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Outlier Detection methods: What methods are most effective for outlier detection in univariate datasets?</args></response>]\n",
      "\n",
      "Processing call [066] out of [100] = [66.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,343 ms\n",
      "Tokens per second [148.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Runtime Warning: Runtime behavior warning</args></response>]\n",
      "\n",
      "Processing call [067] out of [100] = [67.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,400 ms\n",
      "Tokens per second [126.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>how do i cure dandruff?</args></response>]\n",
      "\n",
      "Processing call [068] out of [100] = [68.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,542 ms\n",
      "Tokens per second [149.2]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI in digital marketing: What role does AI play in digital marketing strategies?</args></response>]\n",
      "\n",
      "Processing call [069] out of [100] = [69.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,252 ms\n",
      "Tokens per second [138.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>ArithmeticError</args></response>]\n",
      "\n",
      "Processing call [070] out of [100] = [70.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,193 ms\n",
      "Tokens per second [136.6]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>exchange rates today</args></response>]\n",
      "\n",
      "Processing call [071] out of [100] = [71.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,252 ms\n",
      "Tokens per second [133.4]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Merging strategies in Pandas</args></response>]\n",
      "\n",
      "Processing call [072] out of [100] = [72.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,222 ms\n",
      "Tokens per second [142.4]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [073] out of [100] = [73.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,685 ms\n",
      "Tokens per second [148.4]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>What strategies can you use to fix type errors that arise from operations on incompatible data types in Python?</args></response>]\n",
      "\n",
      "Processing call [074] out of [100] = [74.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,222 ms\n",
      "Tokens per second [130.1]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>DIY home decor ideas</args></response>]\n",
      "\n",
      "Processing call [075] out of [100] = [75.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,513 ms\n",
      "Tokens per second [136.2]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI patent trends: What are the current trends in AI patents?</args></response>]\n",
      "\n",
      "Processing call [076] out of [100] = [76.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,571 ms\n",
      "Tokens per second [147.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI in sports analytics: How is AI used in sports analytics to improve performance?</args></response>]\n",
      "\n",
      "Processing call [077] out of [100] = [77.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,628 ms\n",
      "Tokens per second [148.6]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Data Science career trends: What are the current career trends in the field of data science?</args></response>]\n",
      "\n",
      "Processing call [078] out of [100] = [78.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,284 ms\n",
      "Tokens per second [136.3]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Gradient Boosting</args></response>]\n",
      "\n",
      "Processing call [079] out of [100] = [79.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,222 ms\n",
      "Tokens per second [147.3]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Connection Reset Error</args></response>]\n",
      "\n",
      "Processing call [080] out of [100] = [80.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,251 ms\n",
      "Tokens per second [145.5]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Computer Vision applications</args></response>]\n",
      "\n",
      "Processing call [081] out of [100] = [81.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,222 ms\n",
      "Tokens per second [130.9]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Ethical AI guidelines</args></response>]\n",
      "\n",
      "Processing call [082] out of [100] = [82.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,222 ms\n",
      "Tokens per second [139.9]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>new book releases</args></response>]\n",
      "\n",
      "Processing call [083] out of [100] = [83.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,365 ms\n",
      "Tokens per second [134.1]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>why don't people like me?</args></response>]\n",
      "\n",
      "Processing call [084] out of [100] = [84.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,306 ms\n",
      "Tokens per second [144.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>weather forecast washington dc</args></response>]\n",
      "\n",
      "Processing call [085] out of [100] = [85.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,284 ms\n",
      "Tokens per second [133.2]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>AI in e-commerce personalization</args></response>]\n",
      "\n",
      "Processing call [086] out of [100] = [86.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,221 ms\n",
      "Tokens per second [142.5]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>new dress styles</args></response>]\n",
      "\n",
      "Processing call [087] out of [100] = [87.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,280 ms\n",
      "Tokens per second [130.5]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Performance tuning in Pandas</args></response>]\n",
      "\n",
      "Processing call [088] out of [100] = [88.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,279 ms\n",
      "Tokens per second [132.9]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Data imputation with Pandas</args></response>]\n",
      "\n",
      "Processing call [089] out of [100] = [89.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,626 ms\n",
      "Tokens per second [151.3]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How do you address deprecation warnings in Python to ensure code compatibility with future versions?</args></response>]\n",
      "\n",
      "Processing call [090] out of [100] = [90.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,250 ms\n",
      "Tokens per second [137.6]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Explainable AI</args></response>]\n",
      "\n",
      "Processing call [091] out of [100] = [91.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,307 ms\n",
      "Tokens per second [152.3]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Runtime Error: Runtime exception occurred</args></response>]\n",
      "\n",
      "Processing call [092] out of [100] = [92.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,194 ms\n",
      "Tokens per second [133.2]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Unicode Decode Error</args></response>]\n",
      "\n",
      "Processing call [093] out of [100] = [93.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,222 ms\n",
      "Tokens per second [128.5]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>FloatingPointError</args></response>]\n",
      "\n",
      "Processing call [094] out of [100] = [94.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,341 ms\n",
      "Tokens per second [129.0]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Saving Pandas DataFrame to CSV</args></response>]\n",
      "\n",
      "Processing call [095] out of [100] = [95.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,227 ms\n",
      "Tokens per second [129.6]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>why cats purr</args></response>]\n",
      "\n",
      "Processing call [096] out of [100] = [96.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,484 ms\n",
      "Tokens per second [143.5]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>How do you handle warnings related to Unicode issues in Python?</args></response>]\n",
      "\n",
      "Processing call [097] out of [100] = [97.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,513 ms\n",
      "Tokens per second [140.1]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>What are import warnings in Python, and how can they be addressed?</args></response>]\n",
      "\n",
      "Processing call [098] out of [100] = [98.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,223 ms\n",
      "Tokens per second [138.2]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>why does bread rise</args></response>]\n",
      "\n",
      "Processing call [099] out of [100] = [99.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,279 ms\n",
      "Tokens per second [135.3]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>how to bake chocolate chip cookies</args></response>]\n",
      "\n",
      "Processing call [100] out of [100] = [100.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,307 ms\n",
      "Tokens per second [141.5]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Overflow Error: Value too large to convert</args></response>]\n",
      "\n",
      "\n",
      "Generating responses for 100 rows... Done! in 02:15\n",
      "[1353.9] ms per item\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation stats for model mistralai/Mistral-7B-Instruct-v0.2\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "          Contains response 100.0%\n",
      "   Contains browser command 100.0%\n",
      "              Contains args 100.0%\n",
      "          Response is exact 99.0%\n",
      "Response has correct values 99.0%\n",
      " Browser command is correct 100.0%\n",
      "            Args is correct 99.0%\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "run_validation( base_model, tokenizer, model_name=\"mistralai/Mistral-7B-Instruct-v0.2\", device=\"cuda:1\" )\n",
    "\n",
    "# Generating responses for 100 rows... Done! in 02:15\n",
    "# [1353.9] ms per item\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation stats for model mistralai/Mistral-7B-Instruct-v0.2, raw bfloat16 loaded from w/in Jupiter notebook\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#  Contains <browser-command> 100.0%\n",
    "#             Contains <args> 100.0%\n",
    "#           Response is exact 99.0%\n",
    "# Response has correct values 99.0%\n",
    "#  Browser command is correct 100.0%\n",
    "#             Args is correct 99.0%\n",
    "\n",
    "# Exact same model loaded two different ways:\n",
    "# 0: Using TGI with & w/o --dtype bfloat16 flag\n",
    "#    docker run --name huggingface-tgi --gpus all --shm-size 1g -p 3000:3000 -v `pwd`:/data/model \n",
    "#    ghcr.io/huggingface/text-generation-inference:1.3.4 --dtype bfloat16 --sharded false --num-shard 1 --port 3000 \n",
    "#    --model-id /data/model\n",
    "# 1: Using jupyter notebook with raw model file: \n",
    "#    low_cpu_mem_usage=True, \n",
    "#    use_cache=True, \n",
    "#    attn_implementation=\"flash_attention_2\",\n",
    "#    torch_dtype=torch.bfloat16\n",
    "\n",
    "# +---------------------------------------------------------------------------------------+\n",
    "# Sat Jan 20 18:12:36 2024: \n",
    "# +---------------------------------------------------------------------------------------+\n",
    "# | NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "# |-----------------------------------------+----------------------+----------------------+\n",
    "# | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "# | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "# |                                         |                      |               MIG M. |\n",
    "# |=========================================+======================+======================|\n",
    "# |   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0 Off |                  Off |\n",
    "# |  0%   36C    P8              20W / 450W |  23146MiB / 24564MiB |      0%      Default |\n",
    "# |                                         |                      |                  N/A |\n",
    "# +-----------------------------------------+----------------------+----------------------+\n",
    "# |   1  NVIDIA GeForce RTX 4090        On  | 00000000:02:00.0 Off |                  Off |\n",
    "# |  0%   43C    P8              29W / 450W |  15336MiB / 24564MiB |      0%      Default |\n",
    "# |                                         |                      |                  N/A |\n",
    "# +-----------------------------------------+----------------------+----------------------+\n",
    "# \n",
    "# +---------------------------------------------------------------------------------------+\n",
    "# | Processes:                                                                            |\n",
    "# |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "# |        ID   ID                                                             Usage      |\n",
    "# |=======================================================================================|\n",
    "# |    0   N/A  N/A     11326      C   /opt/conda/bin/python3.10                 23136MiB |\n",
    "# |    1   N/A  N/A      6750      C   /usr/bin/python3                          15326MiB |\n",
    "# +---------------------------------------------------------------------------------------+"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:05:58.247262Z",
     "start_time": "2024-01-20T23:03:42.645818Z"
    }
   },
   "id": "1ea31d974c4ce0c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run benchmark on TGI service listening on port 3000"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48c96a0e91e2d006"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands file for [search new tab] exists: False\n",
      "Commands file for [search current tab] exists: False\n",
      "Commands file for [search google new tab] exists: False\n",
      "Commands file for [search google current tab] exists: False\n",
      "Commands file for [search google scholar new tab] exists: False\n",
      "Commands file for [search google scholar current tab] exists: False\n",
      "\n",
      "Generating responses for 100 rows...\n",
      "Using TGI w/ model_name [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "Processing call [001] out of [100] = [1.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>TabError</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 682 ms\n",
      "Tokens per second [51.3]\n",
      "Token list length [35]\n",
      "Processing call [002] out of [100] = [2.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>How can you handle reference errors, especially with weak references, in Python?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 924 ms\n",
      "Tokens per second [54.1]\n",
      "Token list length [50]\n",
      "Processing call [003] out of [100] = [3.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>IndentationError</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 687 ms\n",
      "Tokens per second [53.9]\n",
      "Token list length [37]\n",
      "Processing call [004] out of [100] = [4.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>BERT model improvements</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 708 ms\n",
      "Tokens per second [53.7]\n",
      "Token list length [38]\n",
      "Processing call [005] out of [100] = [5.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>Creating histograms in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 799 ms\n",
      "Tokens per second [53.8]\n",
      "Token list length [43]\n",
      "Processing call [006] out of [100] = [6.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>What are the best practices for managing timeout errors in Python, especially in network requests?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 939 ms\n",
      "Tokens per second [53.2]\n",
      "Token list length [50]\n",
      "Processing call [007] out of [100] = [7.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>Process Lookup Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 715 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [38]\n",
      "Processing call [008] out of [100] = [8.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>UserWarning</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 678 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [36]\n",
      "Processing call [009] out of [100] = [9.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>AttributeError</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 679 ms\n",
      "Tokens per second [53.0]\n",
      "Token list length [36]\n",
      "Processing call [010] out of [100] = [10.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>pop music</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 698 ms\n",
      "Tokens per second [53.0]\n",
      "Token list length [37]\n",
      "Processing call [011] out of [100] = [11.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>How can you manage errors that occur due to missing files in Python programs?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 898 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [48]\n",
      "Processing call [012] out of [100] = [12.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>Data Science career trends: What are the current career trends in the field of data science?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 970 ms\n",
      "Tokens per second [53.6]\n",
      "Token list length [52]\n",
      "Processing call [013] out of [100] = [13.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>UserWarning</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 680 ms\n",
      "Tokens per second [52.9]\n",
      "Token list length [36]\n",
      "Processing call [014] out of [100] = [14.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>Generating summary statistics in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 772 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [41]\n",
      "Processing call [015] out of [100] = [15.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>Google Cloud AI services: What AI services does Google Cloud offer for developers and data scientists?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 972 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [52]\n",
      "Processing call [016] out of [100] = [16.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>how to paint a room</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 716 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [38]\n",
      "Processing call [017] out of [100] = [17.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Unsupported Function Call: Function not supported for this dtype</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 825 ms\n",
      "Tokens per second [53.3]\n",
      "Token list length [44]\n",
      "Processing call [018] out of [100] = [18.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>PermissionError</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 696 ms\n",
      "Tokens per second [53.2]\n",
      "Token list length [37]\n",
      "Processing call [019] out of [100] = [19.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>What are the reasons for attribute-related errors in Python objects and how can they be fixed?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 966 ms\n",
      "Tokens per second [53.8]\n",
      "Token list length [52]\n",
      "Processing call [020] out of [100] = [20.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Data deduplication in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 782 ms\n",
      "Tokens per second [53.7]\n",
      "Token list length [42]\n",
      "Processing call [021] out of [100] = [21.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>OS Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 671 ms\n",
      "Tokens per second [53.7]\n",
      "Token list length [36]\n",
      "Processing call [022] out of [100] = [22.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Responsible AI practices: How can organizations implement responsible AI practices?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 908 ms\n",
      "Tokens per second [54.0]\n",
      "Token list length [49]\n",
      "Processing call [023] out of [100] = [23.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>popular music genres</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 747 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [40]\n",
      "Processing call [024] out of [100] = [24.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Blocking IO Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 689 ms\n",
      "Tokens per second [53.7]\n",
      "Token list length [37]\n",
      "Processing call [025] out of [100] = [25.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>Convolutional Neural Networks</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 784 ms\n",
      "Tokens per second [53.6]\n",
      "Token list length [42]\n",
      "Processing call [026] out of [100] = [26.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Data partitioning in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 763 ms\n",
      "Tokens per second [53.7]\n",
      "Token list length [41]\n",
      "Processing call [027] out of [100] = [27.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>What are runtime warnings in Python, and how can they be used to identify potential issues?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 979 ms\n",
      "Tokens per second [54.1]\n",
      "Token list length [53]\n",
      "Processing call [028] out of [100] = [28.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Syntax Warning</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 673 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [36]\n",
      "Processing call [029] out of [100] = [29.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>What are the best practices for handling I/O blocking errors in Python?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 908 ms\n",
      "Tokens per second [54.0]\n",
      "Token list length [49]\n",
      "Processing call [030] out of [100] = [30.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>AI for health diagnostics: How does AI contribute to advancements in health diagnostics?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 963 ms\n",
      "Tokens per second [54.0]\n",
      "Token list length [52]\n",
      "Processing call [031] out of [100] = [31.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Deep Learning optimization: What strategies are used for optimizing deep learning models?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 924 ms\n",
      "Tokens per second [54.1]\n",
      "Token list length [50]\n",
      "Processing call [032] out of [100] = [32.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>How can you resolve recursion errors due to excessive recursive calls in Python?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 943 ms\n",
      "Tokens per second [54.1]\n",
      "Token list length [51]\n",
      "Processing call [033] out of [100] = [33.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Buffer Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 693 ms\n",
      "Tokens per second [53.4]\n",
      "Token list length [37]\n",
      "Processing call [034] out of [100] = [34.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>classical music</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 691 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [37]\n",
      "Processing call [035] out of [100] = [35.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Aggregation functions in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 762 ms\n",
      "Tokens per second [53.8]\n",
      "Token list length [41]\n",
      "Processing call [036] out of [100] = [36.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>AI in manufacturing</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 711 ms\n",
      "Tokens per second [53.4]\n",
      "Token list length [38]\n",
      "Processing call [037] out of [100] = [37.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>ResourceWarning</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 691 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [37]\n",
      "Processing call [038] out of [100] = [38.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>why do zebras have stripes</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 766 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [41]\n",
      "Processing call [039] out of [100] = [39.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>BytesWarning</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 695 ms\n",
      "Tokens per second [53.2]\n",
      "Token list length [37]\n",
      "Processing call [040] out of [100] = [40.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>OS Error: Operating system error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 780 ms\n",
      "Tokens per second [53.8]\n",
      "Token list length [42]\n",
      "Processing call [041] out of [100] = [41.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Exploratory data analysis with Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 824 ms\n",
      "Tokens per second [53.4]\n",
      "Token list length [44]\n",
      "Processing call [042] out of [100] = [42.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>PendingDeprecationWarning</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 710 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [38]\n",
      "Processing call [043] out of [100] = [43.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Binning data in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 761 ms\n",
      "Tokens per second [53.9]\n",
      "Token list length [41]\n",
      "Processing call [044] out of [100] = [44.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>latest space exploration news</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 727 ms\n",
      "Tokens per second [53.6]\n",
      "Token list length [39]\n",
      "Processing call [045] out of [100] = [45.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>how to compost at home</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 747 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [40]\n",
      "Processing call [046] out of [100] = [46.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>URL Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 671 ms\n",
      "Tokens per second [53.7]\n",
      "Token list length [36]\n",
      "Processing call [047] out of [100] = [47.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>AWS SageMaker for ML: How does AWS SageMaker facilitate machine learning model development?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 1,034 ms\n",
      "Tokens per second [54.2]\n",
      "Token list length [56]\n",
      "Processing call [048] out of [100] = [48.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>AI in natural language generation</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 728 ms\n",
      "Tokens per second [53.6]\n",
      "Token list length [39]\n",
      "Processing call [049] out of [100] = [49.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Time series analysis in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 763 ms\n",
      "Tokens per second [53.7]\n",
      "Token list length [41]\n",
      "Processing call [050] out of [100] = [50.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Bytes Warning: Bytecode issue</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 746 ms\n",
      "Tokens per second [53.6]\n",
      "Token list length [40]\n",
      "Processing call [051] out of [100] = [51.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>OS Error: Operating system error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 766 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [41]\n",
      "Processing call [052] out of [100] = [52.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>AI in public safety</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 709 ms\n",
      "Tokens per second [53.6]\n",
      "Token list length [38]\n",
      "Processing call [053] out of [100] = [53.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>Using Pandas for predictive modeling</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 743 ms\n",
      "Tokens per second [53.8]\n",
      "Token list length [40]\n",
      "Processing call [054] out of [100] = [54.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>new car reviews</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 693 ms\n",
      "Tokens per second [53.4]\n",
      "Token list length [37]\n",
      "Processing call [055] out of [100] = [55.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>how to plant tomatoes</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 727 ms\n",
      "Tokens per second [53.6]\n",
      "Token list length [39]\n",
      "Processing call [056] out of [100] = [56.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>AI in retail analytics</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 715 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [38]\n",
      "Processing call [057] out of [100] = [57.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Handling large datasets with Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 752 ms\n",
      "Tokens per second [53.2]\n",
      "Token list length [40]\n",
      "Processing call [058] out of [100] = [58.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>What are the steps to resolve permission-related errors in file operations in Python?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 924 ms\n",
      "Tokens per second [54.1]\n",
      "Token list length [50]\n",
      "Processing call [059] out of [100] = [59.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>How can you manage errors that occur due to missing files in Python programs?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 892 ms\n",
      "Tokens per second [53.8]\n",
      "Token list length [48]\n",
      "Processing call [060] out of [100] = [60.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>AI in sports analytics: How is AI used in sports analytics to improve performance?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 927 ms\n",
      "Tokens per second [53.9]\n",
      "Token list length [50]\n",
      "Processing call [061] out of [100] = [61.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>ConnectionResetError</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 711 ms\n",
      "Tokens per second [53.4]\n",
      "Token list length [38]\n",
      "Processing call [062] out of [100] = [62.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>sports</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 692 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [37]\n",
      "Processing call [063] out of [100] = [63.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>AI-driven fraud detection</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 749 ms\n",
      "Tokens per second [53.4]\n",
      "Token list length [40]\n",
      "Processing call [064] out of [100] = [64.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>Real-time data streaming: How do you handle real-time data streaming in big data projects?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 998 ms\n",
      "Tokens per second [54.1]\n",
      "Token list length [54]\n",
      "Processing call [065] out of [100] = [65.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>Outlier Detection methods: What methods are most effective for outlier detection in univariate datasets?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 1,073 ms\n",
      "Tokens per second [54.1]\n",
      "Token list length [58]\n",
      "Processing call [066] out of [100] = [66.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Runtime Warning: Runtime behavior warning</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 786 ms\n",
      "Tokens per second [53.4]\n",
      "Token list length [42]\n",
      "Processing call [067] out of [100] = [67.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>how do i cure dandruff?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 822 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [44]\n",
      "Processing call [068] out of [100] = [68.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>AI in digital marketing: What role does AI play in digital marketing strategies?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 911 ms\n",
      "Tokens per second [53.8]\n",
      "Token list length [49]\n",
      "Processing call [069] out of [100] = [69.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>ArithmeticError</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 729 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [39]\n",
      "Processing call [070] out of [100] = [70.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>exchange rates today</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 691 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [37]\n",
      "Processing call [071] out of [100] = [71.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Merging strategies in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 736 ms\n",
      "Tokens per second [53.0]\n",
      "Token list length [39]\n",
      "Processing call [072] out of [100] = [72.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>Connection Refused Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 716 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [38]\n",
      "Processing call [073] out of [100] = [73.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>What strategies can you use to fix type errors that arise from operations on incompatible data types in Python?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 1,008 ms\n",
      "Tokens per second [53.6]\n",
      "Token list length [54]\n",
      "Processing call [074] out of [100] = [74.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>DIY home decor ideas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 716 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [38]\n",
      "Processing call [075] out of [100] = [75.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>AI patent trends: What are the current trends in AI patents?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 898 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [48]\n",
      "Processing call [076] out of [100] = [76.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>AI in sports analytics: How is AI used in sports analytics to improve performance?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 934 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [50]\n",
      "Processing call [077] out of [100] = [77.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>Data Science career trends: What are the current career trends in the field of data science?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 972 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [52]\n",
      "Processing call [078] out of [100] = [78.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Gradient Boosting</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 752 ms\n",
      "Tokens per second [53.2]\n",
      "Token list length [40]\n",
      "Processing call [079] out of [100] = [79.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Connection Reset Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 715 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [38]\n",
      "Processing call [080] out of [100] = [80.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>Computer Vision applications</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 734 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [39]\n",
      "Processing call [081] out of [100] = [81.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Ethical AI guidelines</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 715 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [38]\n",
      "Processing call [082] out of [100] = [82.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>new book releases</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 717 ms\n",
      "Tokens per second [53.0]\n",
      "Token list length [38]\n",
      "Processing call [083] out of [100] = [83.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>why don't people like me?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 807 ms\n",
      "Tokens per second [53.3]\n",
      "Token list length [43]\n",
      "Processing call [084] out of [100] = [84.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>weather forecast washington dc</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 772 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [41]\n",
      "Processing call [085] out of [100] = [85.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>AI in e-commerce personalization</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 752 ms\n",
      "Tokens per second [53.2]\n",
      "Token list length [40]\n",
      "Processing call [086] out of [100] = [86.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>new dress styles</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 716 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [38]\n",
      "Processing call [087] out of [100] = [87.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Performance tuning in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 753 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [40]\n",
      "Processing call [088] out of [100] = [88.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>Data imputation with Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 757 ms\n",
      "Tokens per second [52.8]\n",
      "Token list length [40]\n",
      "Processing call [089] out of [100] = [89.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>How do you address deprecation warnings in Python to ensure code compatibility with future versions?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 976 ms\n",
      "Tokens per second [53.3]\n",
      "Token list length [52]\n",
      "Processing call [090] out of [100] = [90.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Explainable AI</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 734 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [39]\n",
      "Processing call [091] out of [100] = [91.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Runtime Error: Runtime exception occurred</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 770 ms\n",
      "Tokens per second [53.2]\n",
      "Token list length [41]\n",
      "Processing call [092] out of [100] = [92.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Unicode Decode Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 699 ms\n",
      "Tokens per second [52.9]\n",
      "Token list length [37]\n",
      "Processing call [093] out of [100] = [93.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>FloatingPointError</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 715 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [38]\n",
      "Processing call [094] out of [100] = [94.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>Saving Pandas DataFrame to CSV</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 789 ms\n",
      "Tokens per second [53.2]\n",
      "Token list length [42]\n",
      "Processing call [095] out of [100] = [95.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>why cats purr</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 716 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [38]\n",
      "Processing call [096] out of [100] = [96.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>How do you handle warnings related to Unicode issues in Python?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 879 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [47]\n",
      "Processing call [097] out of [100] = [97.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>What are import warnings in Python, and how can they be addressed?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 898 ms\n",
      "Tokens per second [53.5]\n",
      "Token list length [48]\n",
      "Processing call [098] out of [100] = [98.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>why does bread rise</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 716 ms\n",
      "Tokens per second [53.1]\n",
      "Token list length [38]\n",
      "Processing call [099] out of [100] = [99.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>how to bake chocolate chip cookies</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 752 ms\n",
      "Tokens per second [53.2]\n",
      "Token list length [40]\n",
      "Processing call [100] out of [100] = [100.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>Overflow Error: Value too large to convert</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 770 ms\n",
      "Tokens per second [53.2]\n",
      "Token list length [41]\n",
      "\n",
      "Generating responses for 100 rows... Done! in 01:18\n",
      "[788.9] ms per item\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation Stats for `mistralai/Mistral-7B-Instruct-v0.2-AWQ` on TGI:3000\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "          Contains response 100.0%\n",
      "   Contains browser command 100.0%\n",
      "              Contains args 100.0%\n",
      "          Response is exact 99.0%\n",
      "Response has correct values 99.0%\n",
      " Browser command is correct 100.0%\n",
      "            Args is correct 99.0%\n"
     ]
    }
   ],
   "source": [
    "tgi_validator  = XmlFineTuningPromptGenerator( tgi_url=\"http://172.17.0.4:3000\", debug=True )\n",
    "\n",
    "model_name     = \"mistralai/Mistral-7B-Instruct-v0.2-raw-bfloat16\"\n",
    "\n",
    "validate_df    = pd.read_json( \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\", lines=True ).sample( 100, random_state=42 )\n",
    "validate_df    = tgi_validator.generate_responses( validate_df, switch=\"tgi\", model_name=model_name )\n",
    "validate_df    = tgi_validator.validate_responses( validate_df )\n",
    "\n",
    "tgi_validator.print_validation_stats( validate_df, title=f\"Validation Stats for `{model_name}` on TGI:3000\" )\n",
    "\n",
    "# Generating responses for 100 rows... Done! in 01:18\n",
    "# [788.9] ms per item\n",
    "# \n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation Stats for `mistralai/Mistral-7B-Instruct-v0.2 raw bfloat16` on TGI:3000\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#  Contains <browser-command> 100.0%\n",
    "#             Contains <args> 100.0%\n",
    "#           Response is exact 99.0%\n",
    "# Response has correct values 99.0%\n",
    "#  Browser command is correct 100.0%\n",
    "#             Args is correct 99.0%"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:34:27.307028Z",
     "start_time": "2024-01-20T23:33:08.323032Z"
    }
   },
   "id": "d45ff5f83a4cddc5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantize using AWQ (Adaptive Weight Quantization) and write to disk"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e43fb2866e0570b"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# !pip install autoawq"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:27:23.715624Z",
     "start_time": "2024-01-21T00:27:23.659871Z"
    }
   },
   "id": "fedef541900b6d7d"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/models/Mistral-7B-Instruct-v0.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir( \"/var/model/models/Mistral-7B-Instruct-v0.2/\" )\n",
    "print( os.getcwd() )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T23:48:24.928590Z",
     "start_time": "2024-01-20T23:48:24.913173Z"
    }
   },
   "id": "c6f37e59e4c4f0db"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "900346efd3464bb2bf4c183c0aee9559"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/167 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1959eb94983347bb80f8c5f69ef1b7e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/471M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8aeab124a7564b4a90b13a91008de774"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating validation split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07776814dd0848aea53a9da0c23ec6a0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AWQ: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [09:20<00:00, 17.53s/it]\n"
     ]
    }
   ],
   "source": [
    "from awq          import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4 }\n",
    "\n",
    "# Load model and tokenizer\n",
    "raw_16bit_model     = AutoAWQForCausalLM.from_pretrained( \"./merged-00-2024.01.19/\", device_map=\"auto\", safetensors=True )\n",
    "raw_16bit_tokenizer = AutoTokenizer.from_pretrained( \"./merged-00-2024.01.19/\", use_fast=True )\n",
    "\n",
    "# Quantize\n",
    "raw_16bit_model.quantize( raw_16bit_tokenizer, quant_config=quant_config )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:00:35.787700Z",
     "start_time": "2024-01-20T23:50:48.649254Z"
    }
   },
   "id": "b0c18bea4f9438df"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:`quant_config.json` is being deprecated in the future in favor of quantization_config in config.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": "('./merged-00-2024.01.19.awq/tokenizer_config.json',\n './merged-00-2024.01.19.awq/special_tokens_map.json',\n './merged-00-2024.01.19.awq/tokenizer.model',\n './merged-00-2024.01.19.awq/added_tokens.json',\n './merged-00-2024.01.19.awq/tokenizer.json')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save quantized model\n",
    "path = \"./merged-00-2024.01.19.awq\"\n",
    "raw_16bit_model.save_quantized( path, safetensors=True )\n",
    "raw_16bit_tokenizer.save_pretrained( path )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:00:38.958748Z",
     "start_time": "2024-01-21T00:00:35.790944Z"
    }
   },
   "id": "e3da12541a98d521"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validate AWQ model: In memory loaded by Jupiter notebook"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1887818bfac3413f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/model/models/Mistral-7B-Instruct-v0.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir( \"/var/model/models/Mistral-7B-Instruct-v0.2/\" )\n",
    "print( os.getcwd() )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:29:31.794429Z",
     "start_time": "2024-01-21T00:29:31.784581Z"
    }
   },
   "id": "78536f70c1a952e8"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from awq          import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_aqw     = AutoAWQForCausalLM.from_pretrained( \"./merged-00-2024.01.19.awq\", device_map=\"cuda:1\", safetensors=True )\n",
    "tokenizer_awq = AutoTokenizer.from_pretrained( \"./merged-00-2024.01.19.awq/\", use_fast=True )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:29:40.612299Z",
     "start_time": "2024-01-21T00:29:36.535036Z"
    }
   },
   "id": "e5e35ea922857196"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate_df.shape (100, 5)\n",
      "Commands file for [search new tab] exists: True\n",
      "Commands file for [search current tab] exists: True\n",
      "Commands file for [search google new tab] exists: True\n",
      "Commands file for [search google current tab] exists: True\n",
      "Commands file for [search google scholar new tab] exists: True\n",
      "Commands file for [search google scholar current tab] exists: True\n",
      "\n",
      "Generating responses for 100 rows...\n",
      "Using HuggingFace model_name [mistralai/Mistral-7B-Instruct-v0.2] in memory...\n",
      "\n",
      "Processing call [001] out of [100] = [1.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,150 ms\n",
      "Tokens per second [127.8]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>TabError</args></response>]\n",
      "\n",
      "Processing call [002] out of [100] = [2.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,121 ms\n",
      "Tokens per second [208.7]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>How can you handle reference errors, especially with weak references, in Python?</args></response>]\n",
      "\n",
      "Processing call [003] out of [100] = [3.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 858 ms\n",
      "Tokens per second [180.7]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>IndentationError</args></response>]\n",
      "\n",
      "Processing call [004] out of [100] = [4.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 881 ms\n",
      "Tokens per second [196.4]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>BERT model improvements</args></response>]\n",
      "\n",
      "Processing call [005] out of [100] = [5.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 980 ms\n",
      "Tokens per second [186.7]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Creating histograms in Pandas</args></response>]\n",
      "\n",
      "Processing call [006] out of [100] = [6.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,121 ms\n",
      "Tokens per second [215.0]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>What are the best practices for managing timeout errors in Python, especially in network requests?</args></response>]\n",
      "\n",
      "Processing call [007] out of [100] = [7.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 878 ms\n",
      "Tokens per second [188.9]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Process Lookup Error</args></response>]\n",
      "\n",
      "Processing call [008] out of [100] = [8.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 839 ms\n",
      "Tokens per second [187.1]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>UserWarning</args></response>]\n",
      "\n",
      "Processing call [009] out of [100] = [9.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 838 ms\n",
      "Tokens per second [190.9]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AttributeError</args></response>]\n",
      "\n",
      "Processing call [010] out of [100] = [10.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 858 ms\n",
      "Tokens per second [194.6]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>pop music</args></response>]\n",
      "\n",
      "Processing call [011] out of [100] = [11.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,080 ms\n",
      "Tokens per second [200.0]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>How can you manage errors that occur due to missing files in Python programs?</args></response>]\n",
      "\n",
      "Processing call [012] out of [100] = [12.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,159 ms\n",
      "Tokens per second [208.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Data Science career trends: What are the current career trends in the field of data science?</args></response>]\n",
      "\n",
      "Processing call [013] out of [100] = [13.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 840 ms\n",
      "Tokens per second [191.7]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>UserWarning</args></response>]\n",
      "\n",
      "Processing call [014] out of [100] = [14.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 938 ms\n",
      "Tokens per second [201.5]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Generating summary statistics in Pandas</args></response>]\n",
      "\n",
      "Processing call [015] out of [100] = [15.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,160 ms\n",
      "Tokens per second [213.8]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>Google Cloud AI services: What AI services does Google Cloud offer for developers and data scientists?</args></response>]\n",
      "\n",
      "Processing call [016] out of [100] = [16.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 878 ms\n",
      "Tokens per second [184.5]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>how to paint a room</args></response>]\n",
      "\n",
      "Processing call [017] out of [100] = [17.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,001 ms\n",
      "Tokens per second [202.8]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Unsupported Function Call: Function not supported for this dtype</args></response>]\n",
      "\n",
      "Processing call [018] out of [100] = [18.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 859 ms\n",
      "Tokens per second [201.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>PermissionError</args></response>]\n",
      "\n",
      "Processing call [019] out of [100] = [19.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,159 ms\n",
      "Tokens per second [204.5]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>What are the reasons for attribute-related errors in Python objects and how can they be fixed?</args></response>]\n",
      "\n",
      "Processing call [020] out of [100] = [20.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 958 ms\n",
      "Tokens per second [194.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Data deduplication in Pandas</args></response>]\n",
      "\n",
      "Processing call [021] out of [100] = [21.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 839 ms\n",
      "Tokens per second [188.3]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>OS Error</args></response>]\n",
      "\n",
      "Processing call [022] out of [100] = [22.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,100 ms\n",
      "Tokens per second [219.1]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Responsible AI practices: How can organizations implement responsible AI practices?</args></response>]\n",
      "\n",
      "Processing call [023] out of [100] = [23.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 917 ms\n",
      "Tokens per second [189.7]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>popular music genres</args></response>]\n",
      "\n",
      "Processing call [024] out of [100] = [24.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 859 ms\n",
      "Tokens per second [181.6]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Blocking IO Error</args></response>]\n",
      "\n",
      "Processing call [025] out of [100] = [25.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 960 ms\n",
      "Tokens per second [190.6]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Convolutional Neural Networks</args></response>]\n",
      "\n",
      "Processing call [026] out of [100] = [26.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 941 ms\n",
      "Tokens per second [196.6]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Data partitioning in Pandas</args></response>]\n",
      "\n",
      "Processing call [027] out of [100] = [27.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,177 ms\n",
      "Tokens per second [208.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>What are runtime warnings in Python, and how can they be used to identify potential issues?</args></response>]\n",
      "\n",
      "Processing call [028] out of [100] = [28.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 840 ms\n",
      "Tokens per second [182.1]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Syntax Warning</args></response>]\n",
      "\n",
      "Processing call [029] out of [100] = [29.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,097 ms\n",
      "Tokens per second [197.6]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>What are the best practices for handling I/O blocking errors in Python?</args></response>]\n",
      "\n",
      "Processing call [030] out of [100] = [30.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,161 ms\n",
      "Tokens per second [205.0]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI for health diagnostics: How does AI contribute to advancements in health diagnostics?</args></response>]\n",
      "\n",
      "Processing call [031] out of [100] = [31.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,118 ms\n",
      "Tokens per second [220.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Deep Learning optimization: What strategies are used for optimizing deep learning models?</args></response>]\n",
      "\n",
      "Processing call [032] out of [100] = [32.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,141 ms\n",
      "Tokens per second [208.6]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>How can you resolve recursion errors due to excessive recursive calls in Python?</args></response>]\n",
      "\n",
      "Processing call [033] out of [100] = [33.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 859 ms\n",
      "Tokens per second [197.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Buffer Error</args></response>]\n",
      "\n",
      "Processing call [034] out of [100] = [34.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 860 ms\n",
      "Tokens per second [191.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>classical music</args></response>]\n",
      "\n",
      "Processing call [035] out of [100] = [35.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 937 ms\n",
      "Tokens per second [201.7]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Aggregation functions in Pandas</args></response>]\n",
      "\n",
      "Processing call [036] out of [100] = [36.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 880 ms\n",
      "Tokens per second [196.6]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>AI in manufacturing</args></response>]\n",
      "\n",
      "Processing call [037] out of [100] = [37.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 858 ms\n",
      "Tokens per second [201.6]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>ResourceWarning</args></response>]\n",
      "\n",
      "Processing call [038] out of [100] = [38.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 939 ms\n",
      "Tokens per second [180.0]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>why do zebras have stripes</args></response>]\n",
      "\n",
      "Processing call [039] out of [100] = [39.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 859 ms\n",
      "Tokens per second [193.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>BytesWarning</args></response>]\n",
      "\n",
      "Processing call [040] out of [100] = [40.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 959 ms\n",
      "Tokens per second [198.1]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>OS Error: Operating system error</args></response>]\n",
      "\n",
      "Processing call [041] out of [100] = [41.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 999 ms\n",
      "Tokens per second [195.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Exploratory data analysis with Pandas</args></response>]\n",
      "\n",
      "Processing call [042] out of [100] = [42.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 880 ms\n",
      "Tokens per second [190.9]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>PendingDeprecationWarning</args></response>]\n",
      "\n",
      "Processing call [043] out of [100] = [43.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 944 ms\n",
      "Tokens per second [190.7]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Binning data in Pandas</args></response>]\n",
      "\n",
      "Processing call [044] out of [100] = [44.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 898 ms\n",
      "Tokens per second [208.2]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>latest space exploration news</args></response>]\n",
      "\n",
      "Processing call [045] out of [100] = [45.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 918 ms\n",
      "Tokens per second [187.2]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>how to compost at home</args></response>]\n",
      "\n",
      "Processing call [046] out of [100] = [46.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 840 ms\n",
      "Tokens per second [184.5]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>URL Error</args></response>]\n",
      "\n",
      "Processing call [047] out of [100] = [47.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,285 ms\n",
      "Tokens per second [187.5]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AWS SageMaker for ML: How does AWS SageMaker facilitate machine learning model development?</args></response>]\n",
      "\n",
      "Processing call [048] out of [100] = [48.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 900 ms\n",
      "Tokens per second [198.9]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI in natural language generation</args></response>]\n",
      "\n",
      "Processing call [049] out of [100] = [49.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,003 ms\n",
      "Tokens per second [187.4]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Time series analysis in Pandas</args></response>]\n",
      "\n",
      "Processing call [050] out of [100] = [50.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 920 ms\n",
      "Tokens per second [182.6]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Bytes Warning: Bytecode issue</args></response>]\n",
      "\n",
      "Processing call [051] out of [100] = [51.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 942 ms\n",
      "Tokens per second [193.2]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>OS Error: Operating system error</args></response>]\n",
      "\n",
      "Processing call [052] out of [100] = [52.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 892 ms\n",
      "Tokens per second [189.5]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI in public safety</args></response>]\n",
      "\n",
      "Processing call [053] out of [100] = [53.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 968 ms\n",
      "Tokens per second [184.9]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Using Pandas for predictive modeling</args></response>]\n",
      "\n",
      "Processing call [054] out of [100] = [54.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 860 ms\n",
      "Tokens per second [191.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>new car reviews</args></response>]\n",
      "\n",
      "Processing call [055] out of [100] = [55.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 898 ms\n",
      "Tokens per second [190.4]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>how to plant tomatoes</args></response>]\n",
      "\n",
      "Processing call [056] out of [100] = [56.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 878 ms\n",
      "Tokens per second [195.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI in retail analytics</args></response>]\n",
      "\n",
      "Processing call [057] out of [100] = [57.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 918 ms\n",
      "Tokens per second [189.5]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Handling large datasets with Pandas</args></response>]\n",
      "\n",
      "Processing call [058] out of [100] = [58.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,115 ms\n",
      "Tokens per second [207.2]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>What are the steps to resolve permission-related errors in file operations in Python?</args></response>]\n",
      "\n",
      "Processing call [059] out of [100] = [59.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,075 ms\n",
      "Tokens per second [204.7]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>How can you manage errors that occur due to missing files in Python programs?</args></response>]\n",
      "\n",
      "Processing call [060] out of [100] = [60.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,116 ms\n",
      "Tokens per second [204.3]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI in sports analytics: How is AI used in sports analytics to improve performance?</args></response>]\n",
      "\n",
      "Processing call [061] out of [100] = [61.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 878 ms\n",
      "Tokens per second [198.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>ConnectionResetError</args></response>]\n",
      "\n",
      "Processing call [062] out of [100] = [62.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 857 ms\n",
      "Tokens per second [186.7]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>sports</args></response>]\n",
      "\n",
      "Processing call [063] out of [100] = [63.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 919 ms\n",
      "Tokens per second [194.8]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>AI-driven fraud detection</args></response>]\n",
      "\n",
      "Processing call [064] out of [100] = [64.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,197 ms\n",
      "Tokens per second [200.5]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Real-time data streaming: How do you handle real-time data streaming in big data projects?</args></response>]\n",
      "\n",
      "Processing call [065] out of [100] = [65.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,279 ms\n",
      "Tokens per second [198.6]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Outlier Detection methods: What methods are most effective for outlier detection in univariate datasets?</args></response>]\n",
      "\n",
      "Processing call [066] out of [100] = [66.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 957 ms\n",
      "Tokens per second [207.9]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Runtime Warning: Runtime behavior warning</args></response>]\n",
      "\n",
      "Processing call [067] out of [100] = [67.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 999 ms\n",
      "Tokens per second [177.2]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>how do i cure dandruff?</args></response>]\n",
      "\n",
      "Processing call [068] out of [100] = [68.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,098 ms\n",
      "Tokens per second [209.5]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI in digital marketing: What role does AI play in digital marketing strategies?</args></response>]\n",
      "\n",
      "Processing call [069] out of [100] = [69.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 898 ms\n",
      "Tokens per second [192.7]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>ArithmeticError</args></response>]\n",
      "\n",
      "Processing call [070] out of [100] = [70.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 861 ms\n",
      "Tokens per second [189.3]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>exchange rates today</args></response>]\n",
      "\n",
      "Processing call [071] out of [100] = [71.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 894 ms\n",
      "Tokens per second [186.8]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Merging strategies in Pandas</args></response>]\n",
      "\n",
      "Processing call [072] out of [100] = [72.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 872 ms\n",
      "Tokens per second [199.5]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Connection Refused Error</args></response>]\n",
      "\n",
      "Processing call [073] out of [100] = [73.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,198 ms\n",
      "Tokens per second [208.7]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>What strategies can you use to fix type errors that arise from operations on incompatible data types in Python?</args></response>]\n",
      "\n",
      "Processing call [074] out of [100] = [74.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 874 ms\n",
      "Tokens per second [181.9]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>DIY home decor ideas</args></response>]\n",
      "\n",
      "Processing call [075] out of [100] = [75.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,070 ms\n",
      "Tokens per second [192.5]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>AI patent trends: What are the current trends in AI patents?</args></response>]\n",
      "\n",
      "Processing call [076] out of [100] = [76.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,111 ms\n",
      "Tokens per second [208.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>AI in sports analytics: How is AI used in sports analytics to improve performance?</args></response>]\n",
      "\n",
      "Processing call [077] out of [100] = [77.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,148 ms\n",
      "Tokens per second [210.8]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>Data Science career trends: What are the current career trends in the field of data science?</args></response>]\n",
      "\n",
      "Processing call [078] out of [100] = [78.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 913 ms\n",
      "Tokens per second [191.7]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Gradient Boosting</args></response>]\n",
      "\n",
      "Processing call [079] out of [100] = [79.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 876 ms\n",
      "Tokens per second [205.5]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Connection Reset Error</args></response>]\n",
      "\n",
      "Processing call [080] out of [100] = [80.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 892 ms\n",
      "Tokens per second [204.0]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>Computer Vision applications</args></response>]\n",
      "\n",
      "Processing call [081] out of [100] = [81.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 874 ms\n",
      "Tokens per second [183.1]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Ethical AI guidelines</args></response>]\n",
      "\n",
      "Processing call [082] out of [100] = [82.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 875 ms\n",
      "Tokens per second [195.4]\n",
      "Response: [<response><browser-command>search google scholar new tab</browser-command><args>new book releases</args></response>]\n",
      "\n",
      "Processing call [083] out of [100] = [83.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 973 ms\n",
      "Tokens per second [188.1]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>why don't people like me?</args></response>]\n",
      "\n",
      "Processing call [084] out of [100] = [84.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 932 ms\n",
      "Tokens per second [201.7]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>weather forecast washington dc</args></response>]\n",
      "\n",
      "Processing call [085] out of [100] = [85.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 913 ms\n",
      "Tokens per second [187.3]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>AI in e-commerce personalization</args></response>]\n",
      "\n",
      "Processing call [086] out of [100] = [86.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 874 ms\n",
      "Tokens per second [199.1]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>new dress styles</args></response>]\n",
      "\n",
      "Processing call [087] out of [100] = [87.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 915 ms\n",
      "Tokens per second [182.5]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Performance tuning in Pandas</args></response>]\n",
      "\n",
      "Processing call [088] out of [100] = [88.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 915 ms\n",
      "Tokens per second [185.8]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Data imputation with Pandas</args></response>]\n",
      "\n",
      "Processing call [089] out of [100] = [89.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,152 ms\n",
      "Tokens per second [213.5]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>How do you address deprecation warnings in Python to ensure code compatibility with future versions?</args></response>]\n",
      "\n",
      "Processing call [090] out of [100] = [90.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 896 ms\n",
      "Tokens per second [192.0]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Explainable AI</args></response>]\n",
      "\n",
      "Processing call [091] out of [100] = [91.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 936 ms\n",
      "Tokens per second [212.6]\n",
      "Response: [<response><browser-command>search google scholar current tab</browser-command><args>Runtime Error: Runtime exception occurred</args></response>]\n",
      "\n",
      "Processing call [092] out of [100] = [92.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 856 ms\n",
      "Tokens per second [185.7]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>Unicode Decode Error</args></response>]\n",
      "\n",
      "Processing call [093] out of [100] = [93.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 874 ms\n",
      "Tokens per second [179.6]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>FloatingPointError</args></response>]\n",
      "\n",
      "Processing call [094] out of [100] = [94.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 958 ms\n",
      "Tokens per second [180.6]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Saving Pandas DataFrame to CSV</args></response>]\n",
      "\n",
      "Processing call [095] out of [100] = [95.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 879 ms\n",
      "Tokens per second [180.9]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>why cats purr</args></response>]\n",
      "\n",
      "Processing call [096] out of [100] = [96.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,055 ms\n",
      "Tokens per second [201.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>How do you handle warnings related to Unicode issues in Python?</args></response>]\n",
      "\n",
      "Processing call [097] out of [100] = [97.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 1,076 ms\n",
      "Tokens per second [196.8]\n",
      "Response: [<response><browser-command>search google new tab</browser-command><args>What are import warnings in Python, and how can they be addressed?</args></response>]\n",
      "\n",
      "Processing call [098] out of [100] = [98.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 876 ms\n",
      "Tokens per second [192.9]\n",
      "Response: [<response><browser-command>search google current tab</browser-command><args>why does bread rise</args></response>]\n",
      "\n",
      "Processing call [099] out of [100] = [99.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 916 ms\n",
      "Tokens per second [188.9]\n",
      "Response: [<response><browser-command>search new tab</browser-command><args>how to bake chocolate chip cookies</args></response>]\n",
      "\n",
      "Processing call [100] out of [100] = [100.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]...\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2]... Done! in 936 ms\n",
      "Tokens per second [197.6]\n",
      "Response: [<response><browser-command>search current tab</browser-command><args>Overflow Error: Value too large to convert</args></response>]\n",
      "\n",
      "\n",
      "Generating responses for 100 rows... Done! in 01:36\n",
      "[(966.4,)] ms per item\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation stats for model mistralai/Mistral-7B-Instruct-v0.2\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "          Contains response 100.0%\n",
      " Contains <browser-command> 100.0%\n",
      "            Contains <args> 100.0%\n",
      "          Response is exact 99.0%\n",
      "Response has correct values 99.0%\n",
      " Browser command is correct 100.0%\n",
      "            Args is correct 99.0%\n"
     ]
    }
   ],
   "source": [
    "run_validation( model_aqw, tokenizer_awq )\n",
    "\n",
    "# Generating responses for 100 rows... Done! in 01:36\n",
    "# [966] ms per item\n",
    "# \n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation stats for model mistralai/Mistral-7B-Instruct-v0.2, In memory loaded by Jupiter notebook\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#  Contains <browser-command> 100.0%\n",
    "#             Contains <args> 100.0%\n",
    "#           Response is exact 99.0%\n",
    "# Response has correct values 99.0%\n",
    "#  Browser command is correct 100.0%\n",
    "#             Args is correct 99.0%"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:32:50.207021Z",
     "start_time": "2024-01-21T00:31:13.357571Z"
    }
   },
   "id": "c45b4ebe0d4d18ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validate AWQ model: TGI service listening on port 3000"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aae798c6fc1afbf0"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands file for [search new tab] exists: False\n",
      "Commands file for [search current tab] exists: False\n",
      "Commands file for [search google new tab] exists: False\n",
      "Commands file for [search google current tab] exists: False\n",
      "Commands file for [search google scholar new tab] exists: False\n",
      "Commands file for [search google scholar current tab] exists: False\n",
      "\n",
      "Generating responses for 100 rows...\n",
      "Using TGI w/ model_name [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "Processing call [001] out of [100] = [1.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>TabError</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 404 ms\n",
      "Tokens per second [86.6]\n",
      "Token list length [35]\n",
      "Processing call [002] out of [100] = [2.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>How can you handle reference errors, especially with weak references, in Python?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 508 ms\n",
      "Tokens per second [98.4]\n",
      "Token list length [50]\n",
      "Processing call [003] out of [100] = [3.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>IndentationError</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 390 ms\n",
      "Tokens per second [94.9]\n",
      "Token list length [37]\n",
      "Processing call [004] out of [100] = [4.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>BERT model improvements</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 397 ms\n",
      "Tokens per second [95.7]\n",
      "Token list length [38]\n",
      "Processing call [005] out of [100] = [5.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>Creating histograms in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [96.2]\n",
      "Token list length [43]\n",
      "Processing call [006] out of [100] = [6.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>What are the best practices for managing timeout errors in Python, especially in network requests?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [97.7]\n",
      "Token list length [50]\n",
      "Processing call [007] out of [100] = [7.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>Process Lookup Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 399 ms\n",
      "Tokens per second [95.2]\n",
      "Token list length [38]\n",
      "Processing call [008] out of [100] = [8.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>UserWarning</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 381 ms\n",
      "Tokens per second [94.5]\n",
      "Token list length [36]\n",
      "Processing call [009] out of [100] = [9.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>AttributeError</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 383 ms\n",
      "Tokens per second [94.0]\n",
      "Token list length [36]\n",
      "Processing call [010] out of [100] = [10.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>pop music</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 390 ms\n",
      "Tokens per second [94.9]\n",
      "Token list length [37]\n",
      "Processing call [011] out of [100] = [11.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>How can you manage errors that occur due to missing files in Python programs?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 492 ms\n",
      "Tokens per second [97.6]\n",
      "Token list length [48]\n",
      "Processing call [012] out of [100] = [12.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>Data Science career trends: What are the current career trends in the field of data science?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 530 ms\n",
      "Tokens per second [98.1]\n",
      "Token list length [52]\n",
      "Processing call [013] out of [100] = [13.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>UserWarning</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 383 ms\n",
      "Tokens per second [94.0]\n",
      "Token list length [36]\n",
      "Processing call [014] out of [100] = [14.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>Generating summary statistics in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 425 ms\n",
      "Tokens per second [96.5]\n",
      "Token list length [41]\n",
      "Processing call [015] out of [100] = [15.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>Google Cloud AI services: What AI services does Google Cloud offer for developers and data scientists?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [98.7]\n",
      "Token list length [52]\n",
      "Processing call [016] out of [100] = [16.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>how to paint a room</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 399 ms\n",
      "Tokens per second [95.2]\n",
      "Token list length [38]\n",
      "Processing call [017] out of [100] = [17.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Unsupported Function Call: Function not supported for this dtype</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [96.9]\n",
      "Token list length [44]\n",
      "Processing call [018] out of [100] = [18.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>PermissionError</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 390 ms\n",
      "Tokens per second [94.9]\n",
      "Token list length [37]\n",
      "Processing call [019] out of [100] = [19.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>What are the reasons for attribute-related errors in Python objects and how can they be fixed?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [98.7]\n",
      "Token list length [52]\n",
      "Processing call [020] out of [100] = [20.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Data deduplication in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 434 ms\n",
      "Tokens per second [96.8]\n",
      "Token list length [42]\n",
      "Processing call [021] out of [100] = [21.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>OS Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 380 ms\n",
      "Tokens per second [94.7]\n",
      "Token list length [36]\n",
      "Processing call [022] out of [100] = [22.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Responsible AI practices: How can organizations implement responsible AI practices?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 501 ms\n",
      "Tokens per second [97.8]\n",
      "Token list length [49]\n",
      "Processing call [023] out of [100] = [23.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>popular music genres</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 418 ms\n",
      "Tokens per second [95.7]\n",
      "Token list length [40]\n",
      "Processing call [024] out of [100] = [24.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Blocking IO Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 391 ms\n",
      "Tokens per second [94.6]\n",
      "Token list length [37]\n",
      "Processing call [025] out of [100] = [25.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>Convolutional Neural Networks</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 437 ms\n",
      "Tokens per second [96.1]\n",
      "Token list length [42]\n",
      "Processing call [026] out of [100] = [26.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Data partitioning in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 426 ms\n",
      "Tokens per second [96.2]\n",
      "Token list length [41]\n",
      "Processing call [027] out of [100] = [27.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>What are runtime warnings in Python, and how can they be used to identify potential issues?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 539 ms\n",
      "Tokens per second [98.3]\n",
      "Token list length [53]\n",
      "Processing call [028] out of [100] = [28.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Syntax Warning</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 380 ms\n",
      "Tokens per second [94.7]\n",
      "Token list length [36]\n",
      "Processing call [029] out of [100] = [29.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>What are the best practices for handling I/O blocking errors in Python?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 499 ms\n",
      "Tokens per second [98.2]\n",
      "Token list length [49]\n",
      "Processing call [030] out of [100] = [30.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>AI for health diagnostics: How does AI contribute to advancements in health diagnostics?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 527 ms\n",
      "Tokens per second [98.7]\n",
      "Token list length [52]\n",
      "Processing call [031] out of [100] = [31.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Deep Learning optimization: What strategies are used for optimizing deep learning models?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 509 ms\n",
      "Tokens per second [98.2]\n",
      "Token list length [50]\n",
      "Processing call [032] out of [100] = [32.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>How can you resolve recursion errors due to excessive recursive calls in Python?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [98.5]\n",
      "Token list length [51]\n",
      "Processing call [033] out of [100] = [33.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Buffer Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 391 ms\n",
      "Tokens per second [94.6]\n",
      "Token list length [37]\n",
      "Processing call [034] out of [100] = [34.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>classical music</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 390 ms\n",
      "Tokens per second [94.9]\n",
      "Token list length [37]\n",
      "Processing call [035] out of [100] = [35.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Aggregation functions in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 425 ms\n",
      "Tokens per second [96.5]\n",
      "Token list length [41]\n",
      "Processing call [036] out of [100] = [36.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>AI in manufacturing</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 401 ms\n",
      "Tokens per second [94.8]\n",
      "Token list length [38]\n",
      "Processing call [037] out of [100] = [37.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>ResourceWarning</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 390 ms\n",
      "Tokens per second [94.9]\n",
      "Token list length [37]\n",
      "Processing call [038] out of [100] = [38.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>why do zebras have stripes</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 428 ms\n",
      "Tokens per second [95.8]\n",
      "Token list length [41]\n",
      "Processing call [039] out of [100] = [39.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>BytesWarning</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 390 ms\n",
      "Tokens per second [94.9]\n",
      "Token list length [37]\n",
      "Processing call [040] out of [100] = [40.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>OS Error: Operating system error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 437 ms\n",
      "Tokens per second [96.1]\n",
      "Token list length [42]\n",
      "Processing call [041] out of [100] = [41.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Exploratory data analysis with Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 455 ms\n",
      "Tokens per second [96.7]\n",
      "Token list length [44]\n",
      "Processing call [042] out of [100] = [42.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>PendingDeprecationWarning</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 399 ms\n",
      "Tokens per second [95.2]\n",
      "Token list length [38]\n",
      "Processing call [043] out of [100] = [43.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Binning data in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 429 ms\n",
      "Tokens per second [95.6]\n",
      "Token list length [41]\n",
      "Processing call [044] out of [100] = [44.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>latest space exploration news</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 408 ms\n",
      "Tokens per second [95.6]\n",
      "Token list length [39]\n",
      "Processing call [045] out of [100] = [45.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>how to compost at home</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 424 ms\n",
      "Tokens per second [94.3]\n",
      "Token list length [40]\n",
      "Processing call [046] out of [100] = [46.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>URL Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 388 ms\n",
      "Tokens per second [92.8]\n",
      "Token list length [36]\n",
      "Processing call [047] out of [100] = [47.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>AWS SageMaker for ML: How does AWS SageMaker facilitate machine learning model development?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 582 ms\n",
      "Tokens per second [96.2]\n",
      "Token list length [56]\n",
      "Processing call [048] out of [100] = [48.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>AI in natural language generation</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 410 ms\n",
      "Tokens per second [95.1]\n",
      "Token list length [39]\n",
      "Processing call [049] out of [100] = [49.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Time series analysis in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 425 ms\n",
      "Tokens per second [96.5]\n",
      "Token list length [41]\n",
      "Processing call [050] out of [100] = [50.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Bytes Warning: Bytecode issue</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 424 ms\n",
      "Tokens per second [94.3]\n",
      "Token list length [40]\n",
      "Processing call [051] out of [100] = [51.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>OS Error: Operating system error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 434 ms\n",
      "Tokens per second [94.5]\n",
      "Token list length [41]\n",
      "Processing call [052] out of [100] = [52.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>AI in public safety</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 402 ms\n",
      "Tokens per second [94.5]\n",
      "Token list length [38]\n",
      "Processing call [053] out of [100] = [53.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>Using Pandas for predictive modeling</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 425 ms\n",
      "Tokens per second [94.1]\n",
      "Token list length [40]\n",
      "Processing call [054] out of [100] = [54.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>new car reviews</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 393 ms\n",
      "Tokens per second [94.1]\n",
      "Token list length [37]\n",
      "Processing call [055] out of [100] = [55.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>how to plant tomatoes</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 412 ms\n",
      "Tokens per second [94.7]\n",
      "Token list length [39]\n",
      "Processing call [056] out of [100] = [56.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>AI in retail analytics</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 405 ms\n",
      "Tokens per second [93.8]\n",
      "Token list length [38]\n",
      "Processing call [057] out of [100] = [57.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Handling large datasets with Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 422 ms\n",
      "Tokens per second [94.8]\n",
      "Token list length [40]\n",
      "Processing call [058] out of [100] = [58.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>What are the steps to resolve permission-related errors in file operations in Python?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 514 ms\n",
      "Tokens per second [97.3]\n",
      "Token list length [50]\n",
      "Processing call [059] out of [100] = [59.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>How can you manage errors that occur due to missing files in Python programs?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 497 ms\n",
      "Tokens per second [96.6]\n",
      "Token list length [48]\n",
      "Processing call [060] out of [100] = [60.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>AI in sports analytics: How is AI used in sports analytics to improve performance?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 512 ms\n",
      "Tokens per second [97.7]\n",
      "Token list length [50]\n",
      "Processing call [061] out of [100] = [61.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>ConnectionResetError</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 399 ms\n",
      "Tokens per second [95.2]\n",
      "Token list length [38]\n",
      "Processing call [062] out of [100] = [62.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>sports</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 393 ms\n",
      "Tokens per second [94.1]\n",
      "Token list length [37]\n",
      "Processing call [063] out of [100] = [63.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>AI-driven fraud detection</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 419 ms\n",
      "Tokens per second [95.5]\n",
      "Token list length [40]\n",
      "Processing call [064] out of [100] = [64.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>Real-time data streaming: How do you handle real-time data streaming in big data projects?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 549 ms\n",
      "Tokens per second [98.4]\n",
      "Token list length [54]\n",
      "Processing call [065] out of [100] = [65.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>Outlier Detection methods: What methods are most effective for outlier detection in univariate datasets?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 590 ms\n",
      "Tokens per second [98.3]\n",
      "Token list length [58]\n",
      "Processing call [066] out of [100] = [66.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Runtime Warning: Runtime behavior warning</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 441 ms\n",
      "Tokens per second [95.2]\n",
      "Token list length [42]\n",
      "Processing call [067] out of [100] = [67.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>how do i cure dandruff?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 456 ms\n",
      "Tokens per second [96.5]\n",
      "Token list length [44]\n",
      "Processing call [068] out of [100] = [68.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>AI in digital marketing: What role does AI play in digital marketing strategies?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 499 ms\n",
      "Tokens per second [98.2]\n",
      "Token list length [49]\n",
      "Processing call [069] out of [100] = [69.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>ArithmeticError</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 407 ms\n",
      "Tokens per second [95.8]\n",
      "Token list length [39]\n",
      "Processing call [070] out of [100] = [70.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>exchange rates today</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 391 ms\n",
      "Tokens per second [94.6]\n",
      "Token list length [37]\n",
      "Processing call [071] out of [100] = [71.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Merging strategies in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 407 ms\n",
      "Tokens per second [95.8]\n",
      "Token list length [39]\n",
      "Processing call [072] out of [100] = [72.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>Connection Refused Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 402 ms\n",
      "Tokens per second [94.5]\n",
      "Token list length [38]\n",
      "Processing call [073] out of [100] = [73.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>What strategies can you use to fix type errors that arise from operations on incompatible data types in Python?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 548 ms\n",
      "Tokens per second [98.5]\n",
      "Token list length [54]\n",
      "Processing call [074] out of [100] = [74.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>DIY home decor ideas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 398 ms\n",
      "Tokens per second [95.5]\n",
      "Token list length [38]\n",
      "Processing call [075] out of [100] = [75.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>AI patent trends: What are the current trends in AI patents?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 499 ms\n",
      "Tokens per second [96.2]\n",
      "Token list length [48]\n",
      "Processing call [076] out of [100] = [76.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>AI in sports analytics: How is AI used in sports analytics to improve performance?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 518 ms\n",
      "Tokens per second [96.5]\n",
      "Token list length [50]\n",
      "Processing call [077] out of [100] = [77.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>Data Science career trends: What are the current career trends in the field of data science?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 540 ms\n",
      "Tokens per second [96.3]\n",
      "Token list length [52]\n",
      "Processing call [078] out of [100] = [78.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Gradient Boosting</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 429 ms\n",
      "Tokens per second [93.2]\n",
      "Token list length [40]\n",
      "Processing call [079] out of [100] = [79.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Connection Reset Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 413 ms\n",
      "Tokens per second [92.0]\n",
      "Token list length [38]\n",
      "Processing call [080] out of [100] = [80.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>Computer Vision applications</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 420 ms\n",
      "Tokens per second [92.9]\n",
      "Token list length [39]\n",
      "Processing call [081] out of [100] = [81.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Ethical AI guidelines</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 408 ms\n",
      "Tokens per second [93.1]\n",
      "Token list length [38]\n",
      "Processing call [082] out of [100] = [82.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search google scholar new tab</browser-command>\n",
      "            <args>new book releases</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 412 ms\n",
      "Tokens per second [92.2]\n",
      "Token list length [38]\n",
      "Processing call [083] out of [100] = [83.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>why don't people like me?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 454 ms\n",
      "Tokens per second [94.7]\n",
      "Token list length [43]\n",
      "Processing call [084] out of [100] = [84.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>weather forecast washington dc</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 439 ms\n",
      "Tokens per second [93.4]\n",
      "Token list length [41]\n",
      "Processing call [085] out of [100] = [85.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>AI in e-commerce personalization</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 431 ms\n",
      "Tokens per second [92.8]\n",
      "Token list length [40]\n",
      "Processing call [086] out of [100] = [86.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>new dress styles</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 408 ms\n",
      "Tokens per second [92.9]\n",
      "Token list length [38]\n",
      "Processing call [087] out of [100] = [87.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Performance tuning in Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 429 ms\n",
      "Tokens per second [93.2]\n",
      "Token list length [40]\n",
      "Processing call [088] out of [100] = [88.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>Data imputation with Pandas</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 429 ms\n",
      "Tokens per second [93.2]\n",
      "Token list length [40]\n",
      "Processing call [089] out of [100] = [89.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>How do you address deprecation warnings in Python to ensure code compatibility with future versions?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 542 ms\n",
      "Tokens per second [95.9]\n",
      "Token list length [52]\n",
      "Processing call [090] out of [100] = [90.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Explainable AI</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 420 ms\n",
      "Tokens per second [92.9]\n",
      "Token list length [39]\n",
      "Processing call [091] out of [100] = [91.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google scholar current tab</browser-command>\n",
      "            <args>Runtime Error: Runtime exception occurred</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 435 ms\n",
      "Tokens per second [94.3]\n",
      "Token list length [41]\n",
      "Processing call [092] out of [100] = [92.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>Unicode Decode Error</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 403 ms\n",
      "Tokens per second [91.8]\n",
      "Token list length [37]\n",
      "Processing call [093] out of [100] = [93.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>FloatingPointError</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 410 ms\n",
      "Tokens per second [92.7]\n",
      "Token list length [38]\n",
      "Processing call [094] out of [100] = [94.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>Saving Pandas DataFrame to CSV</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 447 ms\n",
      "Tokens per second [94.0]\n",
      "Token list length [42]\n",
      "Processing call [095] out of [100] = [95.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>why cats purr</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 413 ms\n",
      "Tokens per second [92.0]\n",
      "Token list length [38]\n",
      "Processing call [096] out of [100] = [96.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>How do you handle warnings related to Unicode issues in Python?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 494 ms\n",
      "Tokens per second [95.1]\n",
      "Token list length [47]\n",
      "Processing call [097] out of [100] = [97.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google new tab</browser-command>\n",
      "            <args>What are import warnings in Python, and how can they be addressed?</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 502 ms\n",
      "Tokens per second [95.6]\n",
      "Token list length [48]\n",
      "Processing call [098] out of [100] = [98.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search google current tab</browser-command>\n",
      "            <args>why does bread rise</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 410 ms\n",
      "Tokens per second [92.7]\n",
      "Token list length [38]\n",
      "Processing call [099] out of [100] = [99.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search new tab</browser-command>\n",
      "            <args>how to bake chocolate chip cookies</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 427 ms\n",
      "Tokens per second [93.7]\n",
      "Token list length [40]\n",
      "Processing call [100] out of [100] = [100.0%]... \n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]...\n",
      "\n",
      "        <response>\n",
      "            <browser-command>search current tab</browser-command>\n",
      "            <args>Overflow Error: Value too large to convert</args>\n",
      "        </response>\n",
      "Asking LLM [mistralai/Mistral-7B-Instruct-v0.2-AWQ]... Done! in 440 ms\n",
      "Tokens per second [93.2]\n",
      "Token list length [41]\n",
      "\n",
      "Generating responses for 100 rows... Done! in 44 seconds\n",
      "[(442.4,)] ms per item\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "- Validation Stats for `mistralai/Mistral-7B-Instruct-v0.2-AWQ` on TGI:3000\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "               Is valid xml 100.0%\n",
      "          Contains response 100.0%\n",
      " Contains <browser-command> 100.0%\n",
      "            Contains <args> 100.0%\n",
      "          Response is exact 99.0%\n",
      "Response has correct values 99.0%\n",
      " Browser command is correct 100.0%\n",
      "            Args is correct 99.0%\n"
     ]
    }
   ],
   "source": [
    "tgi_validator  = XmlFineTuningPromptGenerator( tgi_url=\"http://172.17.0.4:3000\", debug=True )\n",
    "\n",
    "model_name     = \"mistralai/Mistral-7B-Instruct-v0.2-AWQ\"\n",
    "\n",
    "validate_df    = pd.read_json( \"/var/model/genie-in-the-box/src/ephemera/prompts/data/voice-commands-xml-validate.jsonl\", lines=True ).sample( 100, random_state=42 )\n",
    "validate_df    = tgi_validator.generate_responses( validate_df, switch=\"tgi\", model_name=model_name )\n",
    "validate_df    = tgi_validator.validate_responses( validate_df )\n",
    "\n",
    "tgi_validator.print_validation_stats( validate_df, title=f\"Validation Stats for `{model_name}` on TGI:3000\" )\n",
    "\n",
    "# Generating responses for 100 rows... Done! in 44 seconds\n",
    "# [442] ms per item\n",
    "# \n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# - Validation Stats for `mistralai/Mistral-7B-Instruct-v0.2-AWQ` on TGI:3000\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# \n",
    "#                Is valid xml 100.0%\n",
    "#           Contains response 100.0%\n",
    "#  Contains <browser-command> 100.0%\n",
    "#             Contains <args> 100.0%\n",
    "#           Response is exact 99.0%\n",
    "# Response has correct values 99.0%\n",
    "#  Browser command is correct 100.0%\n",
    "#             Args is correct 99.0%\n",
    "\n",
    "# Sat Jan 20 19:40:03 2024\n",
    "# +---------------------------------------------------------------------------------------+\n",
    "# | NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "# |-----------------------------------------+----------------------+----------------------+\n",
    "# | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "# | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "# |                                         |                      |               MIG M. |\n",
    "# |=========================================+======================+======================|\n",
    "# |   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0 Off |                  Off |\n",
    "# |  0%   37C    P8              22W / 450W |  20240MiB / 24564MiB |      0%      Default |\n",
    "# |                                         |                      |                  N/A |\n",
    "# +-----------------------------------------+----------------------+----------------------+\n",
    "# |   1  NVIDIA GeForce RTX 4090        On  | 00000000:02:00.0 Off |                  Off |\n",
    "# |  0%   43C    P8              30W / 450W |   5482MiB / 24564MiB |      0%      Default |\n",
    "# |                                         |                      |                  N/A |\n",
    "# +-----------------------------------------+----------------------+----------------------+\n",
    "# \n",
    "# +---------------------------------------------------------------------------------------+\n",
    "# | Processes:                                                                            |\n",
    "# |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "# |        ID   ID                                                             Usage      |\n",
    "# |=======================================================================================|\n",
    "# |    0   N/A  N/A     27822      C   /opt/conda/bin/python3.10                 20230MiB |\n",
    "# |    1   N/A  N/A     25717      C   /usr/bin/python3                           5472MiB |\n",
    "# +---------------------------------------------------------------------------------------+"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T00:44:29.806811Z",
     "start_time": "2024-01-21T00:43:45.479913Z"
    }
   },
   "id": "a1a0e688cb8e66f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "607ebe7d212e2d96"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
